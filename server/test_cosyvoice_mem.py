import os
import sys
import time
import torch
import subprocess

# å¼ºåˆ¶æ·»åŠ  CosyVoice è·¯å¾„
COSYVOICE_PATH = "/workspace/CosyVoice"
sys.path.insert(0, COSYVOICE_PATH)
sys.path.append(os.path.join(COSYVOICE_PATH, "third_party/Matcha-TTS"))

from cosyvoice.cli.cosyvoice import CosyVoice
from cosyvoice.utils.file_utils import load_wav

def check_vram():
    result = subprocess.run(
        ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'],
        capture_output=True, text=True
    )
    print(f"ğŸ”¥ Current VRAM: {result.stdout.strip()} MB")

def test_memory():
    print("=== CosyVoice Memory Profiling ===")
    check_vram()
    
    model_path = "/workspace/CosyVoice/pretrained_models/CosyVoice-300M"
    print(f"Loading CosyVoice from {model_path}...")
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # åˆå§‹åŒ–
    cosy_voice = CosyVoice(model_path)
    
    print(f"âœ… Loaded in {time.time() - start_time:.2f}s")
    check_vram()
    
    print("\nRunning inference...")
    # ç®€å•çš„æ¨ç†æµ‹è¯•
    prompt_speech_16k = load_wav(os.path.join(COSYVOICE_PATH, 'asset/zero_shot_prompt.wav'), 16000)
    
    for i, j in enumerate(cosy_voice.inference_zero_shot('ä½ å¥½ï¼Œæˆ‘æ˜¯ Trinityï¼Œä½ çš„æ•°å­—ä¼´ä¾£ã€‚', 'å¸Œæœ›ä½ ä»Šå¤©è¿‡å¾—å¼€å¿ƒã€‚', prompt_speech_16k)):
        print(f"Generated chunk {i}")
        
    print("\nâœ… Inference done")
    check_vram()

if __name__ == "__main__":
    test_memory()


