"""
Project Trinity - Server Entry Point
æœåŠ¡ç«¯ä¸»å…¥å£

å¯åŠ¨æ–¹å¼:
    uvicorn main:app --host 0.0.0.0 --port 8000

æˆ–:
    python main.py
"""

import sys
import os

# ============== è·¯å¾„é»‘ç§‘æŠ€ ==============
# å¼ºåˆ¶å°† server ç›®å½•åŠ å…¥è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# å¼ºåˆ¶å°† CosyVoice åŠ å…¥è·¯å¾„ (æœ€é«˜ä¼˜å…ˆçº§)
COSYVOICE_PATH = "/workspace/CosyVoice"
if os.path.exists(COSYVOICE_PATH):
    # ç§»é™¤å·²å­˜åœ¨çš„è·¯å¾„ä»¥é˜²æ­¢é‡å¤ï¼Œç„¶åæ’å…¥åˆ°æœ€å‰é¢
    if COSYVOICE_PATH in sys.path:
        sys.path.remove(COSYVOICE_PATH)
    sys.path.insert(0, COSYVOICE_PATH)
    print(f"âœ… å·²å¼ºåˆ¶æ·»åŠ  CosyVoice è·¯å¾„: {COSYVOICE_PATH}")
    
    # éªŒè¯æ˜¯å¦èƒ½å¯¼å…¥
    try:
        import cosyvoice
        print(f"âœ… CosyVoice æ¨¡å—éªŒè¯æˆåŠŸ: {cosyvoice.__file__}")
    except ImportError as e:
        print(f"âŒ CosyVoice æ¨¡å—éªŒè¯å¤±è´¥: {e}")
else:
    print(f"âš ï¸ æœªæ‰¾åˆ° CosyVoice ç›®å½•: {COSYVOICE_PATH}")
# ========================================

import asyncio
import time
from datetime import datetime
from contextlib import asynccontextmanager
from typing import Optional
import json

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, UploadFile, File, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from loguru import logger
import uvicorn
import shutil
import uuid
from pathlib import Path

from config import settings
from adapters import VoiceAdapter, BrainAdapter, MouthAdapter, DriverAdapter
from mind_engine import BioState, NarrativeManager, EgoDirector
from monitor import SystemMonitor

def write_chat_log(log_data: dict):
    """
    å¯¹è¯æ—¥å¿— - æŒ‰å¤©å½’æ¡£åˆ° logs/conversations/YYYY-MM-DD.jsonl
    """
    try:
        import json
        from pathlib import Path
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        total_time_s = time.time() - log_data["start"]
        speed = len(log_data["output"]) / total_time_s if total_time_s > 0 else 0
        
        entry = {
            "timestamp": datetime.now().isoformat(),
            "user": log_data["input"],
            "assistant": log_data["output"],
            "metrics": {
                "ttft_ms": round(log_data.get("ttft", 0), 2),
                "total_time_s": round(total_time_s, 2),
                "speed_char_per_s": round(speed, 2)
            }
        }
        
        # æŒ‰å¤©å½’æ¡£
        log_dir = Path("/workspace/project-trinity/project-trinity/logs/conversations")
        log_dir.mkdir(parents=True, exist_ok=True)
        
        today = datetime.now().strftime("%Y-%m-%d")
        log_path = log_dir / f"{today}.jsonl"
        
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
        logger.info(f"ğŸ“ å¯¹è¯å·²è®°å½•: {today}.jsonl ({len(entry['assistant'])} chars)")
            
    except Exception as e:
        logger.error(f"å¯¹è¯æ—¥å¿—å†™å…¥å¤±è´¥: {e}")

# ============== å…¨å±€ç»„ä»¶ ==============
monitor: Optional[SystemMonitor] = None
voice_adapter: Optional[VoiceAdapter] = None
brain_adapter: Optional[BrainAdapter] = None
mouth_adapter: Optional[MouthAdapter] = None
driver_adapter: Optional[DriverAdapter] = None

bio_state: Optional[BioState] = None
narrative_mgr: Optional[NarrativeManager] = None
ego_director: Optional[EgoDirector] = None


# ============== ç”Ÿå‘½å‘¨æœŸç®¡ç† ==============
@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    global voice_adapter, brain_adapter, mouth_adapter, driver_adapter
    global bio_state, narrative_mgr, ego_director, monitor
    
    # å¯åŠ¨èµ„æºç›‘æ§
    monitor = SystemMonitor()
    monitor.start()

    logger.info("ğŸ”® Project Trinity å¯åŠ¨ä¸­...")
    
    # åˆå§‹åŒ– Layer 1: æœ¬æˆ‘ (BioState)
    bio_state = BioState()
    logger.success("âœ“ Layer 1 (æœ¬æˆ‘) åˆå§‹åŒ–å®Œæˆ")
    
    # åˆå§‹åŒ– Layer 2: è¶…æˆ‘ (NarrativeManager)
    narrative_mgr = NarrativeManager(
        qdrant_host=settings.memory.qdrant_host,
        qdrant_port=settings.memory.qdrant_port
    )
    await narrative_mgr.initialize()
    logger.success("âœ“ Layer 2 (è¶…æˆ‘) åˆå§‹åŒ–å®Œæˆ")
    
    # åˆå§‹åŒ–é€‚é…å™¨ (å¯é€‰ï¼Œæ ¹æ®ç¯å¢ƒå†³å®šæ˜¯å¦åŠ è½½æ¨¡å‹)
    if not settings.server.debug:
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¾®æœåŠ¡æ¨¡å¼
        if os.getenv("TRINITY_MODE") == "microservice":
            # ============== å¾®æœåŠ¡æ¨¡å¼ ==============
            # Brain å’Œ Mouth é€šè¿‡è¿œç¨‹ Cortex æœåŠ¡å™¨è®¿é—®
            logger.info("ğŸš€ å¾®æœåŠ¡æ¨¡å¼: è¿æ¥åˆ° Cortex Model Server...")
            cortex_url = os.getenv("CORTEX_URL", "http://localhost:9000")
            
            # 1. Remote Brain
            try:
                logger.info(f"æ­£åœ¨åˆå§‹åŒ– Remote BrainAdapter -> {cortex_url}/brain...")
                brain_adapter = BrainAdapter(
                    model_path="REMOTE",
                    remote_url=f"{cortex_url}/brain"
                )
                await brain_adapter.initialize()
                logger.success("âœ“ Remote BrainAdapter åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.error(f"âœ— Remote BrainAdapter å¤±è´¥: {e}")

            # 2. Remote Mouth
            try:
                logger.info(f"æ­£åœ¨åˆå§‹åŒ– Remote MouthAdapter -> {cortex_url}/mouth...")
                mouth_adapter = MouthAdapter(
                    model_path="REMOTE",
                    remote_url=f"{cortex_url}/mouth"
                )
                await mouth_adapter.initialize()
                logger.success("âœ“ Remote MouthAdapter åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.error(f"âœ— Remote MouthAdapter å¤±è´¥: {e}")
                
            # 3. Voice (æœ¬åœ°ï¼ŒFunASR è¾ƒè½»)
            try:
                logger.info("æ­£åœ¨åˆå§‹åŒ– VoiceAdapter (Local)...")
                voice_adapter = VoiceAdapter(
                    model_name=settings.model.funasr_model,
                    device=settings.model.funasr_device
                )
                await voice_adapter.initialize()
                logger.success("âœ“ Voice Adapter åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.error(f"âœ— Voice Adapter å¤±è´¥: {e}")
    
            # 4. Driver (æœ¬åœ°)
            try:
                logger.info("æ­£åœ¨åˆå§‹åŒ– DriverAdapter (Local)...")
                driver_adapter = DriverAdapter(
                    geneface_path=settings.model.geneface_model_path
                )
                await driver_adapter.initialize()
                logger.success("âœ“ Driver Adapter åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.error(f"âœ— Driver Adapter å¤±è´¥: {e}")
                
        else:
            # ============== å•ä½“æ¨¡å¼ ==============
            # æ‰€æœ‰æ¨¡å‹åœ¨æœ¬åœ°åŠ è½½
            logger.info("--- å¼€å§‹ä¸²è¡Œåˆå§‹åŒ–ç»„ä»¶ (å•ä½“æ¨¡å¼) ---")
            
            # 1. å¤§è„‘ (Qwen) - æœ€åƒæ˜¾å­˜ï¼Œå¿…é¡»ç¬¬ä¸€ä¸ªåŠ è½½
            try:
                logger.info("æ­£åœ¨åˆå§‹åŒ– BrainAdapter (Priority 1)...")
                brain_adapter = BrainAdapter(
                    model_path=settings.model.qwen_model_path,
                    tensor_parallel_size=settings.model.qwen_tensor_parallel_size,
                    max_model_len=settings.model.qwen_max_model_len,
                    quantization=settings.model.qwen_quantization,
                    gpu_memory_utilization=settings.model.qwen_gpu_memory_utilization
                )
                await brain_adapter.initialize()
                if not brain_adapter.is_initialized:
                    raise RuntimeError("BrainAdapter åˆå§‹åŒ–å¤±è´¥")
                logger.success("âœ“ Brain Adapter åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.error(f"âœ— Brain Adapter å¤±è´¥: {e}")

            # 2. å˜´å·´ (CosyVoice) - æ˜¾å­˜å ç”¨ç¬¬äºŒ
            try:
                logger.info("æ­£åœ¨åˆå§‹åŒ– MouthAdapter (Priority 2)...")
                mouth_adapter = MouthAdapter(
                    model_path=settings.model.cosyvoice_model_path
                )
                await mouth_adapter.initialize()
                logger.success("âœ“ Mouth Adapter åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.error(f"âœ— Mouth Adapter å¤±è´¥: {e}")

            # 3. å¬è§‰ (SenseVoice)
            try:
                logger.info("æ­£åœ¨åˆå§‹åŒ– VoiceAdapter...")
                voice_adapter = VoiceAdapter(
                    model_name=settings.model.funasr_model,
                    device=settings.model.funasr_device
                )
                await voice_adapter.initialize()
                logger.success("âœ“ Voice Adapter åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.error(f"âœ— Voice Adapter å¤±è´¥: {e}")
                
            # 4. è¡¨æƒ… (GeneFace)
            try:
                logger.info("æ­£åœ¨åˆå§‹åŒ– DriverAdapter...")
                driver_adapter = DriverAdapter(
                    geneface_path=settings.model.geneface_model_path
                )
                await driver_adapter.initialize()
                logger.success("âœ“ Driver Adapter åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.error(f"âœ— Driver Adapter å¤±è´¥: {e}")
        
    else:
        logger.warning("âš  Debug æ¨¡å¼: è·³è¿‡æ¨¡å‹åŠ è½½")
        # Debug æ¨¡å¼ä½¿ç”¨ Mock
        brain_adapter = BrainAdapter()
        await brain_adapter.initialize(mock=True)
    
    # åˆå§‹åŒ– Layer 3: è‡ªæˆ‘ (EgoDirector)
    if brain_adapter:
        ego_director = EgoDirector(
            brain=brain_adapter,
            bio_state=bio_state,
            narrative_mgr=narrative_mgr
        )
        logger.success("âœ“ Layer 3 (è‡ªæˆ‘) åˆå§‹åŒ–å®Œæˆ")
    
    logger.info("ğŸ­ Project Trinity å‡†å¤‡å°±ç»ª!")
    
    yield
    
    # æ¸…ç†
    logger.info("æ­£åœ¨å…³é—­ Project Trinity...")
    
    if monitor:
        monitor.stop()

    if voice_adapter:
        await voice_adapter.shutdown()
    if brain_adapter:
        await brain_adapter.shutdown()
    if mouth_adapter:
        await mouth_adapter.shutdown()
    if driver_adapter:
        await driver_adapter.shutdown()
    if narrative_mgr:
        await narrative_mgr.shutdown()
    
    logger.info("Project Trinity å·²å…³é—­")


# ============== FastAPI åº”ç”¨ ==============
app = FastAPI(
    title="Project Trinity",
    description="Next-Gen Digital Life Engine",
    version="0.1.0",
    lifespan=lifespan
)

# CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============== æ•°æ®æ¨¡å‹ ==============
class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚"""
    text: str
    emotion: str = "neutral"
    visual_context: Optional[str] = None


class ChatResponse(BaseModel):
    """èŠå¤©å“åº”"""
    response: str
    emotion_tag: str
    action_hints: list
    bio_state: dict


class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”"""
    status: str
    components: dict


# ============== API è·¯ç”± ==============

# æŒ‚è½½ Web å®¢æˆ·ç«¯ (LLM Workbench)
from fastapi.staticfiles import StaticFiles
static_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "client/llm_workbench")
if os.path.exists(static_dir):
    app.mount("/workbench", StaticFiles(directory=static_dir, html=True), name="workbench")
    logger.info(f"Workbench mounted at /workbench -> {static_dir}")

@app.post("/chat/stream")
async def chat_stream(request: ChatRequest, background_tasks: BackgroundTasks):
    """
    æµå¼å¯¹è¯æ¥å£ (Real-Time Reflex)
    ç›´æ¥è¿æ¥ BrainAdapterï¼Œç»•è¿‡ EgoDirector çš„éƒ¨åˆ†é€»è¾‘ä»¥æµ‹è¯•æè‡´é€Ÿåº¦
    """
    if not brain_adapter:
        raise HTTPException(status_code=503, detail="BrainAdapter æœªåˆå§‹åŒ–")
        
    logger.info(f"Stream Request: {request.text[:50]}...")
    
    # å‡†å¤‡æ—¥å¿—æ•°æ®å®¹å™¨ï¼ˆå¯å˜å¯¹è±¡ï¼‰
    log_data = {
        "input": request.text,
        "output": "",
        "ttft": 0,
        "start": time.time()
    }
    
    # æ·»åŠ åå°ä»»åŠ¡ï¼Œåœ¨å“åº”ç»“æŸåæ‰§è¡Œ
    background_tasks.add_task(write_chat_log, log_data)
    
    async def event_generator():
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = log_data["start"]
        first_token_sent = False
        
        try:
            # ç›´æ¥è°ƒç”¨ BrainAdapter çš„æµå¼æ–¹æ³•
            generator = brain_adapter.process_stream(
                user_input=request.text,
                temperature=0.7 
            )
            
            async for chunk in generator:
                if chunk["type"] == "token":
                    content = chunk["content"]
                    log_data["output"] += content # å®æ—¶æ›´æ–°æ—¥å¿—å®¹å™¨
                    
                    yield content
                    
                    if not first_token_sent:
                        first_token_sent = True
                        ttft_ms = (time.time() - start_time) * 1000
                        log_data["ttft"] = ttft_ms
                        logger.info(f"âš¡ Stream TTFT: {ttft_ms:.2f}ms")
                        
                elif chunk["type"] == "error":
                    error_msg = f"[ERROR: {chunk['content']}]"
                    log_data["output"] += error_msg
                    yield error_msg
                    
        except Exception as e:
            logger.error(f"Stream Error: {e}")
            yield f"[SYSTEM ERROR: {str(e)}]"
            
    return StreamingResponse(event_generator(), media_type="text/event-stream")

@app.get("/")
async def root():
    """æ ¹è·¯ç”±"""
    return {
        "name": "Project Trinity",
        "version": "0.1.0",
        "status": "running"
    }


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    components = {
        "bio_state": bio_state is not None,
        "narrative_mgr": narrative_mgr is not None and narrative_mgr.is_initialized,
        "ego_director": ego_director is not None,
        "voice_adapter": voice_adapter is not None and voice_adapter.is_initialized if voice_adapter else False,
        "brain_adapter": brain_adapter is not None and brain_adapter.is_initialized if brain_adapter else False,
    }
    
    status = "healthy" if all(components.values()) else "degraded"
    
    return HealthResponse(status=status, components=components)


@app.post("/avatar/generate")
async def generate_avatar(
    background_tasks: BackgroundTasks,
    image: UploadFile = File(...)
):
    """
    [FastAvatar] ä»ç…§ç‰‡ç”Ÿæˆ 3DGS èµ„äº§
    è¿™æ˜¯ä¸€ä¸ªè€—æ—¶æ“ä½œï¼Œå°†åœ¨åå°è¿è¡Œã€‚
    """
    if not driver_adapter:
        raise HTTPException(status_code=503, detail="DriverAdapter æœªåˆå§‹åŒ–")
        
    upload_dir = Path("uploads")
    upload_dir.mkdir(exist_ok=True)
    
    file_ext = image.filename.split(".")[-1]
    file_id = str(uuid.uuid4())
    image_path = upload_dir / f"{file_id}.{file_ext}"
    output_dir = Path("assets/avatars") / file_id
    output_dir.mkdir(parents=True, exist_ok=True)
    
    with open(image_path, "wb") as buffer:
        shutil.copyfileobj(image.file, buffer)
        
    logger.info(f"æ”¶åˆ° Avatar ç”Ÿæˆè¯·æ±‚: {image.filename} -> {file_id}")
    
    # å¼‚æ­¥æ‰§è¡Œç”Ÿæˆä»»åŠ¡
    async def _run_generation():
        success = await driver_adapter.generate_avatar(str(image_path), str(output_dir))
        if success:
            logger.success(f"Avatar ç”Ÿæˆå®Œæˆ: {file_id}")
            # TODO: é€šçŸ¥å®¢æˆ·ç«¯æˆ–æ›´æ–°æ•°æ®åº“
        else:
            logger.error(f"Avatar ç”Ÿæˆå¤±è´¥: {file_id}")

    background_tasks.add_task(_run_generation)
    
    return {
        "status": "processing", 
        "task_id": file_id,
        "message": "Avatar ç”Ÿæˆä»»åŠ¡å·²æäº¤ï¼Œè¯·ç¨å€™ã€‚"
    }


@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    æ–‡æœ¬å¯¹è¯æ¥å£
    
    ç”¨äºæµ‹è¯•å’Œç®€å•åœºæ™¯
    """
    if ego_director is None:
        raise HTTPException(status_code=503, detail="EgoDirector æœªåˆå§‹åŒ–")
    
    try:
        decision = await ego_director.process(
            user_text=request.text,
            detected_emotion=request.emotion,
            visual_context=request.visual_context
        )
        
        return ChatResponse(
            response=decision.response_text,
            emotion_tag=decision.emotion_tag,
            action_hints=decision.action_hints,
            bio_state={
                "temperature": decision.llm_temperature,
                "triggered_reflex": decision.triggered_reflex
            }
        )
        
    except Exception as e:
        logger.error(f"Chat å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """
    WebSocket å®æ—¶é€šä¿¡
    
    åè®®:
    - Client -> Server: { "type": "audio", "data": base64 } æˆ– { "type": "text", "data": "..." }
    - Server -> Client: { "type": "response", "text": "...", "audio": base64, "flame": [...] }
    """
    await websocket.accept()
    logger.info("WebSocket å®¢æˆ·ç«¯å·²è¿æ¥")
    
    try:
        while True:
            # æ¥æ”¶æ¶ˆæ¯
            data = await websocket.receive_text()
            message = json.loads(data)
            
            msg_type = message.get("type", "text")
            
            if msg_type == "text":
                # æ–‡æœ¬æ¶ˆæ¯
                user_text = message.get("data", "")
                emotion = message.get("emotion", "neutral")
                
                if ego_director:
                    decision = await ego_director.process(
                        user_text=user_text,
                        detected_emotion=emotion
                    )
                    
                    response = {
                        "type": "response",
                        "text": decision.response_text,
                        "emotion": decision.emotion_tag,
                        "actions": decision.action_hints,
                        "reflex": decision.triggered_reflex
                    }
                else:
                    response = {
                        "type": "error",
                        "message": "System not ready"
                    }
                
                await websocket.send_text(json.dumps(response))
            
            elif msg_type == "audio":
                # éŸ³é¢‘æ¶ˆæ¯ (TODO: Phase 1)
                await websocket.send_text(json.dumps({
                    "type": "error",
                    "message": "Audio processing not implemented yet"
                }))
            
            elif msg_type == "heartbeat":
                # å¿ƒè·³
                await websocket.send_text(json.dumps({
                    "type": "heartbeat",
                    "status": "ok"
                }))
    
    except WebSocketDisconnect:
        logger.info("WebSocket å®¢æˆ·ç«¯å·²æ–­å¼€")
    except Exception as e:
        logger.error(f"WebSocket é”™è¯¯: {e}")


# ============== æµ‹è¯•ç«¯ç‚¹ ==============

@app.post("/transcribe")
async def transcribe_audio(file: UploadFile = File(...)):
    """
    è¯­éŸ³è¯†åˆ«æ¥å£ (ASR)
    
    æ¥å—éŸ³é¢‘æ–‡ä»¶ï¼Œè¿”å›è¯†åˆ«æ–‡æœ¬å’Œæƒ…æ„Ÿ
    """
    if not voice_adapter or not voice_adapter.is_initialized:
        raise HTTPException(status_code=503, detail="VoiceAdapter æœªåˆå§‹åŒ–")
    
    try:
        import io
        import wave
        import numpy as np
        
        # è¯»å–ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶
        audio_bytes = await file.read()
        
        # å°è¯•è§£æ WAV æ ¼å¼
        try:
            wav_buffer = io.BytesIO(audio_bytes)
            with wave.open(wav_buffer, 'rb') as wav_file:
                sample_rate = wav_file.getframerate()
                n_frames = wav_file.getnframes()
                audio_data = wav_file.readframes(n_frames)
                # è½¬æ¢ä¸º numpy array
                audio_array = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0
        except Exception:
            # å¦‚æœä¸æ˜¯æ ‡å‡† WAVï¼Œå°è¯•ç›´æ¥ä½œä¸º PCM å¤„ç†
            audio_array = np.frombuffer(audio_bytes, dtype=np.int16).astype(np.float32) / 32768.0
            sample_rate = 16000
        
        # è°ƒç”¨ ASR
        result = await voice_adapter.process(audio_array, sample_rate)
        
        return {
            "text": result.text,
            "emotion": result.emotion,
            "confidence": result.emotion_confidence,
            "language": result.language
        }
        
    except Exception as e:
        logger.error(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/synthesize")
async def synthesize_speech(request: dict):
    """
    è¯­éŸ³åˆæˆæ¥å£ (TTS)
    
    æ¥å—æ–‡æœ¬ï¼Œè¿”å›éŸ³é¢‘æ•°æ®
    """
    if not mouth_adapter:
        raise HTTPException(status_code=503, detail="MouthAdapter æœªåˆå§‹åŒ–")
    
    text = request.get("text", "")
    instruct_text = request.get("instruct_text", "ç”¨æ¸©æŸ”ç”œç¾çš„å¥³å£°è¯´")
    
    if not text:
        raise HTTPException(status_code=400, detail="text is required")
    
    try:
        # è°ƒç”¨ TTS
        result = await mouth_adapter.process(text, instruct_text)
        
        if "error" in result:
            raise HTTPException(status_code=500, detail=result["error"])
        
        # è½¬æ¢ä¸º WAV æ ¼å¼è¿”å›
        import io
        import wave
        import numpy as np
        
        audio_array = np.array(result["audio"], dtype=np.float32)
        sample_rate = result["sample_rate"]
        
        # è½¬æ¢ä¸º 16-bit PCM
        audio_int16 = (audio_array * 32767).astype(np.int16)
        
        # åˆ›å»º WAV æ–‡ä»¶
        wav_buffer = io.BytesIO()
        with wave.open(wav_buffer, 'w') as wav_file:
            wav_file.setnchannels(1)
            wav_file.setsampwidth(2)
            wav_file.setframerate(sample_rate)
            wav_file.writeframes(audio_int16.tobytes())
        
        wav_buffer.seek(0)
        
        from fastapi.responses import StreamingResponse
        return StreamingResponse(
            wav_buffer,
            media_type="audio/wav",
            headers={"Content-Disposition": "attachment; filename=speech.wav"}
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è¯­éŸ³åˆæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============== å¯¹è¯æ—¥å¿— API ==============

@app.get("/logs/dates")
async def list_log_dates():
    """åˆ—å‡ºæ‰€æœ‰æœ‰æ—¥å¿—çš„æ—¥æœŸ"""
    from pathlib import Path
    log_dir = Path("/workspace/project-trinity/project-trinity/logs/conversations")
    
    if not log_dir.exists():
        return {"dates": []}
    
    dates = [f.stem for f in log_dir.glob("*.jsonl")]
    return {"dates": sorted(dates, reverse=True)}


@app.get("/logs/{date}")
async def get_logs_by_date(date: str):
    """
    è·å–æŒ‡å®šæ—¥æœŸçš„å¯¹è¯è®°å½•
    
    Args:
        date: æ—¥æœŸ YYYY-MM-DD
    """
    import json
    from pathlib import Path
    
    log_file = Path(f"/workspace/project-trinity/project-trinity/logs/conversations/{date}.jsonl")
    
    if not log_file.exists():
        return {"date": date, "conversations": [], "count": 0}
    
    conversations = []
    with open(log_file, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                conversations.append(json.loads(line))
    
    return {
        "date": date,
        "conversations": conversations,
        "count": len(conversations)
    }


@app.get("/logs/today")
async def get_today_logs():
    """è·å–ä»Šå¤©çš„å¯¹è¯è®°å½•"""
    today = datetime.now().strftime("%Y-%m-%d")
    return await get_logs_by_date(today)


# ============== ä¸»å…¥å£ ==============
if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host=settings.server.host,
        port=settings.server.port,
        reload=settings.server.debug
    )
