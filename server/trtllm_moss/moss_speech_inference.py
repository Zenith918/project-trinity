#!/usr/bin/env python3
"""
MOSS-Speech å®Œæ•´æ¨ç†æµ‹è¯•
========================

ä½¿ç”¨å‚è€ƒéŸ³é¢‘æ§åˆ¶æƒ…ç»ªï¼Œç”Ÿæˆæ–°çš„è¯­éŸ³ï¼Œå¹¶åˆ†æè´¨é‡ã€‚

æµç¨‹ï¼š
1. å‚è€ƒéŸ³é¢‘ â†’ Audio Codec ç¼–ç  â†’ é£æ ¼ç‰¹å¾
2. æ–‡æœ¬ + é£æ ¼ç‰¹å¾ â†’ MOSS-Speech â†’ Audio Tokens
3. Audio Tokens + å‚è€ƒéŸ³é¢‘ â†’ Audio Codec è§£ç  â†’ ç”ŸæˆéŸ³é¢‘
4. åˆ†æç”ŸæˆéŸ³é¢‘çš„è´¨é‡
"""

import os
import sys
import time
import json
import torch
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass

# æ·»åŠ è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))


@dataclass
class InferenceResult:
    """æ¨ç†ç»“æœ"""
    emotion: str
    text: str
    prompt_path: str
    output_path: str
    duration_ms: float
    audio_tokens: Optional[torch.Tensor] = None
    success: bool = True
    error: Optional[str] = None


# æµ‹è¯•é…ç½®
TEST_CONFIGS = [
    {
        "emotion": "æ¸©æŸ”",
        "emotion_en": "gentle",
        "prompt_path": "/workspace/audio_benchmark/prompts/gentle.wav",
        "text": "ä½ å¥½ï¼Œå¾ˆé«˜å…´è®¤è¯†ä½ ã€‚ä»Šå¤©çš„å¤©æ°”çœŸä¸é”™å‘¢ã€‚",
    },
    {
        "emotion": "ç„¦æ€¥",
        "emotion_en": "anxious",
        "prompt_path": "/workspace/audio_benchmark/prompts/anxious.wav",
        "text": "æ€ä¹ˆåŠæ€ä¹ˆåŠï¼Œæ—¶é—´æ¥ä¸åŠäº†ï¼æˆ‘ä»¬å¿«ç‚¹èµ°ï¼",
    },
    {
        "emotion": "å¼€å¿ƒ",
        "emotion_en": "happy",
        "prompt_path": "/workspace/audio_benchmark/prompts/happy.wav",
        "text": "å“‡ï¼Œè¿™ä¸ªç¤¼ç‰©å¤ªæ£’äº†ï¼è°¢è°¢ä½ ï¼Œæˆ‘å¥½å–œæ¬¢ï¼",
    },
    {
        "emotion": "å¤±è½",
        "emotion_en": "sad",
        "prompt_path": "/workspace/audio_benchmark/prompts/sad.wav",
        "text": "æ²¡å…³ç³»çš„ï¼Œæˆ‘å·²ç»ä¹ æƒ¯äº†ã€‚ä¸€ä¸ªäººä¹ŸæŒºå¥½çš„ã€‚",
    },
    {
        "emotion": "å†·é™",
        "emotion_en": "calm",
        "prompt_path": "/workspace/audio_benchmark/prompts/calm.wav",
        "text": "è®©æˆ‘æ¥åˆ†æä¸€ä¸‹è¿™ä¸ªé—®é¢˜çš„æœ¬è´¨å’Œè§£å†³æ–¹æ¡ˆã€‚",
    },
]


def load_moss_speech_model():
    """åŠ è½½ MOSS-Speech åŸå§‹æ¨¡å‹ï¼ˆHuggingFaceï¼‰"""
    print("=" * 60)
    print("[åŠ è½½ MOSS-Speech æ¨¡å‹]")
    print("=" * 60)
    
    from transformers import AutoModel, AutoTokenizer
    
    model_path = "/workspace/models/MOSS-Speech"
    
    print(f"  æ¨¡å‹è·¯å¾„: {model_path}")
    print("  åŠ è½½ä¸­...")
    
    start = time.perf_counter()
    
    # åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    
    # åŠ è½½æ¨¡å‹
    model = AutoModel.from_pretrained(
        model_path,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="cuda"
    )
    model.eval()
    
    load_time = time.perf_counter() - start
    print(f"  âœ… åŠ è½½å®Œæˆ: {load_time:.1f}s")
    
    return model, tokenizer


def load_moss_speech_processor():
    """åŠ è½½ MOSS-Speech Processor (éœ€è¦ codec_path)"""
    print("\n[åŠ è½½ Processor]")
    
    sys.path.insert(0, "/workspace/models/MOSS-Speech")
    
    from processing_moss_speech import MossSpeechProcessor
    
    # éœ€è¦æŒ‡å®š codec_path
    processor = MossSpeechProcessor.from_pretrained(
        "/workspace/models/MOSS-Speech",
        codec_path="/workspace/models/MOSS-Speech-Codec",
        trust_remote_code=True,
        device="cuda"
    )
    
    print("  âœ… Processor åŠ è½½å®Œæˆ (å« Audio Codec)")
    return processor


@torch.inference_mode()
def run_inference(
    model,
    processor,
    text: str,
    prompt_path: str,
    output_path: str,
    max_new_tokens: int = 500,
) -> InferenceResult:
    """
    è¿è¡Œ MOSS-Speech æ¨ç†
    
    Args:
        model: MOSS-Speech æ¨¡å‹
        processor: MOSS-Speech Processor
        text: è¦åˆæˆçš„æ–‡æœ¬
        prompt_path: å‚è€ƒéŸ³é¢‘è·¯å¾„
        output_path: è¾“å‡ºéŸ³é¢‘è·¯å¾„
    """
    try:
        start = time.perf_counter()
        
        # å‡†å¤‡è¾“å…¥
        conversation = [
            {"role": "user", "content": text}
        ]
        
        # ç¼–ç è¾“å…¥
        inputs = processor(
            conversation,
            output_modality="speech",
            return_tensors="pt"
        )
        
        # ç§»åŠ¨åˆ° GPU
        inputs = {k: v.cuda() if isinstance(v, torch.Tensor) else v 
                  for k, v in inputs.items()}
        
        # ç”Ÿæˆ
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.8,
            top_p=0.9,
        )
        
        # è§£ç éŸ³é¢‘
        responses = processor.batch_decode(
            outputs,
            decoder_audio_prompt_path=prompt_path
        )
        
        # ä¿å­˜éŸ³é¢‘
        if responses and responses[0].audio is not None:
            import soundfile as sf
            audio = responses[0].audio.squeeze().numpy()
            sf.write(output_path, audio, responses[0].sampling_rate)
        
        duration_ms = (time.perf_counter() - start) * 1000
        
        return InferenceResult(
            emotion="",
            text=text,
            prompt_path=prompt_path,
            output_path=output_path,
            duration_ms=duration_ms,
            success=True
        )
        
    except Exception as e:
        return InferenceResult(
            emotion="",
            text=text,
            prompt_path=prompt_path,
            output_path=output_path,
            duration_ms=0,
            success=False,
            error=str(e)
        )


def run_all_tests(model, processor, output_dir: str) -> List[InferenceResult]:
    """è¿è¡Œæ‰€æœ‰æƒ…ç»ªæµ‹è¯•"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    results = []
    
    print("\n" + "=" * 60)
    print("[å¼€å§‹æ¨ç†æµ‹è¯•]")
    print("=" * 60)
    
    for i, config in enumerate(TEST_CONFIGS):
        print(f"\n--- [{i+1}/{len(TEST_CONFIGS)}] {config['emotion']} ({config['emotion_en']}) ---")
        print(f"  æ–‡æœ¬: {config['text']}")
        print(f"  å‚è€ƒ: {config['prompt_path']}")
        
        output_file = output_path / f"generated_{config['emotion_en']}.wav"
        
        result = run_inference(
            model=model,
            processor=processor,
            text=config['text'],
            prompt_path=config['prompt_path'],
            output_path=str(output_file),
        )
        
        result.emotion = config['emotion']
        
        if result.success:
            print(f"  âœ… ç”ŸæˆæˆåŠŸ: {result.duration_ms:.0f}ms")
            print(f"  ğŸ“ è¾“å‡º: {output_file}")
        else:
            print(f"  âŒ ç”Ÿæˆå¤±è´¥: {result.error}")
        
        results.append(result)
    
    return results


def analyze_generated_audio(results: List[InferenceResult]):
    """åˆ†æç”Ÿæˆçš„éŸ³é¢‘è´¨é‡"""
    print("\n" + "=" * 60)
    print("[åˆ†æç”ŸæˆéŸ³é¢‘è´¨é‡]")
    print("=" * 60)
    
    try:
        from audio_quality_benchmark import AudioQualityAnalyzer
        analyzer = AudioQualityAnalyzer(sample_rate=24000)
        
        for result in results:
            if result.success and Path(result.output_path).exists():
                print(f"\nåˆ†æ: {result.emotion}")
                analysis = analyzer.analyze(result.output_path, result.emotion)
                
                # ç”Ÿæˆå›¾è¡¨
                plot_path = result.output_path.replace('.wav', '_analysis.png')
                analyzer.plot_analysis(result.output_path, plot_path, result.emotion)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = analyzer.generate_report()
        print(report)
        
        return analyzer.results
        
    except Exception as e:
        print(f"åˆ†æå¤±è´¥: {e}")
        return {}


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("MOSS-Speech æƒ…æ„Ÿè¯­éŸ³ç”Ÿæˆæµ‹è¯•")
    print("=" * 60)
    
    OUTPUT_DIR = "/workspace/audio_benchmark/generated"
    
    # æ£€æŸ¥å‚è€ƒéŸ³é¢‘
    print("\n[æ£€æŸ¥å‚è€ƒéŸ³é¢‘]")
    for config in TEST_CONFIGS:
        exists = "âœ…" if Path(config['prompt_path']).exists() else "âŒ"
        print(f"  {exists} {config['emotion']}: {config['prompt_path']}")
    
    # åŠ è½½æ¨¡å‹
    try:
        model, tokenizer = load_moss_speech_model()
        processor = load_moss_speech_processor()
    except Exception as e:
        print(f"\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("\nä½¿ç”¨ç®€åŒ–æ¨¡å¼ï¼ˆä»…æµ‹è¯• TRT-LLM Engineï¼‰...")
        
        # ä½¿ç”¨ TRT-LLM Engine è¿›è¡Œç®€åŒ–æµ‹è¯•
        run_trtllm_test(OUTPUT_DIR)
        return
    
    # è¿è¡Œæµ‹è¯•
    results = run_all_tests(model, processor, OUTPUT_DIR)
    
    # åˆ†æç»“æœ
    analysis = analyze_generated_audio(results)
    
    # ç”Ÿæˆæ±‡æ€»
    print("\n" + "=" * 60)
    print("[æ±‡æ€»]")
    print("=" * 60)
    
    success_count = sum(1 for r in results if r.success)
    print(f"  æˆåŠŸ: {success_count}/{len(results)}")
    
    if success_count > 0:
        avg_time = np.mean([r.duration_ms for r in results if r.success])
        print(f"  å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_time:.0f}ms")
    
    print(f"\n  ç”Ÿæˆçš„éŸ³é¢‘ä¿å­˜åœ¨: {OUTPUT_DIR}/")


def run_trtllm_test(output_dir: str):
    """ä½¿ç”¨ TRT-LLM Engine è¿›è¡Œç®€åŒ–æµ‹è¯•"""
    print("\n[TRT-LLM Engine ç®€åŒ–æµ‹è¯•]")
    
    from moss_paged_runtime import MossSpeechPagedRuntime
    
    # åŠ è½½ Engine
    runtime = MossSpeechPagedRuntime("/workspace/models/MOSS-Speech-Engine")
    runtime.load()
    
    # æµ‹è¯•æ¨ç†
    vocab_size = runtime.config.get('pretrained_config', {}).get('vocab_size', 151680)
    
    print("\næµ‹è¯•ä¸åŒé•¿åº¦è¾“å…¥...")
    for seq_len in [128, 256, 512]:
        input_ids = torch.randint(0, vocab_size, (1, seq_len), dtype=torch.int32, device='cuda')
        result = runtime.infer(input_ids)
        
        print(f"  seq_len={seq_len}: prefill={result.prefill_time_ms:.1f}ms, "
              f"logits_valid={result.logits_valid}, audio_logits_valid={result.audio_logits_valid}")


if __name__ == "__main__":
    main()

