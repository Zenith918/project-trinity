#!/usr/bin/env python3
"""
MOSS-Speech + BigVGAN-v2 éŸ³è´¨ä¸æƒ…æ„ŸåŸºå‡†è¯„ä¼°
==========================================

é¦–å¸­ç ”ç©¶å‘˜ä»»åŠ¡ï¼š
1. ç”Ÿæˆ 5 ç»„ä¸åŒæƒ…æ„Ÿè¯çš„éŸ³é¢‘ï¼ˆæ¸©æŸ”ã€ç„¦æ€¥ã€å¼€å¿ƒã€å¤±è½ã€å†·é™ï¼‰
2. è‡ªåŠ¨åŒ–è´¨é‡åˆ†æï¼ˆMCDã€F0ã€é¢‘è°±æ£€æŸ¥ï¼‰
3. æ€§èƒ½ä¸éŸ³è´¨å¹³è¡¡ç›‘æµ‹

é‡è¦å‘ç°ï¼š
MOSS-Speech ä½¿ç”¨å‚è€ƒéŸ³é¢‘ (prompt_speech) æ§åˆ¶æƒ…ç»ªå’ŒéŸ³è‰²ï¼
éœ€è¦å‡†å¤‡ 5 ç§æƒ…ç»ªçš„é«˜è´¨é‡å‚è€ƒéŸ³é¢‘ã€‚
"""

import os
import sys
import json
import time
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import warnings
warnings.filterwarnings('ignore')

# éŸ³é¢‘å¤„ç†åº“
try:
    import librosa
    import librosa.display
    import soundfile as sf
    HAS_LIBROSA = True
except ImportError:
    HAS_LIBROSA = False
    print("âš ï¸ librosa æœªå®‰è£…ï¼Œéƒ¨åˆ†åŠŸèƒ½ä¸å¯ç”¨")

try:
    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False

try:
    import torch
    import torchaudio
    HAS_TORCH = True
except ImportError:
    HAS_TORCH = False


@dataclass
class EmotionPrompt:
    """æƒ…ç»ªå‚è€ƒéŸ³é¢‘é…ç½®"""
    name: str           # æƒ…ç»ªåç§°
    name_en: str        # è‹±æ–‡åç§°
    text: str           # æµ‹è¯•æ–‡æœ¬
    prompt_path: str    # å‚è€ƒéŸ³é¢‘è·¯å¾„
    description: str    # æè¿°


# 5ç§æƒ…ç»ªé…ç½®
EMOTION_PROMPTS = [
    EmotionPrompt(
        name="æ¸©æŸ”",
        name_en="gentle",
        text="äº²çˆ±çš„ï¼Œä»Šå¤©è¿‡å¾—æ€ä¹ˆæ ·ï¼Ÿæˆ‘ä¸€ç›´åœ¨æƒ³ä½ ã€‚",
        prompt_path="prompts/gentle.wav",
        description="è½»æŸ”ã€æ¸©æš–ã€å…³æ€€çš„è¯­æ°”"
    ),
    EmotionPrompt(
        name="ç„¦æ€¥",
        name_en="anxious",
        text="å¿«ç‚¹å¿«ç‚¹ï¼Œæˆ‘ä»¬è¦è¿Ÿåˆ°äº†ï¼è½¦é©¬ä¸Šå°±å¼€äº†ï¼",
        prompt_path="prompts/anxious.wav",
        description="ç´§å¼ ã€æ€¥ä¿ƒã€è¯­é€Ÿå¿«"
    ),
    EmotionPrompt(
        name="å¼€å¿ƒ",
        name_en="happy",
        text="å¤ªæ£’äº†ï¼æˆ‘ä»¬ä¸­å¥–äº†ï¼è¿™æ˜¯æˆ‘ä»Šå¹´æœ€å¼€å¿ƒçš„ä¸€å¤©ï¼",
        prompt_path="prompts/happy.wav",
        description="æ¬¢å¿«ã€æ´»æ³¼ã€è¯­è°ƒä¸Šæ‰¬"
    ),
    EmotionPrompt(
        name="å¤±è½",
        name_en="sad",
        text="ç®—äº†ï¼Œå¯èƒ½è¿™å°±æ˜¯å‘½å§ã€‚æˆ‘ä¹Ÿä¸çŸ¥é“è¯¥æ€ä¹ˆåŠäº†ã€‚",
        prompt_path="prompts/sad.wav",
        description="ä½æ²‰ã€ç¼“æ…¢ã€ç•¥å¸¦å¹æ¯"
    ),
    EmotionPrompt(
        name="å†·é™",
        name_en="calm",
        text="æ ¹æ®ç›®å‰çš„æ•°æ®åˆ†æï¼Œæˆ‘è®¤ä¸ºæœ€ä½³æ–¹æ¡ˆæ˜¯è¿™æ ·çš„ã€‚",
        prompt_path="prompts/calm.wav",
        description="å¹³ç¨³ã€ç†æ€§ã€ä¸“ä¸š"
    ),
]


class AudioQualityAnalyzer:
    """éŸ³é¢‘è´¨é‡åˆ†æå™¨"""
    
    def __init__(self, sample_rate: int = 22050):
        self.sample_rate = sample_rate
        self.results: Dict = {}
        
    def analyze(self, audio_path: str, label: str) -> Dict:
        """
        å…¨é¢åˆ†æéŸ³é¢‘è´¨é‡
        
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        if not HAS_LIBROSA:
            return {"error": "librosa not installed"}
        
        # åŠ è½½éŸ³é¢‘
        y, sr = librosa.load(audio_path, sr=self.sample_rate)
        duration = len(y) / sr
        
        result = {
            "label": label,
            "duration_s": duration,
            "sample_rate": sr,
        }
        
        # 1. èƒ½é‡åˆ†æ
        result["energy"] = self._analyze_energy(y)
        
        # 2. F0 (åŸºé¢‘) åˆ†æ
        result["f0"] = self._analyze_f0(y, sr)
        
        # 3. é¢‘è°±åˆ†æ
        result["spectral"] = self._analyze_spectral(y, sr)
        
        # 4. è´¨é‡æŒ‡æ ‡
        result["quality"] = self._analyze_quality(y, sr)
        
        self.results[label] = result
        return result
    
    def _analyze_energy(self, y: np.ndarray) -> Dict:
        """èƒ½é‡åˆ†æ - æ£€æµ‹çˆ†éŸ³å’Œé™éŸ³"""
        rms = librosa.feature.rms(y=y)[0]
        
        # æ£€æµ‹å¼‚å¸¸å³°å€¼ï¼ˆçˆ†éŸ³ï¼‰
        threshold = np.mean(rms) + 3 * np.std(rms)
        spikes = np.sum(rms > threshold)
        
        # æ£€æµ‹é™éŸ³
        silence_threshold = 0.01
        silence_ratio = np.sum(rms < silence_threshold) / len(rms)
        
        return {
            "mean_rms": float(np.mean(rms)),
            "max_rms": float(np.max(rms)),
            "min_rms": float(np.min(rms)),
            "std_rms": float(np.std(rms)),
            "spike_count": int(spikes),
            "silence_ratio": float(silence_ratio),
            "has_spikes": spikes > 5,
            "has_long_silence": silence_ratio > 0.3,
        }
    
    def _analyze_f0(self, y: np.ndarray, sr: int) -> Dict:
        """F0 (åŸºé¢‘) åˆ†æ - æ£€æµ‹éŸ³è°ƒç¨³å®šæ€§"""
        # æå– F0
        f0, voiced_flag, voiced_probs = librosa.pyin(
            y, fmin=50, fmax=500, sr=sr
        )
        
        # è¿‡æ»¤æ— æ•ˆå€¼
        f0_valid = f0[~np.isnan(f0)]
        
        if len(f0_valid) == 0:
            return {"error": "no valid f0"}
        
        # è®¡ç®— F0 ç»Ÿè®¡
        f0_mean = float(np.mean(f0_valid))
        f0_std = float(np.std(f0_valid))
        f0_range = float(np.max(f0_valid) - np.min(f0_valid))
        
        # æ£€æµ‹ F0 è·³è·ƒ
        f0_diff = np.abs(np.diff(f0_valid))
        jumps = np.sum(f0_diff > 50)  # >50Hz è§†ä¸ºè·³è·ƒ
        
        # F0 å¹³ç›´åº¦ï¼ˆæœºå™¨äººæ„Ÿï¼‰
        flatness = f0_std / f0_mean if f0_mean > 0 else 0
        is_robotic = flatness < 0.05  # å˜åŒ–å¤ªå°
        
        return {
            "mean_hz": f0_mean,
            "std_hz": f0_std,
            "range_hz": f0_range,
            "jump_count": int(jumps),
            "flatness": float(flatness),
            "voiced_ratio": float(np.mean(voiced_probs[~np.isnan(voiced_probs)])),
            "is_robotic": is_robotic,
            "has_jumps": jumps > 10,
        }
    
    def _analyze_spectral(self, y: np.ndarray, sr: int) -> Dict:
        """é¢‘è°±åˆ†æ - æ£€æµ‹çˆ†éŸ³æ¡çº¹å’Œç©ºæ´"""
        # è®¡ç®— Mel é¢‘è°±
        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80)
        mel_db = librosa.power_to_db(mel_spec, ref=np.max)
        
        # é«˜é¢‘èƒ½é‡æ¯”ä¾‹
        high_freq_ratio = np.mean(mel_db[60:, :]) / np.mean(mel_db[:20, :])
        
        # æ£€æµ‹å‚ç›´æ¡çº¹ï¼ˆå¸§é—´å·®å¼‚è¿‡å¤§ï¼‰
        frame_diff = np.abs(np.diff(mel_db, axis=1))
        stripe_score = np.mean(frame_diff > 10)
        
        # æ£€æµ‹ç©ºæ´ï¼ˆè¿ç»­ä½èƒ½é‡åŒºåŸŸï¼‰
        low_energy_mask = mel_db < -60
        hole_ratio = np.mean(low_energy_mask)
        
        return {
            "high_freq_ratio": float(high_freq_ratio),
            "stripe_score": float(stripe_score),
            "hole_ratio": float(hole_ratio),
            "has_stripes": stripe_score > 0.1,
            "has_holes": hole_ratio > 0.3,
        }
    
    def _analyze_quality(self, y: np.ndarray, sr: int) -> Dict:
        """ç»¼åˆè´¨é‡æŒ‡æ ‡"""
        # ä¿¡å™ªæ¯”ä¼°è®¡
        signal_power = np.mean(y ** 2)
        noise_est = np.mean(np.abs(y[y < np.percentile(np.abs(y), 10)]) ** 2)
        snr_db = 10 * np.log10(signal_power / (noise_est + 1e-10))
        
        # å‰Šæ³¢æ£€æµ‹
        clip_threshold = 0.95
        clip_ratio = np.mean(np.abs(y) > clip_threshold)
        
        # é›¶äº¤å‰ç‡ï¼ˆå™ªå£°æŒ‡æ ‡ï¼‰
        zcr = librosa.feature.zero_crossing_rate(y)[0]
        zcr_mean = np.mean(zcr)
        
        return {
            "snr_db": float(snr_db),
            "clip_ratio": float(clip_ratio),
            "zcr_mean": float(zcr_mean),
            "is_clipped": clip_ratio > 0.01,
            "is_noisy": zcr_mean > 0.2,
        }
    
    def plot_analysis(self, audio_path: str, output_path: str, label: str):
        """ç”Ÿæˆåˆ†æå›¾è¡¨"""
        if not HAS_MATPLOTLIB or not HAS_LIBROSA:
            return
        
        y, sr = librosa.load(audio_path, sr=self.sample_rate)
        
        fig, axes = plt.subplots(4, 1, figsize=(12, 10))
        fig.suptitle(f"Audio Quality Analysis: {label}", fontsize=14)
        
        # 1. æ³¢å½¢
        axes[0].set_title("Waveform")
        librosa.display.waveshow(y, sr=sr, ax=axes[0])
        axes[0].set_xlabel("Time (s)")
        
        # 2. Mel é¢‘è°±
        axes[1].set_title("Mel Spectrogram")
        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=80)
        mel_db = librosa.power_to_db(mel_spec, ref=np.max)
        img = librosa.display.specshow(
            mel_db, sr=sr, x_axis='time', y_axis='mel', ax=axes[1]
        )
        fig.colorbar(img, ax=axes[1], format='%+2.0f dB')
        
        # 3. F0 æ›²çº¿
        axes[2].set_title("F0 (Pitch) Curve")
        f0, _, _ = librosa.pyin(y, fmin=50, fmax=500, sr=sr)
        times = librosa.times_like(f0, sr=sr)
        axes[2].plot(times, f0, label='F0', color='blue')
        axes[2].set_xlabel("Time (s)")
        axes[2].set_ylabel("Frequency (Hz)")
        axes[2].legend()
        
        # 4. RMS èƒ½é‡
        axes[3].set_title("RMS Energy")
        rms = librosa.feature.rms(y=y)[0]
        times = librosa.times_like(rms, sr=sr)
        axes[3].plot(times, rms, label='RMS', color='green')
        axes[3].set_xlabel("Time (s)")
        axes[3].set_ylabel("RMS")
        axes[3].legend()
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=150)
        plt.close()
        
        print(f"  ğŸ“Š Saved plot: {output_path}")
    
    def generate_report(self) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report = []
        report.append("=" * 60)
        report.append("MOSS-Speech éŸ³è´¨åˆ†ææŠ¥å‘Š")
        report.append("=" * 60)
        
        for label, result in self.results.items():
            report.append(f"\n### {label}")
            report.append(f"æ—¶é•¿: {result['duration_s']:.2f}s")
            
            # èƒ½é‡åˆ†æ
            energy = result.get('energy', {})
            report.append(f"\n[èƒ½é‡åˆ†æ]")
            report.append(f"  å¹³å‡ RMS: {energy.get('mean_rms', 0):.4f}")
            report.append(f"  çˆ†éŸ³æ•°: {energy.get('spike_count', 0)}")
            report.append(f"  é™éŸ³æ¯”: {energy.get('silence_ratio', 0):.1%}")
            
            # F0 åˆ†æ
            f0 = result.get('f0', {})
            report.append(f"\n[F0 (åŸºé¢‘) åˆ†æ]")
            report.append(f"  å¹³å‡: {f0.get('mean_hz', 0):.1f} Hz")
            report.append(f"  æ ‡å‡†å·®: {f0.get('std_hz', 0):.1f} Hz")
            report.append(f"  è·³è·ƒæ•°: {f0.get('jump_count', 0)}")
            report.append(f"  æœºå™¨äººæ„Ÿ: {'âš ï¸ æ˜¯' if f0.get('is_robotic') else 'âœ… å¦'}")
            
            # é¢‘è°±åˆ†æ
            spec = result.get('spectral', {})
            report.append(f"\n[é¢‘è°±åˆ†æ]")
            report.append(f"  é«˜é¢‘æ¯”: {spec.get('high_freq_ratio', 0):.2f}")
            report.append(f"  æ¡çº¹åˆ†æ•°: {spec.get('stripe_score', 0):.2%}")
            report.append(f"  ç©ºæ´æ¯”: {spec.get('hole_ratio', 0):.2%}")
            
            # è´¨é‡æŒ‡æ ‡
            quality = result.get('quality', {})
            report.append(f"\n[è´¨é‡æŒ‡æ ‡]")
            report.append(f"  ä¿¡å™ªæ¯”: {quality.get('snr_db', 0):.1f} dB")
            report.append(f"  å‰Šæ³¢: {'âš ï¸ æ˜¯' if quality.get('is_clipped') else 'âœ… å¦'}")
            report.append(f"  å™ªå£°: {'âš ï¸ æ˜¯' if quality.get('is_noisy') else 'âœ… å¦'}")
        
        return "\n".join(report)


class EmotionAudioDownloader:
    """æƒ…ç»ªå‚è€ƒéŸ³é¢‘ä¸‹è½½å™¨"""
    
    # å¼€æºæƒ…ç»ªéŸ³é¢‘æ•°æ®é›† URLs
    EMOTION_DATASETS = {
        "LibriTTS": "https://www.openslr.org/60/",
        "RAVDESS": "https://zenodo.org/record/1188976",
        "EmoV-DB": "https://github.com/numediart/EmoV-DB",
    }
    
    @staticmethod
    def download_sample_prompts(output_dir: str) -> Dict[str, str]:
        """
        ä¸‹è½½ç¤ºä¾‹å‚è€ƒéŸ³é¢‘
        
        ç”±äºç‰ˆæƒé—®é¢˜ï¼Œè¿™é‡Œç”Ÿæˆåˆæˆçš„ç¤ºä¾‹éŸ³é¢‘
        å®é™…ä½¿ç”¨æ—¶åº”è¯¥å½•åˆ¶æˆ–è´­ä¹°ä¸“ä¸šé…éŸ³
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        print("\n=== ç”Ÿæˆç¤ºä¾‹å‚è€ƒéŸ³é¢‘ ===")
        print("æ³¨æ„ï¼šè¿™äº›æ˜¯åˆæˆçš„å ä½éŸ³é¢‘")
        print("å®é™…éƒ¨ç½²éœ€è¦å½•åˆ¶é«˜è´¨é‡çš„æƒ…ç»ªå‚è€ƒéŸ³é¢‘\n")
        
        downloaded = {}
        
        for emotion in EMOTION_PROMPTS:
            audio_path = output_path / f"{emotion.name_en}.wav"
            
            # ç”Ÿæˆä¸åŒæƒ…ç»ªçš„ç¤ºä¾‹æ³¢å½¢
            duration = 3.0
            sr = 22050
            t = np.linspace(0, duration, int(duration * sr))
            
            # æ ¹æ®æƒ…ç»ªç”Ÿæˆä¸åŒç‰¹å¾çš„éŸ³é¢‘
            if emotion.name_en == "gentle":
                # æ¸©æŸ”ï¼šä½é¢‘ã€å¹³æ»‘
                freq = 200
                y = 0.3 * np.sin(2 * np.pi * freq * t)
                y *= np.exp(-t / 2)  # æ¸å¼±
                
            elif emotion.name_en == "anxious":
                # ç„¦æ€¥ï¼šé«˜é¢‘ã€å¿«é€Ÿæ³¢åŠ¨
                freq = 350
                y = 0.4 * np.sin(2 * np.pi * freq * t * (1 + 0.1 * np.sin(10 * t)))
                
            elif emotion.name_en == "happy":
                # å¼€å¿ƒï¼šæ˜äº®ã€ä¸Šæ‰¬
                freq = 300
                y = 0.4 * np.sin(2 * np.pi * freq * (1 + t/10) * t)
                
            elif emotion.name_en == "sad":
                # å¤±è½ï¼šä½æ²‰ã€æ¸å¼±
                freq = 150
                y = 0.25 * np.sin(2 * np.pi * freq * t)
                y *= np.exp(-t / 1.5)
                
            else:  # calm
                # å†·é™ï¼šç¨³å®šã€ä¸­é¢‘
                freq = 250
                y = 0.35 * np.sin(2 * np.pi * freq * t)
            
            # æ·»åŠ è½»å¾®å™ªå£°ä½¿å…¶æ›´è‡ªç„¶
            y += 0.01 * np.random.randn(len(y))
            y = np.clip(y, -1, 1).astype(np.float32)
            
            # ä¿å­˜
            sf.write(audio_path, y, sr)
            downloaded[emotion.name_en] = str(audio_path)
            
            print(f"  âœ… {emotion.name} ({emotion.name_en}): {audio_path}")
        
        return downloaded


def generate_test_audios(output_dir: str = "/workspace/audio_benchmark"):
    """
    ç”Ÿæˆæµ‹è¯•éŸ³é¢‘ï¼ˆä½¿ç”¨ BigVGAN åˆæˆï¼‰
    
    æ³¨æ„ï¼šå®Œæ•´çš„ MOSS-Speech æµç¨‹éœ€è¦ï¼š
    1. å‚è€ƒéŸ³é¢‘ â†’ Audio Codec ç¼–ç 
    2. æ–‡æœ¬ + å‚è€ƒ Token â†’ MOSS-Speech ç”Ÿæˆ Audio Token
    3. Audio Token + å‚è€ƒéŸ³é¢‘ â†’ Audio Codec è§£ç  â†’ æ³¢å½¢
    4. æ³¢å½¢ â†’ BigVGAN åå¤„ç†ï¼ˆå¯é€‰ï¼‰
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # 1. å‡†å¤‡å‚è€ƒéŸ³é¢‘
    prompts_dir = output_path / "prompts"
    downloaded = EmotionAudioDownloader.download_sample_prompts(str(prompts_dir))
    
    # 2. åˆ†æå‚è€ƒéŸ³é¢‘è´¨é‡
    analyzer = AudioQualityAnalyzer()
    
    print("\n=== åˆ†æå‚è€ƒéŸ³é¢‘ ===")
    for emotion in EMOTION_PROMPTS:
        audio_path = downloaded.get(emotion.name_en)
        if audio_path and Path(audio_path).exists():
            result = analyzer.analyze(audio_path, f"{emotion.name} (å‚è€ƒ)")
            
            # ç”Ÿæˆåˆ†æå›¾è¡¨
            plot_path = output_path / f"analysis_{emotion.name_en}.png"
            analyzer.plot_analysis(audio_path, str(plot_path), emotion.name)
    
    # 3. ç”ŸæˆæŠ¥å‘Š
    report = analyzer.generate_report()
    report_path = output_path / "quality_report.txt"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    print(report)
    
    return {
        "prompts": downloaded,
        "report_path": str(report_path),
        "output_dir": str(output_path),
    }


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("MOSS-Speech éŸ³è´¨åŸºå‡†è¯„ä¼°")
    print("=" * 60)
    
    print("\nâš ï¸ é‡è¦æç¤ºï¼š")
    print("MOSS-Speech éœ€è¦å‚è€ƒéŸ³é¢‘ (prompt_speech) æ¥æ§åˆ¶æƒ…ç»ªå’ŒéŸ³è‰²ï¼")
    print("\næ¨èçš„å‚è€ƒéŸ³é¢‘æ¥æºï¼š")
    print("1. è‡ªè¡Œå½•åˆ¶é«˜è´¨é‡é…éŸ³ï¼ˆæ¨èï¼‰")
    print("2. ä½¿ç”¨å¼€æºæ•°æ®é›†ï¼š")
    print("   - RAVDESS (æƒ…æ„Ÿè¯­éŸ³æ•°æ®åº“)")
    print("   - EmoV-DB (æƒ…ç»ªè¯­éŸ³æ•°æ®åº“)")
    print("   - LibriTTS (é«˜è´¨é‡ TTS æ•°æ®)")
    print("3. è´­ä¹°ä¸“ä¸šé…éŸ³ç´ æ")
    
    # ç”Ÿæˆæµ‹è¯•éŸ³é¢‘å’Œåˆ†æ
    results = generate_test_audios()
    
    print("\n" + "=" * 60)
    print("[ä¸‹ä¸€æ­¥]")
    print("=" * 60)
    print("1. å½•åˆ¶æˆ–ä¸‹è½½ 5 ç§æƒ…ç»ªçš„é«˜è´¨é‡å‚è€ƒéŸ³é¢‘ (3-5ç§’)")
    print("2. å°†å‚è€ƒéŸ³é¢‘æ”¾å…¥ /workspace/audio_benchmark/prompts/")
    print("3. å‘½åæ ¼å¼: gentle.wav, anxious.wav, happy.wav, sad.wav, calm.wav")
    print("4. é‡æ–°è¿è¡Œæ­¤è„šæœ¬è¿›è¡Œå®Œæ•´è¯„ä¼°")


if __name__ == "__main__":
    main()

