"""
MOSS-Speech æµå¼æ¨ç†
=====================

ç ”ç©¶å‘˜æ–¹æ¡ˆ:
- first_chunk_size=5 å®ç°æé€Ÿé¦–åŒ…å“åº”
- Token æµå¼å–‚ç»™å£°ç å™¨
- å¼‚æ­¥å¹¶è¡Œ: è§£ç  Chunk N æ—¶ç”Ÿæˆ Chunk N+1

ç›®æ ‡:
- TTFA < 300ms
- RTF < 1
"""

import os
import sys
import torch
import numpy as np
import asyncio
import time
from typing import Optional, List, Generator, AsyncGenerator
from dataclasses import dataclass
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class StreamingConfig:
    """æµå¼æ¨ç†é…ç½®"""
    # é¦–åŒ… chunk å¤§å° (ç ”ç©¶å‘˜æ–¹æ¡ˆ: 5 tokens)
    first_chunk_size: int = 5
    # åç»­ chunk å¤§å°
    normal_chunk_size: int = 20
    # æœ€å¤§ç”Ÿæˆé•¿åº¦
    max_length: int = 2048
    # é‡‡æ ·å‚æ•°
    temperature: float = 0.7
    top_p: float = 0.9
    # éŸ³é¢‘å‚æ•°
    sample_rate: int = 22050


class StreamingBuffer:
    """
    æµå¼ Token ç¼“å†²åŒº
    
    ç ”ç©¶å‘˜æ–¹æ¡ˆ: æ¯ 10-20ms ä¸€ç‰‡
    """
    
    def __init__(self, config: StreamingConfig):
        self.config = config
        self._tokens: List[int] = []
        self._is_first_chunk = True
        self._total_generated = 0
    
    @property
    def chunk_size(self) -> int:
        """å½“å‰ chunk å¤§å° (é¦–åŒ…æ›´å°)"""
        if self._is_first_chunk:
            return self.config.first_chunk_size
        return self.config.normal_chunk_size
    
    def add(self, token: int) -> Optional[List[int]]:
        """
        æ·»åŠ  tokenï¼Œè¿”å›å®Œæ•´ chunk (å¦‚æœæœ‰)
        
        Returns:
            chunk tokens æˆ– None
        """
        self._tokens.append(token)
        self._total_generated += 1
        
        if len(self._tokens) >= self.chunk_size:
            chunk = self._tokens[:self.chunk_size]
            self._tokens = self._tokens[self.chunk_size:]
            
            if self._is_first_chunk:
                self._is_first_chunk = False
            
            return chunk
        
        return None
    
    def flush(self) -> Optional[List[int]]:
        """åˆ·æ–°å‰©ä½™ tokens"""
        if self._tokens:
            chunk = self._tokens
            self._tokens = []
            return chunk
        return None
    
    def reset(self):
        """é‡ç½®çŠ¶æ€"""
        self._tokens = []
        self._is_first_chunk = True
        self._total_generated = 0


class MOSSSpeechStreamingInference:
    """
    MOSS-Speech æµå¼æ¨ç†å¼•æ“
    
    æ¶æ„:
    1. TRT-LLM Engine ç”Ÿæˆ Audio Tokens (æµå¼)
    2. StreamingBuffer æ”¶é›† chunks
    3. BigVGAN Vocoder è§£ç åˆ°æ³¢å½¢
    """
    
    def __init__(
        self,
        engine_path: str,
        vocoder_path: str,
        config: Optional[StreamingConfig] = None,
        device: str = "cuda",
    ):
        self.engine_path = Path(engine_path)
        self.vocoder_path = Path(vocoder_path)
        self.config = config or StreamingConfig()
        self.device = device
        
        self._engine = None
        self._vocoder = None
        self._tokenizer = None
        self._buffer = StreamingBuffer(self.config)
        
        # æ€§èƒ½ç»Ÿè®¡
        self._stats = {
            'ttfa_ms': 0,
            'total_time_ms': 0,
            'tokens_generated': 0,
            'audio_duration_s': 0,
        }
    
    def load(self):
        """åŠ è½½æ¨¡å‹"""
        logger.info("Loading MOSS-Speech TRT-LLM Engine...")
        self._load_engine()
        
        logger.info("Loading BigVGAN Vocoder...")
        self._load_vocoder()
        
        logger.info("âœ… All models loaded")
    
    def _load_engine(self):
        """åŠ è½½ TRT-LLM Engine"""
        try:
            # TRT-LLM Runner
            import tensorrt_llm
            from tensorrt_llm.runtime import ModelRunner
            
            if self.engine_path.exists():
                self._engine = ModelRunner.from_dir(str(self.engine_path))
                logger.info(f"âœ… Engine loaded from {self.engine_path}")
            else:
                logger.warning(f"Engine not found: {self.engine_path}")
                logger.info("Using fallback PyTorch model")
                self._load_fallback_model()
                
        except ImportError as e:
            logger.warning(f"TRT-LLM not available: {e}")
            self._load_fallback_model()
    
    def _load_fallback_model(self):
        """åŠ è½½åŸå§‹ PyTorch æ¨¡å‹ä½œä¸º fallback"""
        from transformers import AutoModel, AutoTokenizer
        
        model_path = "/workspace/models/MOSS-Speech"
        self._engine = AutoModel.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            device_map=self.device,
        )
        self._tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
        )
        logger.info("âœ… Fallback PyTorch model loaded")
    
    def _load_vocoder(self):
        """åŠ è½½å£°ç å™¨"""
        from vocoder import BigVGANVocoder
        
        self._vocoder = BigVGANVocoder(
            model_path=str(self.vocoder_path),
            use_cuda_kernel=True,
        )
        self._vocoder.load()
    
    async def generate_streaming(
        self,
        text: str,
        audio_prompt: Optional[torch.Tensor] = None,
    ) -> AsyncGenerator[np.ndarray, None]:
        """
        æµå¼ç”Ÿæˆè¯­éŸ³
        
        ç ”ç©¶å‘˜æ–¹æ¡ˆ: Token æµå¼å–‚ç»™å£°ç å™¨
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            audio_prompt: å¯é€‰çš„éŸ³é¢‘ prompt (ç”¨äºå…‹éš†)
            
        Yields:
            audio_chunk: PCM éŸ³é¢‘å—
        """
        self._buffer.reset()
        self._stats = {k: 0 for k in self._stats}
        
        start_time = time.perf_counter()
        first_audio_time = None
        
        # ç”Ÿæˆ tokens (æµå¼)
        async for token in self._generate_tokens_async(text, audio_prompt):
            # æ·»åŠ åˆ° buffer
            chunk = self._buffer.add(token)
            
            if chunk is not None:
                # è§£ç  chunk åˆ°éŸ³é¢‘
                audio = self._decode_chunk(chunk)
                
                # è®°å½• TTFA
                if first_audio_time is None:
                    first_audio_time = time.perf_counter()
                    self._stats['ttfa_ms'] = (first_audio_time - start_time) * 1000
                    logger.info(f"ğŸ¯ TTFA: {self._stats['ttfa_ms']:.2f}ms")
                
                yield audio
        
        # å¤„ç†å‰©ä½™ tokens
        remaining = self._buffer.flush()
        if remaining:
            audio = self._decode_chunk(remaining)
            yield audio
        
        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        total_time = time.perf_counter() - start_time
        self._stats['total_time_ms'] = total_time * 1000
        self._stats['tokens_generated'] = self._buffer._total_generated
        
        # ä¼°ç®—éŸ³é¢‘æ—¶é•¿ (å‡è®¾ 86 tokens/sec)
        self._stats['audio_duration_s'] = self._buffer._total_generated / 86
        
        rtf = total_time / max(self._stats['audio_duration_s'], 0.001)
        logger.info(f"ğŸ“Š æ€»æ—¶é—´: {self._stats['total_time_ms']:.2f}ms, RTF: {rtf:.2f}")
    
    async def _generate_tokens_async(
        self,
        text: str,
        audio_prompt: Optional[torch.Tensor] = None,
    ) -> AsyncGenerator[int, None]:
        """å¼‚æ­¥ç”Ÿæˆ tokens"""
        # TRT-LLM Engine æˆ– PyTorch fallback
        if hasattr(self._engine, 'generate_streaming'):
            # TRT-LLM æµå¼ç”Ÿæˆ
            for token in self._engine.generate_streaming(text):
                yield token
                await asyncio.sleep(0)  # è®©å‡ºæ§åˆ¶æƒ
        else:
            # PyTorch fallback (æ¨¡æ‹Ÿæµå¼)
            for token in self._generate_tokens_pytorch(text, audio_prompt):
                yield token
                await asyncio.sleep(0)
    
    def _generate_tokens_pytorch(
        self,
        text: str,
        audio_prompt: Optional[torch.Tensor] = None,
    ) -> Generator[int, None, None]:
        """PyTorch æ¨¡å‹ç”Ÿæˆ tokens (é€ä¸ª)"""
        if self._tokenizer is None:
            # Mock tokens
            for i in range(100):
                yield i % 4096
            return
        
        # ç¼–ç è¾“å…¥
        inputs = self._tokenizer(text, return_tensors="pt").to(self.device)
        
        # é€ token ç”Ÿæˆ
        past_key_values = None
        generated = []
        
        with torch.no_grad():
            for _ in range(self.config.max_length):
                outputs = self._engine(
                    **inputs if past_key_values is None else {'input_ids': inputs['input_ids'][:, -1:]},
                    past_key_values=past_key_values,
                    use_cache=True,
                )
                
                logits = outputs.logits[:, -1, :]
                past_key_values = outputs.past_key_values
                
                # é‡‡æ ·
                if self.config.temperature > 0:
                    probs = torch.softmax(logits / self.config.temperature, dim=-1)
                    token = torch.multinomial(probs, num_samples=1).item()
                else:
                    token = logits.argmax(dim=-1).item()
                
                # æ£€æŸ¥ EOS (å‡è®¾ audio_eos_token_id åœ¨é…ç½®ä¸­)
                if hasattr(self._engine.config, 'audio_eos_token_id'):
                    if token == self._engine.config.audio_eos_token_id:
                        break
                
                generated.append(token)
                yield token
                
                inputs['input_ids'] = torch.cat([
                    inputs['input_ids'],
                    torch.tensor([[token]], device=self.device)
                ], dim=1)
    
    def _decode_chunk(self, tokens: List[int]) -> np.ndarray:
        """è§£ç  token chunk åˆ°éŸ³é¢‘"""
        if self._vocoder is None:
            # Mock éŸ³é¢‘
            return np.random.randn(len(tokens) * 256).astype(np.float32) * 0.01
        
        return self._vocoder.decode_tokens(tokens)
    
    def get_stats(self) -> dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        return self._stats.copy()


# === åŒæ­¥ç‰ˆæœ¬ (ç”¨äºç®€å•æµ‹è¯•) ===
def generate_speech(
    engine,
    vocoder,
    text: str,
    config: Optional[StreamingConfig] = None,
) -> tuple[np.ndarray, dict]:
    """
    åŒæ­¥ç”Ÿæˆè¯­éŸ³
    
    Returns:
        (audio, stats)
    """
    config = config or StreamingConfig()
    inference = MOSSSpeechStreamingInference(
        engine_path=engine,
        vocoder_path=vocoder,
        config=config,
    )
    inference._engine = engine  # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„ engine
    inference._vocoder = vocoder
    
    # æ”¶é›†æ‰€æœ‰éŸ³é¢‘å—
    audio_chunks = []
    
    async def run():
        async for chunk in inference.generate_streaming(text):
            audio_chunks.append(chunk)
    
    asyncio.run(run())
    
    # åˆå¹¶éŸ³é¢‘
    if audio_chunks:
        audio = np.concatenate(audio_chunks)
    else:
        audio = np.array([])
    
    return audio, inference.get_stats()


if __name__ == "__main__":
    print("=" * 60)
    print("MOSS-Speech Streaming Inference Test")
    print("=" * 60)
    
    # æµ‹è¯•é…ç½®
    config = StreamingConfig(
        first_chunk_size=5,
        normal_chunk_size=20,
    )
    
    # åˆ›å»ºæ¨ç†å¼•æ“
    inference = MOSSSpeechStreamingInference(
        engine_path="/workspace/models/MOSS-Speech-TRTLLM-Engine",
        vocoder_path="/workspace/models/BigVGAN",
        config=config,
    )
    
    print(f"Config: first_chunk={config.first_chunk_size}, normal_chunk={config.normal_chunk_size}")
    
    try:
        inference.load()
        
        # æµ‹è¯•æµå¼ç”Ÿæˆ
        async def test():
            text = "Hello, this is a test of streaming speech synthesis."
            async for chunk in inference.generate_streaming(text):
                print(f"  Got chunk: {len(chunk)} samples")
            print(f"\nğŸ“Š Stats: {inference.get_stats()}")
        
        asyncio.run(test())
        
    except Exception as e:
        logger.error(f"Test failed: {e}")
        import traceback
        traceback.print_exc()



