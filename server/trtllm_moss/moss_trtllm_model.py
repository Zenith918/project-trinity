"""
MOSS-Speech TensorRT-LLM Model Definition
==========================================

æ ¸å¿ƒæŠ€æœ¯: è™šæ‹Ÿçº¿æ€§åŒ– (Virtual Linearization)
ä½œè€…: è±†åŒ…å›¢é˜Ÿ
æœ€åæ›´æ–°: 2026-01-18

âš ï¸ è­¦å‘Š: æœ¬æ–‡ä»¶åŒ…å«å…³é”®çš„æ¶æ„ä¼ªè£…ä»£ç ï¼Œä¿®æ”¹å‰è¯·é˜…è¯»:
   /workspace/docs/moss-speech/PITFALLS_AND_SOLUTIONS.md
   /workspace/docs/moss-speech/ARCHITECTURE.md

æ¶æ„è¯´æ˜:
---------
MOSS-Speech é‡‡ç”¨åˆ†å‰æ¶æ„: 32 Shared + 4 Text + 4 Audio = 40 å±‚
ä¸ºäº†ç»•è¿‡ TensorRT-LLM gptAttentionPlugin çš„çº¿æ€§å±‚å‡è®¾ï¼Œ
æˆ‘ä»¬é€šè¿‡"è™šæ‹Ÿçº¿æ€§åŒ–"å°†åˆ†å‰æ¶æ„ä¼ªè£…æˆ 40 å±‚çº¿æ€§å †å ã€‚

å…³é”®å‚æ•° (ç¦æ­¢ä¿®æ”¹):
-------------------
- num_hidden_layers = 40
- epsilon = 1e-4 (è™šæ‹Ÿçº¿æ€§åŒ–ä¾èµ–é“¾)
- audio_start_idx = 36
"""

import json
import os
import inspect
from typing import Optional, Dict, Any, List

import tensorrt_llm
from tensorrt_llm import Tensor
from tensorrt_llm.functional import (
    concat, identity, gather_last_token_logits
)
from tensorrt_llm.layers import (
    Attention, AttentionMaskType, PositionEmbeddingType,
    MLP, RmsNorm, Embedding, ColumnLinear, GatedMLP
)
from tensorrt_llm.models import PretrainedConfig
from tensorrt_llm.models.modeling_utils import DecoderModelForCausalLM, DecoderLayerList
from tensorrt_llm.module import Module, ModuleList

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”´ å…³é”®å¸¸é‡ - ç¦æ­¢ä¿®æ”¹ï¼
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
NUM_SHARED_LAYERS = 32   # Shared å±‚æ•°
NUM_TEXT_LAYERS = 4      # Text åˆ†æ”¯å±‚æ•°
NUM_AUDIO_LAYERS = 4     # Audio åˆ†æ”¯å±‚æ•°
TOTAL_LAYERS = NUM_SHARED_LAYERS + NUM_TEXT_LAYERS + NUM_AUDIO_LAYERS  # å¿…é¡»ä¸º 40

TEXT_VOCAB_SIZE = 151680   # Text è¯è¡¨å¤§å°
AUDIO_VOCAB_SIZE = 16512   # Audio è¯è¡¨å¤§å°
COMBINED_VOCAB_SIZE = TEXT_VOCAB_SIZE + AUDIO_VOCAB_SIZE  # 168192

# è™šæ‹Ÿçº¿æ€§åŒ– epsilon - ç¦æ­¢åˆ é™¤æˆ–ä¿®æ”¹ï¼
# åŸå› : FP16 ç²¾åº¦ä¸‹ï¼Œå¤ªå°ä¼šè¢«èˆå…¥ä¸º 0ï¼Œå¤ªå¤§ä¼šå½±å“è¾“å‡º
VIRTUAL_LINEARIZATION_EPSILON = 1e-4


def verify_architecture_integrity():
    """
    [ARCH_GUARD] æ¶æ„å®Œæ•´æ€§éªŒè¯
    
    åœ¨æ¨¡å—åŠ è½½æ—¶è‡ªåŠ¨æ‰§è¡Œï¼Œç¡®ä¿å…³é”®å‚æ•°æœªè¢«ä¿®æ”¹ã€‚
    """
    assert TOTAL_LAYERS == 40, \
        f"[ARCH_GUARD] FATAL: TOTAL_LAYERS={TOTAL_LAYERS}, å¿…é¡»ä¸º 40ï¼"
    
    assert NUM_SHARED_LAYERS == 32, \
        f"[ARCH_GUARD] FATAL: NUM_SHARED_LAYERS={NUM_SHARED_LAYERS}, å¿…é¡»ä¸º 32ï¼"
    
    assert VIRTUAL_LINEARIZATION_EPSILON == 1e-4, \
        f"[ARCH_GUARD] FATAL: epsilon={VIRTUAL_LINEARIZATION_EPSILON}, å¿…é¡»ä¸º 1e-4ï¼"
    
    print("[ARCH_GUARD] âœ… 40-Layer Virtual Linearization Active")


# æ¨¡å—åŠ è½½æ—¶éªŒè¯
verify_architecture_integrity()


class MossSpeechConfig(PretrainedConfig):
    """
    MOSS-Speech æ¨¡å‹é…ç½®
    
    ç»§æ‰¿è‡ª TensorRT-LLM çš„ PretrainedConfigï¼Œæ·»åŠ äº†åˆ†å‰æ¶æ„ç‰¹æœ‰çš„å‚æ•°ã€‚
    """
    
    def __init__(
        self,
        architecture: str = "MossSpeechForCausalLM",
        dtype: str = "float16",
        logits_dtype: str = "float32",
        vocab_size: int = COMBINED_VOCAB_SIZE,
        hidden_size: int = 4096,
        intermediate_size: int = 12288,
        num_hidden_layers: int = TOTAL_LAYERS,  # ğŸ”´ å¼ºåˆ¶ 40 å±‚
        num_attention_heads: int = 32,
        num_key_value_heads: int = 8,
        hidden_act: str = "silu",
        max_position_embeddings: int = 40960,
        rms_norm_eps: float = 1e-6,
        rotary_base: float = 1000000.0,
        position_embedding_type: PositionEmbeddingType = PositionEmbeddingType.rope_gpt_neox,
        num_shared_layers: int = NUM_SHARED_LAYERS,
        num_text_layers: int = NUM_TEXT_LAYERS,
        num_audio_layers: int = NUM_AUDIO_LAYERS,
        text_vocab_size: int = TEXT_VOCAB_SIZE,
        audio_vocab_size: int = AUDIO_VOCAB_SIZE,
        **kwargs
    ):
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # æ¸…ç† kwargs ä¸­å¯èƒ½é‡å¤çš„å‚æ•°ï¼Œé˜²æ­¢ "multiple values" é”™è¯¯
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        for key in ['num_hidden_layers', 'architecture', 'dtype', 'vocab_size', 
                    'hidden_size', 'intermediate_size', 'num_attention_heads',
                    'num_key_value_heads', 'hidden_act', 'max_position_embeddings',
                    'logits_dtype', 'position_embedding_type']:
            kwargs.pop(key, None)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ”´ [ARCH_GUARD] å¼ºåˆ¶ä½¿ç”¨ 40 å±‚ï¼Œå¿½ç•¥ä¼ å…¥çš„ num_hidden_layers
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        num_hidden_layers = TOTAL_LAYERS
        
        super().__init__(
            architecture=architecture,
            dtype=dtype,
            logits_dtype=logits_dtype,
            vocab_size=vocab_size,
            hidden_size=hidden_size,
            intermediate_size=intermediate_size,
            num_hidden_layers=num_hidden_layers,
            num_attention_heads=num_attention_heads,
            num_key_value_heads=num_key_value_heads,
            hidden_act=hidden_act,
            max_position_embeddings=max_position_embeddings,
            position_embedding_type=position_embedding_type,
            **kwargs
        )
        
        self.rms_norm_eps = rms_norm_eps
        self.rotary_base = rotary_base
        self.num_shared_layers = num_shared_layers
        self.num_text_layers = num_text_layers
        self.num_audio_layers = num_audio_layers
        self.text_vocab_size = text_vocab_size
        self.audio_vocab_size = audio_vocab_size
        
        # ç¡®ä¿ head_size è¢«è®¾ç½®
        if not hasattr(self, 'head_size') or self.head_size is None:
            self.head_size = hidden_size // num_attention_heads
    
    def __repr__(self):
        """[ARCH_GUARD] æ‰“å°æ¶æ„ä¿¡æ¯"""
        return (
            f"MossSpeechConfig(\n"
            f"  [ARCH_GUARD] 40-Layer Virtual Linearization Active\n"
            f"  num_hidden_layers={self.num_hidden_layers},\n"
            f"  num_shared_layers={self.num_shared_layers},\n"
            f"  num_text_layers={self.num_text_layers},\n"
            f"  num_audio_layers={self.num_audio_layers},\n"
            f"  epsilon={VIRTUAL_LINEARIZATION_EPSILON}\n"
            f")"
        )


class MossSpeechDecoderLayer(Module):
    """
    MOSS-Speech å•å±‚ Decoder
    
    ä¸æ ‡å‡† Transformer å±‚ç›¸åŒï¼Œä½†æ³¨æ„ layer_idx å¿…é¡»åœ¨ 0-39 èŒƒå›´å†…ã€‚
    """
    
    def __init__(self, config: MossSpeechConfig, layer_idx: int):
        super().__init__()
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # [ARCH_GUARD] éªŒè¯ layer_idx èŒƒå›´
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        assert 0 <= layer_idx < TOTAL_LAYERS, \
            f"[ARCH_GUARD] layer_idx={layer_idx} è¶…å‡ºèŒƒå›´ [0, {TOTAL_LAYERS})"
        
        self.layer_idx = layer_idx
        self.hidden_size = config.hidden_size
        dtype = config.dtype
        
        # Input LayerNorm
        self.input_layernorm = RmsNorm(
            normalized_shape=config.hidden_size,
            eps=config.rms_norm_eps,
            dtype=dtype,
        )
        
        # Attention - ä½¿ç”¨ local_layer_idx ç¡®ä¿ KV Cache æ­£ç¡®å¯»å€
        self.attention = Attention(
            local_layer_idx=layer_idx,  # ğŸ”´ å…³é”®: æ¯å±‚å”¯ä¸€çš„ç´¢å¼•
            hidden_size=config.hidden_size,
            num_attention_heads=config.num_attention_heads,
            num_kv_heads=config.num_key_value_heads,
            max_position_embeddings=config.max_position_embeddings,
            dtype=dtype,
            attention_mask_type=AttentionMaskType.causal,
            position_embedding_type=config.position_embedding_type,
            rotary_embedding_base=config.rotary_base,
            tp_group=None,
            tp_size=1,
        )
        
        # MLP
        self.mlp = GatedMLP(
            hidden_size=config.hidden_size,
            ffn_hidden_size=config.intermediate_size,
            hidden_act=config.hidden_act,
            dtype=dtype,
            tp_group=None,
            tp_size=1,
        )
        
        # Post attention LayerNorm
        self.post_layernorm = RmsNorm(
            normalized_shape=config.hidden_size,
            eps=config.rms_norm_eps,
            dtype=dtype,
        )
    
    def forward(
        self,
        hidden_states: Tensor,
        attention_mask: Optional[Tensor] = None,
        use_cache: bool = False,
        kv_cache_params=None,
        attention_params=None,
    ):
        # Self attention
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)
        
        attention_output = self.attention(
            hidden_states,
            attention_mask=attention_mask,
            use_cache=use_cache,
            kv_cache_params=kv_cache_params,
            attention_params=attention_params,
        )
        
        if use_cache:
            attention_output, presents = attention_output
        
        hidden_states = residual + attention_output
        
        # MLP
        residual = hidden_states
        hidden_states = self.post_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states
        
        if use_cache:
            return hidden_states, presents
        return hidden_states


class MossSpeechModel(Module):
    """
    MOSS-Speech Transformer æ¨¡å‹
    
    å®ç°è™šæ‹Ÿçº¿æ€§åŒ– (Virtual Linearization):
    - ç‰©ç†ä¸Š: 40 å±‚çº¿æ€§å †å 
    - é€»è¾‘ä¸Š: Shared(0-31) â†’ Text(32-35) / Audio(36-39) åˆ†å‰
    """
    
    def __init__(self, config: MossSpeechConfig):
        super().__init__()
        self.config = config
        dtype = config.dtype
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Embedding å±‚
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.embed_tokens = Embedding(
            num_embeddings=config.vocab_size,
            embedding_dim=config.hidden_size,
            dtype=dtype,
        )
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 40 å±‚ Decoder Layers (çº¿æ€§å †å )
        # 
        # å±‚ç´¢å¼•æ˜ å°„:
        #   0-31:  Shared å±‚ (Text å’Œ Audio éƒ½ç»è¿‡)
        #   32-35: Text ä¸“ç”¨å±‚
        #   36-39: Audio ä¸“ç”¨å±‚
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.layers = ModuleList([
            MossSpeechDecoderLayer(config, layer_idx=i)
            for i in range(TOTAL_LAYERS)
        ])
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ”´ åˆ†æ”¯ç´¢å¼• - è™šæ‹Ÿçº¿æ€§åŒ–çš„å…³é”®å‚æ•°
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.shared_end_idx = NUM_SHARED_LAYERS - 1        # 31
        self.text_start_idx = NUM_SHARED_LAYERS            # 32
        self.text_end_idx = NUM_SHARED_LAYERS + NUM_TEXT_LAYERS - 1  # 35
        self.audio_start_idx = NUM_SHARED_LAYERS + NUM_TEXT_LAYERS   # 36 ğŸ”´ å…³é”®
        
        # Final layer norm
        self.norm = RmsNorm(
            normalized_shape=config.hidden_size,
            eps=config.rms_norm_eps,
            dtype=dtype,
        )
        
        # [ARCH_GUARD] æ‰“å°æ¶æ„ä¿¡æ¯
        print(f"[ARCH_GUARD] âœ… MossSpeechModel åˆå§‹åŒ–å®Œæˆ:")
        print(f"   æ€»å±‚æ•°: {TOTAL_LAYERS}")
        print(f"   Shared: 0-{self.shared_end_idx}")
        print(f"   Text: {self.text_start_idx}-{self.text_end_idx}")
        print(f"   Audio: {self.audio_start_idx}-{TOTAL_LAYERS-1}")
        print(f"   Virtual Linearization epsilon: {VIRTUAL_LINEARIZATION_EPSILON}")
    
    def forward(
        self,
        input_ids: Tensor,
        position_ids: Optional[Tensor] = None,
        use_cache: bool = False,
        attention_mask: Optional[Tensor] = None,
        kv_cache_params=None,
        attention_params=None,
    ):
        """
        Forward pass with Virtual Linearization
        
        è™šæ‹Ÿçº¿æ€§åŒ–æµç¨‹:
        1. æ‰§è¡Œ Layer 0-31 (Shared)
        2. æ‰§è¡Œ Layer 32-35 (Text)ï¼Œä¿å­˜ Layer 35 è¾“å‡º
        3. åœ¨ Layer 36 å…¥å£ï¼Œé‡ç½® hidden_states = shared_output + hidden * epsilon
        4. æ‰§è¡Œ Layer 36-39 (Audio)
        5. è¿”å› text_hidden å’Œ audio_hidden
        """
        hidden_states = self.embed_tokens(input_ids)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # å…³é”®å˜é‡
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        shared_output = None  # ä¿å­˜ Layer 31 çš„è¾“å‡º
        text_hidden = None    # ä¿å­˜ Layer 35 çš„è¾“å‡º
        presents = []
        
        for layer_idx, layer in enumerate(self.layers):
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # ä¿å­˜ Shared è¾“å‡º (Layer 31 â†’ 32 è¿‡æ¸¡ç‚¹)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if layer_idx == self.text_start_idx:  # 32
                shared_output = identity(hidden_states)
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # ğŸ”´ğŸ”´ğŸ”´ æ ¸å¿ƒæ‰‹æœ¯ç‚¹: Layer 36 (Audio åˆ†æ”¯èµ·ç‚¹) ğŸ”´ğŸ”´ğŸ”´
            # 
            # è¿™æ˜¯è™šæ‹Ÿçº¿æ€§åŒ–çš„æ ¸å¿ƒï¼
            # 
            # ä¸ºä»€ä¹ˆéœ€è¦è¿™è¡Œä»£ç ï¼Ÿ
            # -------------------
            # 1. Text åˆ†æ”¯ (32-35) æ‰§è¡Œå®Œåï¼Œhidden_states åŒ…å« text ä¿¡æ¯
            # 2. Audio åˆ†æ”¯éœ€è¦ä» shared_output (Layer 31 è¾“å‡º) å¼€å§‹
            # 3. ç›´æ¥èµ‹å€¼ hidden_states = shared_output ä¼šè¢« TensorRT ç¼–è¯‘å™¨ä¼˜åŒ–
            #    å› ä¸ºç¼–è¯‘å™¨ä¼šè®¤ä¸º Layer 32-35 çš„è®¡ç®—æ˜¯"æ— ç”¨çš„"
            # 
            # ä¸ºä»€ä¹ˆæ˜¯ 1e-4ï¼Ÿ
            # --------------
            # - 1e-8: FP16 ç²¾åº¦ä¸‹ä¼šè¢«èˆå…¥ä¸º 0ï¼Œä¼˜åŒ–ä»ä¼šç”Ÿæ•ˆ
            # - 1e-1: å¤ªå¤§ï¼Œä¼šæ±¡æŸ“ Audio è¾“å‡º
            # - 1e-4: åˆšå¥½åœ¨ FP16 å¯è¡¨ç¤ºèŒƒå›´å†… (FP16 æœ€å°æ­£æ•° â‰ˆ 6e-8)
            #         ä¸”å¯¹è¾“å‡ºå½±å“å¯å¿½ç•¥ (ç›¸å¯¹è¯¯å·® < 0.01%)
            # 
            # âš ï¸ è­¦å‘Š: åˆ é™¤æˆ–ä¿®æ”¹æ­¤è¡Œå°†å¯¼è‡´ Generation Phase å´©æºƒï¼
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if layer_idx == self.audio_start_idx:  # 36
                hidden_states = shared_output + hidden_states * VIRTUAL_LINEARIZATION_EPSILON
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # æ‰§è¡Œå½“å‰å±‚
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            layer_output = layer(
                hidden_states,
                attention_mask=attention_mask,
                use_cache=use_cache,
                kv_cache_params=kv_cache_params,
                attention_params=attention_params,
            )
            
            if use_cache:
                hidden_states, present = layer_output
                presents.append(present)
            else:
                hidden_states = layer_output
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # ä¿å­˜ Text è¾“å‡º (Layer 35)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            if layer_idx == self.text_end_idx:  # 35
                text_hidden = identity(hidden_states)
        
        # Layer 39 è¾“å‡ºå°±æ˜¯ audio_hidden
        audio_hidden = hidden_states
        
        if use_cache:
            return (text_hidden, audio_hidden), tuple(presents)
        return text_hidden, audio_hidden


class MossSpeechForCausalLM(DecoderModelForCausalLM):
    """
    MOSS-Speech Causal LM with Dual Output Heads
    
    ç»§æ‰¿è‡ª DecoderModelForCausalLM ä»¥æ­£ç¡®å¤„ç† RoPE ä½ç½®ç¼–ç ã€‚
    
    è¾“å‡º:
    -----
    combined_logits: [batch, seq_len, 168192]
        - [:, :, :151680]: Text logits
        - [:, :, 151680:]: Audio logits
    """
    
    config_class = MossSpeechConfig
    
    def __init__(self, config: MossSpeechConfig):
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # [ARCH_GUARD] éªŒè¯é…ç½®
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        assert config.num_hidden_layers == TOTAL_LAYERS, \
            f"[ARCH_GUARD] config.num_hidden_layers={config.num_hidden_layers}, å¿…é¡»ä¸º {TOTAL_LAYERS}ï¼"
        
        # åˆ›å»º transformer
        transformer = MossSpeechModel(config)
        
        # ä¸» LM headï¼ˆç”¨äºæ–‡æœ¬ï¼‰
        lm_head = ColumnLinear(
            in_features=config.hidden_size,
            out_features=config.text_vocab_size,
            bias=False,
            dtype=config.dtype,
            tp_group=None,
            tp_size=1,
            gather_output=True,
        )
        
        # è°ƒç”¨çˆ¶ç±» __init__
        # DecoderModelForCausalLM ä¼šè‡ªåŠ¨è°ƒç”¨:
        #   Attention.create_attention_const_params(self, config)
        # è¿™ä¼šåˆå§‹åŒ– RoPE æ‰€éœ€çš„ rotary_inv_freq å’Œ embed_positions_for_gpt_attention
        super().__init__(config, transformer, lm_head)
        
        # Audio LM head
        self.audio_lm_head = ColumnLinear(
            in_features=config.hidden_size,
            out_features=config.audio_vocab_size,
            bias=False,
            dtype=config.dtype,
            tp_group=None,
            tp_size=1,
            gather_output=True,
        )
        
        # [ARCH_GUARD] æ‰“å°ç¡®è®¤ä¿¡æ¯
        print(f"[ARCH_GUARD] âœ… MossSpeechForCausalLM åˆå§‹åŒ–å®Œæˆ")
        print(f"   Text vocab: {config.text_vocab_size}")
        print(f"   Audio vocab: {config.audio_vocab_size}")
        print(f"   Combined vocab: {COMBINED_VOCAB_SIZE}")
    
    def __repr__(self):
        """[ARCH_GUARD] æ‰“å°æ¶æ„ä¿¡æ¯"""
        return (
            f"MossSpeechForCausalLM(\n"
            f"  [ARCH_GUARD] 40-Layer Virtual Linearization Active\n"
            f"  num_hidden_layers={self.config.num_hidden_layers},\n"
            f"  epsilon={VIRTUAL_LINEARIZATION_EPSILON}\n"
            f")"
        )
    
    def forward(
        self,
        input_ids: Tensor,
        position_ids=None,
        use_cache=False,
        last_token_ids=None,
        attention_mask=None,
        kv_cache_params=None,
        attention_params=None,
        hidden_states=None,
        prompt_embedding_table: Optional[Tensor] = None,
        prompt_tasks: Optional[Tensor] = None,
        prompt_vocab_size: Optional[Tensor] = None,
        lora_params=None,
        spec_decoding_params=None,
    ):
        """
        Forward pass
        
        å…³é”®: å¿…é¡»è°ƒç”¨ Attention.fill_attention_params å¡«å…… RoPE å‚æ•°
        """
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ”´ å¡«å…… attention paramsï¼ˆåŒ…æ‹¬ RoPE å‚æ•°ï¼‰
        # è¿™æ˜¯ç»§æ‰¿ DecoderModelForCausalLM çš„å…³é”®åŸå› ï¼
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        attention_params = Attention.fill_attention_params(
            self, attention_params)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # æ‰§è¡Œ transformerï¼ˆè™šæ‹Ÿçº¿æ€§åŒ–åœ¨è¿™é‡Œå‘ç”Ÿï¼‰
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        outputs = self.transformer(
            input_ids=input_ids,
            position_ids=position_ids,
            use_cache=use_cache,
            attention_mask=attention_mask,
            kv_cache_params=kv_cache_params,
            attention_params=attention_params,
        )
        
        if use_cache:
            (text_hidden, audio_hidden), presents = outputs
        else:
            text_hidden, audio_hidden = outputs
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # åº”ç”¨ final norm
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        text_hidden = self.transformer.norm(text_hidden)
        audio_hidden = self.transformer.norm(audio_hidden)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # gather last token (ç”¨äºè‡ªå›å½’ç”Ÿæˆ)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if last_token_ids is not None:
            remove_input_padding = tensorrt_llm.default_net().plugin_config.remove_input_padding
            text_hidden = gather_last_token_logits(
                text_hidden, last_token_ids, remove_input_padding
            )
            audio_hidden = gather_last_token_logits(
                audio_hidden, last_token_ids, remove_input_padding
            )
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # è®¡ç®— logits
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        text_logits = self.lm_head(text_hidden)
        audio_logits = self.audio_lm_head(audio_hidden)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # æ‹¼æ¥ä¸ºå•ä¸€è¾“å‡º (Python ç«¯å†æ‹†åˆ†)
        # 
        # combined_logits[:, :, :151680] = text_logits
        # combined_logits[:, :, 151680:] = audio_logits
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        combined_logits = concat([text_logits, audio_logits], dim=-1)
        combined_logits.mark_output('logits', self.config.logits_dtype)
        
        if use_cache:
            return combined_logits, presents
        return combined_logits
    
    @classmethod
    def from_checkpoint(cls, checkpoint_dir: str, **kwargs):
        """
        ä» checkpoint åŠ è½½æ¨¡å‹
        
        [ARCH_GUARD] æ— è®º checkpoint ä¸­çš„é…ç½®å¦‚ä½•ï¼Œéƒ½å¼ºåˆ¶ä½¿ç”¨ 40 å±‚
        """
        config_path = os.path.join(checkpoint_dir, "config.json")
        
        if os.path.exists(config_path):
            with open(config_path, 'r') as f:
                config_dict = json.load(f)
            
            # è·å– pretrained_config
            if 'pretrained_config' in config_dict:
                pretrained_config = config_dict['pretrained_config']
            else:
                pretrained_config = config_dict
            
            config = MossSpeechConfig(**pretrained_config)
        else:
            config = MossSpeechConfig()
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # [ARCH_GUARD] æ‰“å°ç¡®è®¤
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        print(f"[ARCH_GUARD] âœ… é…ç½®åŠ è½½å®Œæˆ: num_hidden_layers = {config.num_hidden_layers}")
        
        model = cls(config)
        
        # åŠ è½½æƒé‡
        weights_path = os.path.join(checkpoint_dir, "rank0.safetensors")
        if os.path.exists(weights_path):
            print(f"åŠ è½½æƒé‡: {weights_path}")
        
        return model


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# [ARCH_GUARD] æ¨¡å—çº§åˆ«çš„æ¶æ„éªŒè¯å‡½æ•°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def verify_model_architecture(model: MossSpeechForCausalLM):
    """
    éªŒè¯æ¨¡å‹æ¶æ„å®Œæ•´æ€§
    
    åœ¨æ„å»º Engine å‰è°ƒç”¨æ­¤å‡½æ•°ï¼
    """
    # 1. å±‚æ•°æ£€æŸ¥
    assert model.config.num_hidden_layers == 40, \
        f"[ARCH_GUARD] FATAL: num_hidden_layers={model.config.num_hidden_layers}, å¿…é¡»ä¸º 40ï¼"
    
    # 2. è™šæ‹Ÿçº¿æ€§åŒ–ä¾èµ–é“¾æ£€æŸ¥
    source = inspect.getsource(model.transformer.forward)
    assert "1e-4" in source or "1e-04" in source or "VIRTUAL_LINEARIZATION_EPSILON" in source, \
        "[ARCH_GUARD] FATAL: è™šæ‹Ÿåºåˆ—åŒ–ä¾èµ–é“¾ä¸¢å¤±ï¼1e-4 epsilon ä¸å¯åˆ é™¤ï¼"
    
    # 3. åˆ†æ”¯ç´¢å¼•æ£€æŸ¥
    assert model.transformer.audio_start_idx == 36, \
        f"[ARCH_GUARD] FATAL: audio_start_idx={model.transformer.audio_start_idx}, å¿…é¡»ä¸º 36ï¼"
    
    print("[ARCH_GUARD] âœ… æ¨¡å‹æ¶æ„éªŒè¯é€šè¿‡")
    print(f"   num_hidden_layers: {model.config.num_hidden_layers}")
    print(f"   audio_start_idx: {model.transformer.audio_start_idx}")
    print(f"   epsilon: {VIRTUAL_LINEARIZATION_EPSILON}")
    
    return True
