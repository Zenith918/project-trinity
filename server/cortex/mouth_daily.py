"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ‘„ CORTEX-MOUTH-DAILY (ç«¯å£ 9003)                                            â•‘
â•‘  VoxCPM 1.5 - æè‡´ä½å»¶è¿Ÿé…ç½®                                                  â•‘
â•‘                                                                              â•‘
â•‘  ğŸ”¥ optimize=True + mode="default" = TTFA ~285ms                              â•‘
â•‘  ğŸ’¡ é¦–æ¬¡å¯åŠ¨éœ€è¦ ~10 åˆ†é’Ÿ JIT ç¼–è¯‘ï¼Œä¹‹åç¨³å®šåœ¨ ~285ms                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âš ï¸âš ï¸âš ï¸ è­¦å‘Šï¼šç»å¯¹ä¸è¦ä¿®æ”¹ä»¥ä¸‹é…ç½®ï¼âš ï¸âš ï¸âš ï¸                                      â•‘
â•‘                                                                              â•‘
â•‘  1. optimize=True  - å¿…é¡»ä¸º Trueï¼Œå¦åˆ™ TTFA ä¼šä» 285ms é€€åŒ–åˆ° 450ms            â•‘
â•‘  2. mode="default" - VoxCPM æºç å·²ä¿®æ”¹ï¼Œä¸è¦æ”¹å› "reduce-overhead"             â•‘
â•‘                                                                              â•‘
â•‘  å¦‚æœä½ çœ‹åˆ° CUDA Graph ç›¸å…³é”™è¯¯ï¼Œé—®é¢˜åœ¨ VoxCPM æºç ï¼Œä¸æ˜¯è¿™é‡Œï¼                 â•‘
â•‘  è§£å†³æ–¹æ¡ˆï¼šä¿®æ”¹ /usr/local/lib/python3.11/dist-packages/voxcpm/model/voxcpm.py â•‘
â•‘  å°† mode="reduce-overhead" æ”¹ä¸º mode="default"                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import signal
import subprocess

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”§ å¯åŠ¨å‰è‡ªåŠ¨æ¸…ç†ç«¯å£ï¼ˆé˜²æ­¢ "Address already in use" é”™è¯¯ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SERVICE_PORT = 9003

def kill_port(port: int):
    """æ€æ‰å ç”¨æŒ‡å®šç«¯å£çš„è¿›ç¨‹"""
    try:
        # ä½¿ç”¨ fuser æŸ¥æ‰¾å¹¶æ€æ‰å ç”¨ç«¯å£çš„è¿›ç¨‹
        result = subprocess.run(
            f"fuser -k {port}/tcp 2>/dev/null || true",
            shell=True, capture_output=True, text=True
        )
        # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ ss + kill
        result = subprocess.run(
            f"ss -tlnp 2>/dev/null | grep ':{port}' | awk '{{print $NF}}' | grep -oP 'pid=\\K[0-9]+' | xargs -r kill -9 2>/dev/null || true",
            shell=True, capture_output=True, text=True
        )
    except Exception:
        pass

# å¯åŠ¨å‰å…ˆæ¸…ç†ç«¯å£
kill_port(SERVICE_PORT)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”‘ å…³é”®ï¼šåœ¨å¯¼å…¥ torch ä¹‹å‰ç¦ç”¨ CUDA Graphï¼ˆè™½ç„¶å·²æ”¹ VoxCPM æºç ï¼Œä½†åŒé‡ä¿é™©ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
os.environ['TORCHINDUCTOR_CUDAGRAPHS'] = '0'

import torch
try:
    torch._inductor.config.triton.cudagraphs = False
except AttributeError:
    pass  # torch 2.4.x ä¸éœ€è¦è¿™ä¸ªè®¾ç½®ï¼Œç¯å¢ƒå˜é‡å·²ç”Ÿæ•ˆ

import io
import wave
import numpy as np
from loguru import logger
from fastapi import FastAPI
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, Response, FileResponse
from contextlib import asynccontextmanager
import time

mouth = None


class DailyMouthHandler:
    """
    VoxCPM 1.5 å¤„ç†å™¨
    
    âš ï¸ é‡è¦é…ç½®ï¼ˆä¸è¦ä¿®æ”¹ï¼‰ï¼š
    - optimize=True  â†’ å¯ç”¨ torch.compileï¼ŒTTFA ~285ms
    - optimize=False â†’ ç¦ç”¨ä¼˜åŒ–ï¼ŒTTFA é€€åŒ–åˆ° ~450msï¼ˆæ…¢ 37%ï¼‰
    """
    
    def __init__(self):
        self.model = None
        self.is_ready = False
        # ğŸ”¥ VoxCPM 1.5 ä½¿ç”¨ 44.1kHz é«˜ä¿çœŸé‡‡æ ·ç‡ï¼ä¸æ˜¯ 24kHzï¼
        self.sample_rate = 44100
        # steps=2 å®ç° RTF < 1 (å®æ—¶æµç•…æ’­æ”¾)ï¼Œsteps=4 éŸ³è´¨æ›´å¥½ä½†ä¼šå¡é¡¿
        self.config = {"steps": 2, "cfg_value": 2.0}
        
        # éŸ³è‰² Prompt é…ç½®
        # VoxCPM è¦æ±‚ prompt_wav_path å’Œ prompt_text å¿…é¡»åŒæ—¶æä¾›æˆ–åŒæ—¶ä¸ºç©º
        # ä½¿ç”¨ 44.1kHz é‡é‡‡æ ·ç‰ˆæœ¬ï¼ŒåŒ¹é… VoxCPM 1.5 è¾“å‡ºé‡‡æ ·ç‡
        self.default_prompt_wav = os.path.join(
            os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
            "assets", "prompt_female_44k.wav"
        )
        # prompt éŸ³é¢‘å†…å®¹ï¼ˆé€šè¿‡ ASR è¯†åˆ«ï¼‰
        self.default_prompt_text = "å¸Œæœ›ä½ ä»¥åèƒ½å¤Ÿåšçš„æ¯”æˆ‘è¿˜å¥½å“Ÿ"
        
    async def initialize(self):
        logger.info("=" * 60)
        logger.info("æ­£åœ¨åˆå§‹åŒ– VoxCPM 1.5 (optimize=True)...")
        logger.info("âš ï¸ é¦–æ¬¡å¯åŠ¨éœ€è¦ ~10 åˆ†é’Ÿ JIT ç¼–è¯‘ï¼Œè¯·è€å¿ƒç­‰å¾…ï¼")
        logger.info("=" * 60)
        
        try:
            from voxcpm import VoxCPM
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # ğŸš¨ğŸš¨ğŸš¨ ç»å¯¹ä¸è¦æŠŠ optimize æ”¹æˆ Falseï¼ğŸš¨ğŸš¨ğŸš¨
            # 
            # optimize=True  â†’ TTFA ~285ms âœ…
            # optimize=False â†’ TTFA ~450ms âŒ (æ…¢ 37%)
            #
            # å¦‚æœé‡åˆ° CUDA Graph é”™è¯¯ï¼Œä¿®æ”¹ VoxCPM æºç ï¼Œä¸è¦æ”¹è¿™é‡Œï¼
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            OPTIMIZE_ENABLED = True  # ğŸš¨ ä¸è¦æ”¹æˆ Falseï¼
            assert OPTIMIZE_ENABLED is True, "âŒ optimize å¿…é¡»ä¸º Trueï¼ä¸è¦æ”¹æˆ Falseï¼"
            
            self.model = VoxCPM.from_pretrained(
                hf_model_id="openbmb/VoxCPM1.5",
                load_denoiser=False,
                optimize=OPTIMIZE_ENABLED,  # ğŸš¨ å¿…é¡»ä¸º True
            )
            
            # é¢„çƒ­ 1: éæµå¼
            logger.info("é¢„çƒ­ 1/2: éæµå¼æ¨ç†...")
            _ = self.model.generate(
                text="é¢„çƒ­",
                inference_timesteps=self.config["steps"],
                cfg_value=self.config["cfg_value"],
            )
            
            # é¢„çƒ­ 2: æµå¼ (è§¦å‘å®Œæ•´ JIT)
            logger.info("é¢„çƒ­ 2/2: æµå¼æ¨ç† (è§¦å‘ JIT ç¼–è¯‘)...")
            for chunk in self.model.generate_streaming(
                text="æµå¼é¢„çƒ­",
                inference_timesteps=self.config["steps"],
                cfg_value=self.config["cfg_value"],
            ):
                pass
            
            self.is_ready = True
            logger.success("âœ… VoxCPM 1.5 åˆå§‹åŒ–å®Œæˆ (TTFA ~285ms)")
            return True
            
        except Exception as e:
            logger.error(f"VoxCPM åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def synthesize(self, text, inference_timesteps=None, cfg_value=None, 
                   prompt_wav_path=None, prompt_text=None):
        """
        éæµå¼è¯­éŸ³åˆæˆ
        
        Args:
            text: è¦åˆæˆçš„æ–‡æœ¬
            inference_timesteps: æ‰©æ•£æ­¥æ•° (2-10, è¶Šé«˜è¶Šæ¸…æ™°ä½†è¶Šæ…¢)
            cfg_value: CFG å€¼ (1.0-3.0, è¶Šé«˜è¶Šæ¸…æ™°)
            prompt_wav_path: å‚è€ƒéŸ³é¢‘è·¯å¾„ (ç”¨äºå…‹éš†éŸ³è‰²)
            prompt_text: å‚è€ƒéŸ³é¢‘å¯¹åº”çš„æ–‡æœ¬ (å¯é€‰)
        """
        if not self.is_ready:
            return b""
        steps = inference_timesteps or self.config["steps"]
        cfg = cfg_value or self.config["cfg_value"]
        prompt_wav = prompt_wav_path or self.default_prompt_wav
        prompt_txt = prompt_text or self.default_prompt_text
        
        try:
            start = time.time()
            actual_prompt_wav = prompt_wav if os.path.exists(prompt_wav) else None
            actual_prompt_txt = prompt_txt if prompt_txt else None
            
            # ä½¿ç”¨ prompt éŸ³é¢‘å…‹éš†éŸ³è‰²
            # æ³¨æ„ï¼šcore.py çš„å‚æ•°æ˜¯ "text" ä¸æ˜¯ "target_text"
            audio = self.model.generate(
                text=text, 
                cfg_value=cfg, 
                inference_timesteps=steps,
                prompt_wav_path=actual_prompt_wav,
                prompt_text=actual_prompt_txt,
            )
            logger.info(f"ç”Ÿæˆ: {len(text)}å­—, {(time.time()-start)*1000:.0f}ms, prompt={os.path.basename(prompt_wav) if prompt_wav else 'none'}")
            audio_int16 = (audio * 32767).astype(np.int16)
            buf = io.BytesIO()
            with wave.open(buf, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(self.sample_rate)
                wf.writeframes(audio_int16.tobytes())
            return buf.getvalue()
        except Exception as e:
            logger.error(f"æ¨ç†å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return b""

    def synthesize_stream(self, text, inference_timesteps=None, cfg_value=None,
                          prompt_wav_path=None, prompt_text=None):
        """
        æµå¼è¯­éŸ³åˆæˆ (å¸¦ Early Stopping + Chunk åˆå¹¶)
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. Early Stopping: æ ¹æ®æ–‡æœ¬é•¿åº¦é¢„ä¼°æœ€å¤§ chunk æ•°ï¼Œé˜²æ­¢ AR å¹»è§‰å¾ªç¯
        2. Chunk åˆå¹¶: æ¯ 2 ä¸ª chunk åˆå¹¶åå† yieldï¼Œå‡å°‘ç½‘ç»œ IO
        """
        if not self.is_ready:
            yield b""
            return
        steps = inference_timesteps or self.config["steps"]
        cfg = cfg_value or self.config["cfg_value"]
        prompt_wav = prompt_wav_path or self.default_prompt_wav
        prompt_txt = prompt_text or self.default_prompt_text
        
        # ğŸ”§ Early Stopping: 1ä¸ªæ±‰å­—çº¦ 3-5 ä¸ª token (480-800ms)
        # æ¯ä¸ª chunk = 160ms = 1 tokenï¼Œç»™å®½æ¾ä¸Šé™ï¼šå­—æ•° * 8
        text_len = len(text.replace(" ", ""))
        max_chunks = max(15, text_len * 8)  # æœ€å°‘ 15 ä¸ª chunk (2.4s)
        
        # ğŸ”§ Chunk åˆå¹¶: å‡å°‘ IO æ¬¡æ•°
        MERGE_COUNT = 2  # æ¯ 2 ä¸ª chunk åˆå¹¶å‘é€
        
        try:
            start = time.time()
            first = True
            chunk_count = 0
            pending_chunks = []  # å¾…åˆå¹¶çš„ chunk ç¼“å†²
            
            for chunk in self.model.generate_streaming(
                text=text, 
                cfg_value=cfg, 
                inference_timesteps=steps,
                prompt_wav_path=prompt_wav if os.path.exists(prompt_wav) else None,
                prompt_text=prompt_txt if prompt_txt else None,
            ):
                chunk_count += 1
                
                if first:
                    logger.info(f"TTFA: {(time.time()-start)*1000:.0f}ms, max_chunks={max_chunks}")
                    first = False
                
                # Early Stopping: é˜²æ­¢ AR å¹»è§‰å¾ªç¯
                if chunk_count > max_chunks:
                    logger.warning(f"âš ï¸ Early Stop: å·²è¾¾ {chunk_count} chunks (ä¸Šé™ {max_chunks})ï¼Œå¼ºåˆ¶æˆªæ–­")
                    # è¾“å‡ºå‰©ä½™ç¼“å†²
                    if pending_chunks:
                        merged = np.concatenate(pending_chunks)
                        yield (merged * 32767).astype(np.int16).tobytes()
                    break
                
                pending_chunks.append(chunk)
                
                # Chunk åˆå¹¶: ç§¯æ”’å¤Ÿäº†å†å‘é€
                if len(pending_chunks) >= MERGE_COUNT:
                    merged = np.concatenate(pending_chunks)
                    yield (merged * 32767).astype(np.int16).tobytes()
                    pending_chunks = []
            
            # è¾“å‡ºæœ€åçš„ç¼“å†²
            if pending_chunks:
                merged = np.concatenate(pending_chunks)
                yield (merged * 32767).astype(np.int16).tobytes()
                
            logger.info(f"æµå¼å®Œæˆ: {chunk_count} chunks, {(time.time()-start)*1000:.0f}ms")
                    
        except Exception as e:
            logger.error(f"æµå¼å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            yield b""


@asynccontextmanager
async def lifespan(app: FastAPI):
    global mouth
    logger.info(f"ğŸ‘„ Cortex-Mouth-Daily å¯åŠ¨ä¸­ (ç«¯å£ {SERVICE_PORT})...")
    mouth = DailyMouthHandler()
    await mouth.initialize()
    if mouth.is_ready:
        logger.success(f"âœ… Mouth-Daily å°±ç»ª (ç«¯å£ {SERVICE_PORT})")
    yield
    logger.info("ğŸ›‘ Mouth-Daily å…³é—­")


app = FastAPI(lifespan=lifespan, title="Cortex-Mouth-Daily")

# æ‰˜ç®¡é™æ€æ–‡ä»¶ (ç”¨äºæµ‹è¯•é¡µé¢)
static_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "static")
if os.path.exists(static_dir):
    app.mount("/test", StaticFiles(directory=static_dir, html=True), name="static")

@app.get("/")
async def root():
    return FileResponse(os.path.join(static_dir, "index.html"))

# CORS æ”¯æŒ - å…è®¸å‰ç«¯è®¿é—®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/health")
async def health():
    prompt_exists = mouth and os.path.exists(mouth.default_prompt_wav) if mouth else False
    return {
        "service": "mouth-daily",
        "status": "ok" if mouth and mouth.is_ready else "loading",
        "model": "VoxCPM 1.5 (optimize=True, mode=default)",
        "sample_rate": 44100,  # VoxCPM 1.5 é«˜ä¿çœŸé‡‡æ ·ç‡
        "ttfa_target": "~285ms",
        "config": mouth.config if mouth else {},
        "voice_prompt": {
            "enabled": prompt_exists,
            "path": mouth.default_prompt_wav if mouth else None,
        }
    }


@app.post("/tts")
async def tts(request: dict):
    """
    éæµå¼ TTS
    
    Request body:
        text: è¦åˆæˆçš„æ–‡æœ¬ (å¿…å¡«)
        inference_timesteps: æ­¥æ•° (å¯é€‰, é»˜è®¤ 4, èŒƒå›´ 2-10)
        cfg_value: CFG å€¼ (å¯é€‰, é»˜è®¤ 2.0, èŒƒå›´ 1.0-3.0)
        prompt_wav_path: å‚è€ƒéŸ³é¢‘è·¯å¾„ (å¯é€‰, é»˜è®¤ä½¿ç”¨å†…ç½®å¥³å£°)
        prompt_text: å‚è€ƒéŸ³é¢‘æ–‡æœ¬ (å¯é€‰)
    """
    if not mouth or not mouth.is_ready:
        return {"error": "Not ready"}
    text = request.get("text", "")
    if not text:
        return {"error": "text required"}
    audio = mouth.synthesize(
        text, 
        request.get("inference_timesteps"), 
        request.get("cfg_value"),
        request.get("prompt_wav_path"),
        request.get("prompt_text"),
    )
    if not audio:
        return {"error": "failed"}
    return Response(content=audio, media_type="audio/wav")


@app.post("/tts/stream")
async def tts_stream(request: dict):
    """
    æµå¼ TTS
    
    Request body:
        text: è¦åˆæˆçš„æ–‡æœ¬ (å¿…å¡«)
        inference_timesteps: æ­¥æ•° (å¯é€‰, é»˜è®¤ 4, èŒƒå›´ 2-10)
        cfg_value: CFG å€¼ (å¯é€‰, é»˜è®¤ 2.0, èŒƒå›´ 1.0-3.0)
        prompt_wav_path: å‚è€ƒéŸ³é¢‘è·¯å¾„ (å¯é€‰, é»˜è®¤ä½¿ç”¨å†…ç½®å¥³å£°)
        prompt_text: å‚è€ƒéŸ³é¢‘æ–‡æœ¬ (å¯é€‰)
    """
    if not mouth or not mouth.is_ready:
        return {"error": "Not ready"}
    text = request.get("text", "")
    if not text:
        return {"error": "text required"}
    return StreamingResponse(
        mouth.synthesize_stream(
            text, 
            request.get("inference_timesteps"), 
            request.get("cfg_value"),
            request.get("prompt_wav_path"),
            request.get("prompt_text"),
        ),
        media_type="audio/pcm",
        headers={"X-Sample-Rate": "44100"}  # VoxCPM 1.5 é«˜ä¿çœŸé‡‡æ ·ç‡
    )


if __name__ == "__main__":
    import uvicorn
    # å†æ¬¡ç¡®ä¿ç«¯å£æ¸…ç†
    kill_port(SERVICE_PORT)
    uvicorn.run(app, host="0.0.0.0", port=SERVICE_PORT)
