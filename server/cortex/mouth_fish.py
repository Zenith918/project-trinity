"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ‘„ CORTEX-MOUTH-FISH (ç«¯å£ 9006)                                             â•‘
â•‘  Fish Speech 1.5 - äººæƒ…å‘³è¯­éŸ³åˆæˆ                                             â•‘
â•‘                                                                              â•‘
â•‘  ğŸ”¥ ç‰¹æ€§ï¼š                                                                    â•‘
â•‘    - åŸç”Ÿ 44.1kHz è¾“å‡ºï¼ˆæ— éœ€é‡é‡‡æ ·ï¼‰                                          â•‘
â•‘    - BF16 ç²¾åº¦ (4090 æœ€ä¼˜)                                                    â•‘
â•‘    - æ”¯æŒæƒ…æ„Ÿæ ‡ç­¾ [laughter], [sigh] ç­‰                                       â•‘
â•‘    - LLM + VQ-GAN åˆ†æ®µæµå¼æ¨ç†                                                â•‘
â•‘    - é˜²å¹»è§‰ï¼šRTF > 0.5 è‡ªåŠ¨æŠ¥é”™é‡ç½®                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import sys
import io
import wave
import time
import queue
import threading
import numpy as np
import torch

# æ·»åŠ  Fish Speech æºç è·¯å¾„
FISH_SPEECH_PATH = "/workspace/models/fish-speech"
sys.path.insert(0, FISH_SPEECH_PATH)
os.chdir(FISH_SPEECH_PATH)

from loguru import logger
from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.responses import StreamingResponse, Response
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from typing import Optional, List
import tempfile

# ============================================================================
# é…ç½®å¸¸é‡
# ============================================================================
SERVICE_PORT = 9006
MODEL_DIR = "/workspace/models/FishSpeech/fish-speech-1.5"

# Fish Speech åŸç”Ÿ 44.1kHz
OUTPUT_SAMPLE_RATE = 44100

# é˜²å¹»è§‰é˜ˆå€¼ï¼šRTF è¶…è¿‡ 0.5 è§†ä¸ºå¼‚å¸¸
MAX_RTF = 0.5

mouth = None


class FishSpeechHandler:
    """Fish Speech 1.5 å¤„ç†å™¨"""
    
    def __init__(self):
        self.model_manager = None
        self.is_ready = False
        self.sample_rate = OUTPUT_SAMPLE_RATE
        self.lock = threading.Lock()
        
        # é»˜è®¤å‚è€ƒéŸ³é¢‘
        self.default_reference = "/workspace/project-trinity/project-trinity/assets/prompt_female.wav"
        
    async def initialize(self):
        logger.info("=" * 60)
        logger.info("æ­£åœ¨åˆå§‹åŒ– Fish Speech 1.5...")
        logger.info(f"æ¨¡å‹ç›®å½•: {MODEL_DIR}")
        logger.info("=" * 60)
        
        try:
            from tools.server.model_manager import ModelManager
            
            start_time = time.time()
            
            # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
            self.model_manager = ModelManager(
                mode="tts",
                device="cuda",
                half=False,           # ä½¿ç”¨ BF16ï¼Œä¸æ˜¯ FP16
                compile=True,         # ğŸ”¥ å¯ç”¨ torch.compile
                llama_checkpoint_path=MODEL_DIR,
                decoder_checkpoint_path=f"{MODEL_DIR}/firefly-gan-vq-fsq-8x1024-21hz-generator.pth",
                decoder_config_name="firefly_gan_vq",
            )
            
            load_time = time.time() - start_time
            logger.info(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶ {load_time:.1f}s")
            
            # è·å–å®é™…é‡‡æ ·ç‡
            if hasattr(self.model_manager, 'engine') and hasattr(self.model_manager.engine, 'decoder_model'):
                if hasattr(self.model_manager.engine.decoder_model, 'sample_rate'):
                    self.sample_rate = self.model_manager.engine.decoder_model.sample_rate
                    logger.info(f"å®é™…é‡‡æ ·ç‡: {self.sample_rate}Hz")
            
            # é¢„çƒ­æ¨ç†
            logger.info("é¢„çƒ­æ¨ç†ä¸­...")
            warmup_start = time.time()
            
            # ç®€å•é¢„çƒ­
            _ = self._synthesize_internal("é¢„çƒ­", streaming=False)
            
            warmup_time = time.time() - warmup_start
            logger.success(f"âœ… é¢„çƒ­å®Œæˆï¼Œè€—æ—¶ {warmup_time:.1f}s")
            
            self.is_ready = True
            total_time = time.time() - start_time
            logger.success(f"âœ… Fish Speech 1.5 åˆå§‹åŒ–å®Œæˆï¼æ€»è€—æ—¶ {total_time:.1f}s")
            return True
            
        except Exception as e:
            logger.error(f"Fish Speech åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def _synthesize_internal(self, text: str, 
                             reference_audio: Optional[str] = None,
                             streaming: bool = False):
        """å†…éƒ¨åˆæˆæ–¹æ³•"""
        from fish_speech.utils.schema import ServeTTSRequest
        
        # æ„å»ºè¯·æ±‚
        req = ServeTTSRequest(
            text=text,
            references=[],  # å¯ä»¥æ·»åŠ å‚è€ƒéŸ³é¢‘
            reference_id=None,
            streaming=streaming,
            chunk_length=200 if streaming else 0,  # æµå¼åˆ†å—
            top_p=0.7,
            temperature=0.7,
            repetition_penalty=1.2,
            max_new_tokens=2048,
            use_memory_cache=True,
        )
        
        return self.model_manager.engine.inference(req)

    def synthesize(self, text: str, 
                   reference_audio: Optional[str] = None) -> bytes:
        """éæµå¼åˆæˆ"""
        if not self.is_ready:
            return b""
        
        try:
            start = time.time()
            
            with self.lock:
                results = list(self._synthesize_internal(text, reference_audio, streaming=False))
            
            # æŸ¥æ‰¾æœ€ç»ˆéŸ³é¢‘
            audio_data = None
            for result in results:
                if result.code == "final" and result.audio is not None:
                    sr, audio_data = result.audio
                    self.sample_rate = sr
                elif result.code == "error":
                    logger.error(f"åˆæˆé”™è¯¯: {result.error}")
                    return b""
            
            if audio_data is None:
                return b""
            
            elapsed = time.time() - start
            audio_duration = len(audio_data) / self.sample_rate
            rtf = elapsed / audio_duration if audio_duration > 0 else float('inf')
            
            # é˜²å¹»è§‰æ£€æŸ¥
            if rtf > MAX_RTF:
                logger.warning(f"âš ï¸ RTF å¼‚å¸¸: {rtf:.2f} > {MAX_RTF}ï¼Œå¯èƒ½å­˜åœ¨å¹»è§‰")
            
            # è½¬æ¢ä¸º WAV
            audio_int16 = (audio_data * 32767).astype(np.int16)
            buf = io.BytesIO()
            with wave.open(buf, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(self.sample_rate)
                wf.writeframes(audio_int16.tobytes())
            
            logger.info(f"åˆæˆå®Œæˆ: {len(text)}å­—, {elapsed*1000:.0f}ms, RTF={rtf:.2f}")
            
            return buf.getvalue()
            
        except Exception as e:
            logger.error(f"åˆæˆå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return b""

    def synthesize_stream(self, text: str,
                          reference_audio: Optional[str] = None):
        """æµå¼åˆæˆ"""
        if not self.is_ready:
            yield b""
            return
        
        try:
            start = time.time()
            first_chunk = True
            chunk_count = 0
            total_audio_duration = 0
            
            with self.lock:
                for result in self._synthesize_internal(text, reference_audio, streaming=True):
                    if result.code == "header":
                        # è·³è¿‡ WAV headerï¼ˆæˆ‘ä»¬å‘é€ raw PCMï¼‰
                        continue
                    elif result.code == "segment":
                        chunk_count += 1
                        sr, audio_chunk = result.audio
                        self.sample_rate = sr
                        
                        if first_chunk:
                            ttfa = (time.time() - start) * 1000
                            logger.info(f"TTFA: {ttfa:.0f}ms")
                            first_chunk = False
                        
                        # è½¬æ¢ä¸º int16
                        audio_int16 = (audio_chunk * 32767).astype(np.int16)
                        total_audio_duration += len(audio_chunk) / sr
                        
                        # é˜²å¹»è§‰æ£€æŸ¥
                        elapsed = time.time() - start
                        if total_audio_duration > 0 and elapsed / total_audio_duration > MAX_RTF * 3:
                            logger.warning(f"âš ï¸ æ£€æµ‹åˆ°å¹»è§‰ï¼Œå¼ºåˆ¶æˆªæ–­ï¼RTF={elapsed/total_audio_duration:.2f}")
                            break
                        
                        yield audio_int16.tobytes()
                        
                    elif result.code == "error":
                        logger.error(f"æµå¼é”™è¯¯: {result.error}")
                        yield b""
                        return
                    elif result.code == "final":
                        # æµå¼æ¨¡å¼ä¸‹ final ä¹‹å‰å·²ç»å‘é€äº†æ‰€æœ‰ segment
                        pass
            
            logger.info(f"æµå¼å®Œæˆ: {chunk_count} chunks")
                    
        except Exception as e:
            logger.error(f"æµå¼åˆæˆå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            yield b""


@asynccontextmanager
async def lifespan(app: FastAPI):
    global mouth
    logger.info(f"ğŸ‘„ Cortex-Mouth-Fish å¯åŠ¨ä¸­ (ç«¯å£ {SERVICE_PORT})...")
    mouth = FishSpeechHandler()
    success = await mouth.initialize()
    if success:
        logger.success(f"âœ… Mouth-Fish å°±ç»ª (ç«¯å£ {SERVICE_PORT})")
    else:
        logger.error("âŒ Mouth-Fish åˆå§‹åŒ–å¤±è´¥")
    yield
    logger.info("ğŸ›‘ Mouth-Fish å…³é—­")


app = FastAPI(lifespan=lifespan, title="Cortex-Mouth-Fish (Fish Speech 1.5)")

# CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/health")
async def health():
    return {
        "service": "mouth-fish",
        "status": "ok" if mouth and mouth.is_ready else "loading",
        "model": "Fish Speech 1.5 (BF16, compile=True)",
        "sample_rate": mouth.sample_rate if mouth else OUTPUT_SAMPLE_RATE,
        "features": ["emotion_tags", "reference_audio", "anti_hallucination"],
        "emotion_tags": ["[laughter]", "[sigh]", "[breath]", "[cough]"],
    }


@app.post("/tts")
async def tts(request: dict):
    """
    éæµå¼ TTS æ¥å£
    
    è¯·æ±‚ä½“:
    {
        "text": "è¦åˆæˆçš„æ–‡æœ¬ï¼Œæ”¯æŒ [laughter] [sigh] ç­‰æƒ…æ„Ÿæ ‡ç­¾",
        "reference_audio": "å¯é€‰ï¼Œå‚è€ƒéŸ³é¢‘è·¯å¾„"
    }
    """
    if not mouth or not mouth.is_ready:
        raise HTTPException(status_code=503, detail="Service not ready")
    
    text = request.get("text", "")
    if not text:
        raise HTTPException(status_code=400, detail="text required")
    
    audio = mouth.synthesize(
        text=text,
        reference_audio=request.get("reference_audio"),
    )
    
    if not audio:
        raise HTTPException(status_code=500, detail="Synthesis failed")
    
    return Response(content=audio, media_type="audio/wav")


@app.post("/tts/stream")
async def tts_stream(request: dict):
    """
    æµå¼ TTS æ¥å£
    
    è¿”å› PCM éŸ³é¢‘æµ (44.1kHz, 16bit, mono)
    """
    if not mouth or not mouth.is_ready:
        raise HTTPException(status_code=503, detail="Service not ready")
    
    text = request.get("text", "")
    if not text:
        raise HTTPException(status_code=400, detail="text required")
    
    return StreamingResponse(
        mouth.synthesize_stream(
            text=text,
            reference_audio=request.get("reference_audio"),
        ),
        media_type="audio/pcm",
        headers={"X-Sample-Rate": str(mouth.sample_rate)}
    )


if __name__ == "__main__":
    import uvicorn
    from server.utils.port_utils import kill_port
    
    kill_port(SERVICE_PORT)
    uvicorn.run(app, host="0.0.0.0", port=SERVICE_PORT)


