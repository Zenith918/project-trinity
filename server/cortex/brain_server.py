"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ§  CORTEX-BRAIN SERVER (ç«¯å£ 9000)                                           â•‘
â•‘  åªè´Ÿè´£ LLM æ¨ç† (Qwen2.5-VL)                                                 â•‘
â•‘  å¯ç‹¬ç«‹é‡å¯ï¼Œä¸å½±å“ Mouth å’Œ Ear                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import sys
import json
import subprocess
from loguru import logger

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”§ è‡ªåŠ¨ç«¯å£æ¸…ç†
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SERVICE_PORT = 9000

def kill_port(port: int):
    """æ€æ‰å ç”¨æŒ‡å®šç«¯å£çš„è¿›ç¨‹"""
    try:
        subprocess.run(f"fuser -k {port}/tcp 2>/dev/null || true", shell=True, timeout=5)
    except Exception:
        pass

kill_port(SERVICE_PORT)
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from contextlib import asynccontextmanager

# å…¨å±€æ¨¡å‹å®ä¾‹
brain = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global brain
    logger.info("ğŸ§  Cortex-Brain Server å¯åŠ¨ä¸­...")
    
    from server.cortex.models.brain import BrainHandler
    brain = BrainHandler()
    await brain.initialize()
    
    logger.success("âœ… Brain Server å°±ç»ª (ç«¯å£ 9000)")
    yield
    
    logger.info("ğŸ›‘ Brain Server å…³é—­ä¸­...")
    if brain:
        await brain.shutdown()

app = FastAPI(lifespan=lifespan, title="Cortex-Brain")

@app.get("/health")
async def health():
    return {
        "service": "brain",
        "status": "ok" if brain and brain.is_ready else "loading",
        "model": "Qwen2.5-VL-7B-AWQ"
    }

@app.post("/chat")
async def chat(request: dict):
    """éæµå¼èŠå¤©"""
    if not brain or not brain.is_ready:
        return {"error": "Brain not ready"}
    return await brain.generate(request)

@app.post("/chat/stream")
async def chat_stream(request: dict):
    """æµå¼èŠå¤© (SSE)"""
    if not brain or not brain.is_ready:
        return {"error": "Brain not ready"}
    
    async def event_generator():
        try:
            async for token in brain.generate_stream(request):
                yield f"data: {json.dumps({'token': token})}\n\n"
            yield f"data: {json.dumps({'done': True})}\n\n"
        except Exception as e:
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )

if __name__ == "__main__":
    import uvicorn
    kill_port(SERVICE_PORT)  # å¯åŠ¨å‰å†æ¬¡ç¡®ä¿ç«¯å£æ¸…ç†
    uvicorn.run(app, host="0.0.0.0", port=SERVICE_PORT)

