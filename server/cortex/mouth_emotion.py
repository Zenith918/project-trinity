"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ’‹ CORTEX-MOUTH-EMOTION (ç«¯å£ 9004)                                          â•‘
â•‘  IndexTTS 2.5 - æƒ…æ„Ÿå¢å¼º TTS æœåŠ¡                                              â•‘
â•‘                                                                              â•‘
â•‘  ğŸ­ ç‰¹æ€§ï¼š                                                                    â•‘
â•‘    - 8ç»´æƒ…æ„Ÿå‘é‡æ§åˆ¶ (happy/angry/sad/afraid/disgusted/melancholic/surprised/calm)â•‘
â•‘    - è‡ªåŠ¨æ–‡æœ¬æƒ…æ„Ÿåˆ†æ (use_emo_text=True)                                      â•‘
â•‘    - torch.compile åŠ é€Ÿ                                                       â•‘
â•‘    - 22kHz -> 44.1kHz å®æ—¶é‡é‡‡æ ·                                               â•‘
â•‘    - AR å¹»è§‰æˆªæ–­ä¿æŠ¤                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import sys

# ğŸ”‘ è®¾ç½® IndexTTS æ¨¡å—è·¯å¾„
INDEXTTS_PATH = "/workspace/models/IndexTTS2.5/index-tt2.5"
sys.path.insert(0, INDEXTTS_PATH)
os.chdir(INDEXTTS_PATH)  # IndexTTS ä¾èµ–ç›¸å¯¹è·¯å¾„åŠ è½½ checkpoints

# ç¦ç”¨ CUDA Graph (ä¸æµå¼ä¸å…¼å®¹)
os.environ['TORCHINDUCTOR_CUDAGRAPHS'] = '0'

import torch
# torch 2.4.x API å…¼å®¹æ€§å¤„ç†
try:
    torch._inductor.config.triton.cudagraphs = False
except AttributeError:
    pass  # torch 2.4.x ä¸éœ€è¦è¿™ä¸ªè®¾ç½®

import io
import wave
import time
import numpy as np
import torchaudio
from loguru import logger
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse, Response
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from contextlib import asynccontextmanager
from typing import Optional, List

# ============================================================================
# é…ç½®å¸¸é‡
# ============================================================================
SERVICE_PORT = 9004
MODEL_DIR = os.path.join(INDEXTTS_PATH, "checkpoints")
CFG_PATH = os.path.join(MODEL_DIR, "config.yaml")

# é‡‡æ ·ç‡é…ç½®
NATIVE_SAMPLE_RATE = 22050  # IndexTTS åŸç”Ÿé‡‡æ ·ç‡
OUTPUT_SAMPLE_RATE = 44100  # ç»Ÿä¸€è¾“å‡ºé‡‡æ ·ç‡

# AR å¹»è§‰æˆªæ–­é˜ˆå€¼ (tokens per character)
MAX_TOKENS_PER_CHAR = 10

mouth = None


class EmotionMouthHandler:
    """IndexTTS 2.5 å¤„ç†å™¨ - æƒ…æ„Ÿå¢å¼ºç‰ˆ"""
    
    def __init__(self):
        self.model = None
        self.is_ready = False
        self.native_sample_rate = NATIVE_SAMPLE_RATE
        self.output_sample_rate = OUTPUT_SAMPLE_RATE
        self.resampler = None  # å»¶è¿Ÿåˆå§‹åŒ–
        
        # é»˜è®¤ prompt éŸ³é¢‘ (ä½¿ç”¨ç°æœ‰çš„)
        self.default_prompt_wav = "/workspace/project-trinity/project-trinity/assets/prompt_female.wav"
        
        # æƒ…æ„Ÿå‘é‡é¡ºåº (ä¸ IndexTTS2 å†…éƒ¨ä¸€è‡´)
        self.emotion_keys = ["happy", "angry", "sad", "afraid", "disgusted", "melancholic", "surprised", "calm"]
        
    async def initialize(self):
        logger.info("=" * 60)
        logger.info("æ­£åœ¨åˆå§‹åŒ– IndexTTS 2.5 (æƒ…æ„Ÿå¢å¼ºç‰ˆ)...")
        logger.info(f"æ¨¡å‹ç›®å½•: {MODEL_DIR}")
        logger.info(f"é…ç½®æ–‡ä»¶: {CFG_PATH}")
        logger.info("âš ï¸ é¦–æ¬¡å¯åŠ¨éœ€è¦åŠ è½½å¤šä¸ªæ¨¡å‹ + torch.compile é¢„çƒ­ï¼Œè¯·è€å¿ƒç­‰å¾…ï¼")
        logger.info("=" * 60)
        
        try:
            from indextts.infer_v2 import IndexTTS2
            
            start_time = time.time()
            
            # ğŸ”¥ ä¿®å¤ç‰ˆé…ç½®ï¼šé¿å… CUDA Graph å†²çª
            # ============================================================
            # 2026-01-11 ä¿®å¤ï¼šé™é»˜å´©æºƒé—®é¢˜
            # æ ¹å› ï¼šuse_accel (GPT CUDA Graph) ä¸ use_torch_compile å†²çª
            # æ–¹æ¡ˆï¼šç¦ç”¨ torch.compileï¼Œä¿ç•™æ¨¡å‹åŸç”Ÿ accel å¼•æ“
            # ============================================================
            
            # æ­¥éª¤ 4: æ˜¾å­˜é˜²ç¢ç‰‡ (åœ¨åŠ è½½å‰æ¸…ç†)
            torch.cuda.empty_cache()
            
            self.model = IndexTTS2(
                cfg_path=CFG_PATH,
                model_dir=MODEL_DIR,
                use_fp16=True,           # ğŸ”‘ FP16 åŠ é€Ÿ
                use_cuda_kernel=True,   # BigVGAN CUDA kernel (éœ€è¦ Ninja ç¼–è¯‘ï¼Œç¦ç”¨)
                use_deepspeed=False,     # å•ç”¨æˆ·ä¸éœ€è¦
                use_accel=True,          # ğŸ”‘ ä¿ç•™ï¼šæ¨¡å‹åŸç”Ÿ CUDA Graph (GPT åŠ é€Ÿ)
                use_torch_compile=False, # ğŸ”‘ ç¦ç”¨ï¼šé˜²æ­¢ä¸ accel å†²çª
                device="cuda:0"
            )
            
            # åŠ è½½åå†æ¬¡æ¸…ç†ç¢ç‰‡
            torch.cuda.empty_cache()
            
            load_time = time.time() - start_time
            logger.info(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶ {load_time:.1f}s")
            
            # åˆå§‹åŒ–é‡é‡‡æ ·å™¨ (22kHz -> 44.1kHz)
            self.resampler = torchaudio.transforms.Resample(
                orig_freq=self.native_sample_rate,
                new_freq=self.output_sample_rate
            ).cuda()
            logger.info(f"é‡é‡‡æ ·å™¨å°±ç»ª: {self.native_sample_rate}Hz -> {self.output_sample_rate}Hz")
            
            # ğŸ”¥ é¢„çƒ­æ¨ç† (è§¦å‘ torch.compile JIT)
            logger.info("é¢„çƒ­æ¨ç†ä¸­ (è§¦å‘ JIT ç¼–è¯‘ï¼Œå¯èƒ½éœ€è¦ 30-60 ç§’)...")
            warmup_start = time.time()
            
            # ä½¿ç”¨ä¸€ä¸ªç®€çŸ­çš„"ã€‚"æ¥é¢„çƒ­
            _ = list(self.model.infer_generator(
                spk_audio_prompt=self.default_prompt_wav,
                text="ã€‚",  # æœ€çŸ­çš„åˆæ³•è¾“å…¥
                output_path=None,
                stream_return=False,
                verbose=False
            ))
            
            warmup_time = time.time() - warmup_start
            logger.success(f"âœ… é¢„çƒ­å®Œæˆï¼Œè€—æ—¶ {warmup_time:.1f}s")
            
            self.is_ready = True
            total_time = time.time() - start_time
            logger.success(f"âœ… IndexTTS 2.5 åˆå§‹åŒ–å®Œæˆï¼æ€»è€—æ—¶ {total_time:.1f}s")
            return True
            
        except Exception as e:
            logger.error(f"IndexTTS åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def _resample_to_44k(self, audio_tensor: torch.Tensor) -> torch.Tensor:
        """å°†éŸ³é¢‘ä» 22kHz é‡é‡‡æ ·åˆ° 44.1kHz"""
        if audio_tensor.dim() == 1:
            audio_tensor = audio_tensor.unsqueeze(0)
        return self.resampler(audio_tensor.cuda()).cpu()

    def synthesize(self, text: str, 
                   prompt_wav_path: Optional[str] = None,
                   emo_vector: Optional[List[float]] = None,
                   use_emo_text: bool = False) -> bytes:
        """éæµå¼åˆæˆ"""
        if not self.is_ready:
            return b""
        
        prompt_wav = prompt_wav_path or self.default_prompt_wav
        
        # AR å¹»è§‰ä¿æŠ¤ï¼šé¢„ä¼°æœ€å¤§åˆç†é•¿åº¦
        text_len = len(text.replace(" ", ""))
        max_mel_tokens = min(1500, max(200, text_len * MAX_TOKENS_PER_CHAR))
        
        try:
            start = time.time()
            
            # è°ƒç”¨ IndexTTS2 æ¨ç†
            result = list(self.model.infer_generator(
                spk_audio_prompt=prompt_wav,
                text=text,
                output_path=None,
                emo_vector=emo_vector,
                use_emo_text=use_emo_text,
                stream_return=False,
                verbose=False,
                max_mel_tokens=max_mel_tokens,  # å¹»è§‰æˆªæ–­
            ))
            
            if not result:
                logger.warning("IndexTTS è¿”å›ç©ºç»“æœ")
                return b""
            
            # result[-1] æ˜¯ (sample_rate, wav_data) å…ƒç»„
            sr, wav_data = result[-1]
            
            # é‡é‡‡æ ·åˆ° 44.1kHz
            wav_tensor = torch.from_numpy(wav_data.T.astype(np.float32) / 32767.0)
            wav_44k = self._resample_to_44k(wav_tensor)
            
            # è½¬æ¢ä¸º WAV å­—èŠ‚
            audio_int16 = (wav_44k.squeeze().numpy() * 32767).astype(np.int16)
            buf = io.BytesIO()
            with wave.open(buf, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(self.output_sample_rate)
                wf.writeframes(audio_int16.tobytes())
            
            elapsed = (time.time() - start) * 1000
            logger.info(f"åˆæˆå®Œæˆ: {text_len}å­—, {elapsed:.0f}ms, emo={emo_vector or 'auto' if use_emo_text else 'none'}")
            
            return buf.getvalue()
            
        except Exception as e:
            logger.error(f"åˆæˆå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return b""

    def synthesize_stream(self, text: str,
                          prompt_wav_path: Optional[str] = None,
                          emo_vector: Optional[List[float]] = None,
                          use_emo_text: bool = False):
        """æµå¼åˆæˆ (Generator)"""
        if not self.is_ready:
            yield b""
            return
        
        prompt_wav = prompt_wav_path or self.default_prompt_wav
        
        # AR å¹»è§‰ä¿æŠ¤
        text_len = len(text.replace(" ", ""))
        max_mel_tokens = min(1500, max(200, text_len * MAX_TOKENS_PER_CHAR))
        max_chunks = max(10, text_len * 5)  # æ¯ä¸ªå­—æœ€å¤š 5 ä¸ª chunk
        
        try:
            start = time.time()
            first_chunk = True
            chunk_count = 0
            
            for chunk in self.model.infer_generator(
                spk_audio_prompt=prompt_wav,
                text=text,
                output_path=None,
                emo_vector=emo_vector,
                use_emo_text=use_emo_text,
                stream_return=True,
                verbose=False,
                max_mel_tokens=max_mel_tokens,
            ):
                chunk_count += 1
                
                # å¹»è§‰æˆªæ–­
                if chunk_count > max_chunks:
                    logger.warning(f"âš ï¸ Early Stop: æ£€æµ‹åˆ°æµå¼å¹»è§‰ï¼Œå¼ºåˆ¶æˆªæ–­ï¼å·²è¾“å‡º {chunk_count} chunks")
                    break
                
                if first_chunk:
                    ttfa = (time.time() - start) * 1000
                    logger.info(f"TTFA: {ttfa:.0f}ms")
                    first_chunk = False
                
                # chunk æ˜¯ torch.Tensor, shape [1, samples] æˆ– [samples]
                if isinstance(chunk, torch.Tensor):
                    # é‡é‡‡æ ·åˆ° 44.1kHz
                    wav_44k = self._resample_to_44k(chunk.float() / 32767.0)
                    audio_int16 = (wav_44k.squeeze().numpy() * 32767).astype(np.int16)
                    yield audio_int16.tobytes()
                    
        except Exception as e:
            logger.error(f"æµå¼åˆæˆå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            yield b""


@asynccontextmanager
async def lifespan(app: FastAPI):
    global mouth
    logger.info(f"ğŸ’‹ Cortex-Mouth-Emotion å¯åŠ¨ä¸­ (ç«¯å£ {SERVICE_PORT})...")
    mouth = EmotionMouthHandler()
    success = await mouth.initialize()
    if success:
        logger.success(f"âœ… Mouth-Emotion å°±ç»ª (ç«¯å£ {SERVICE_PORT})")
    else:
        logger.error("âŒ Mouth-Emotion åˆå§‹åŒ–å¤±è´¥")
    yield
    logger.info("ğŸ›‘ Mouth-Emotion å…³é—­")


app = FastAPI(lifespan=lifespan, title="Cortex-Mouth-Emotion (IndexTTS 2.5)")

# CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/health")
async def health():
    return {
        "service": "mouth-emotion",
        "status": "ok" if mouth and mouth.is_ready else "loading",
        "model": "IndexTTS 2.5 (use_fp16=True, torch_compile=True)",
        "sample_rate": OUTPUT_SAMPLE_RATE,
        "native_sample_rate": NATIVE_SAMPLE_RATE,
        "emotion_keys": mouth.emotion_keys if mouth else [],
        "voice_prompt": mouth.default_prompt_wav if mouth else None,
    }


@app.post("/tts")
async def tts(request: dict):
    """
    éæµå¼ TTS æ¥å£
    
    è¯·æ±‚ä½“:
    {
        "text": "è¦åˆæˆçš„æ–‡æœ¬",
        "prompt_wav_path": "å¯é€‰ï¼Œå‚è€ƒéŸ³é¢‘è·¯å¾„",
        "emo_vector": [0.5, 0, 0, 0, 0, 0, 0, 0.5],  // å¯é€‰ï¼Œ8ç»´æƒ…æ„Ÿå‘é‡
        "use_emo_text": false  // å¯é€‰ï¼Œæ˜¯å¦ä»æ–‡æœ¬è‡ªåŠ¨æ¨æ–­æƒ…æ„Ÿ
    }
    """
    if not mouth or not mouth.is_ready:
        raise HTTPException(status_code=503, detail="Service not ready")
    
    text = request.get("text", "")
    if not text:
        raise HTTPException(status_code=400, detail="text required")
    
    audio = mouth.synthesize(
        text=text,
        prompt_wav_path=request.get("prompt_wav_path"),
        emo_vector=request.get("emo_vector"),
        use_emo_text=request.get("use_emo_text", False),
    )
    
    if not audio:
        raise HTTPException(status_code=500, detail="Synthesis failed")
    
    return Response(content=audio, media_type="audio/wav")


@app.post("/tts/stream")
async def tts_stream(request: dict):
    """
    æµå¼ TTS æ¥å£
    
    è¿”å› PCM éŸ³é¢‘æµ (44.1kHz, 16bit, mono)
    """
    if not mouth or not mouth.is_ready:
        raise HTTPException(status_code=503, detail="Service not ready")
    
    text = request.get("text", "")
    if not text:
        raise HTTPException(status_code=400, detail="text required")
    
    return StreamingResponse(
        mouth.synthesize_stream(
            text=text,
            prompt_wav_path=request.get("prompt_wav_path"),
            emo_vector=request.get("emo_vector"),
            use_emo_text=request.get("use_emo_text", False),
        ),
        media_type="audio/pcm",
        headers={"X-Sample-Rate": str(OUTPUT_SAMPLE_RATE)}
    )


@app.post("/analyze_emotion")
async def analyze_emotion(request: dict):
    """
    åˆ†ææ–‡æœ¬æƒ…æ„Ÿ
    
    è¯·æ±‚ä½“: {"text": "è¦åˆ†æçš„æ–‡æœ¬"}
    è¿”å›: {"happy": 0.5, "angry": 0, ...}
    """
    if not mouth or not mouth.is_ready:
        raise HTTPException(status_code=503, detail="Service not ready")
    
    text = request.get("text", "")
    if not text:
        raise HTTPException(status_code=400, detail="text required")
    
    try:
        emo_dict = mouth.model.qwen_emo.inference(text)
        return emo_dict
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    from server.utils.port_utils import kill_port
    
    kill_port(SERVICE_PORT)
    uvicorn.run(app, host="0.0.0.0", port=SERVICE_PORT)

