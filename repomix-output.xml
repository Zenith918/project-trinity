This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: server/**/*.py, client/**/*.py
- Files matching these patterns are excluded: *.wav, *.mp3, *.mp4, *.png, *.jpg, *.jpeg, *.gif, *.ico, *.svg, *.pyc, *.pyo, *.egg-info, *.egg, *.whl, __pycache__/, .git/, .venv/, venv/, env/, node_modules/, .pytest_cache/, .mypy_cache/, *.log, logs/, data/, test_*.py, *_test.py, tests/, test/, *.test.js, *.spec.js, assets/, scripts/, docs/, *.md, *.txt, *.json, *.xml, *.yaml, *.yml, *.lock, *.css, .env*, Dockerfile*, docker-compose*, Makefile, requirements*.txt, setup.py, setup.cfg, pyproject.toml, *.html, static/, repomix-output.xml, repomix.config.json
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
server/
  adapters/
    __init__.py
    base_adapter.py
    brain_adapter.py
    driver_adapter.py
    mouth_adapter.py
    voice_adapter.py
  config/
    __init__.py
    settings.py
  cortex/
    brain_server.py
    ear_server.py
    main.py
    mouth_cosyvoice2.py
    mouth_daily.py
    mouth_emotion.py
    mouth_fish.py
    mouth_server.py
  mind_engine/
    __init__.py
    bio_state.py
    ego_director.py
    narrative_mgr.py
  pipeline/
    __init__.py
    orchestrator.py
    packager.py
  static/
    audio/
      fix_wav.py
  utils/
    __init__.py
    conversation_logger.py
    port_utils.py
  main.py
  monitor.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="server/static/audio/fix_wav.py">
#!/usr/bin/env python3
"""
ä¿®å¤æµå¼ WAV æ–‡ä»¶çš„å¤´éƒ¨é•¿åº¦å­—æ®µ
æµå¼æ¥å£ç”Ÿæˆçš„ WAV å¤´åªåŒ…å«ç¬¬ä¸€ä¸ª chunk çš„é•¿åº¦ï¼Œéœ€è¦æ›´æ–°ä¸ºå®é™…æ€»é•¿åº¦
"""
import struct
import sys
import os

def fix_wav_header(input_path, output_path=None):
    """ä¿®å¤ WAV æ–‡ä»¶å¤´éƒ¨çš„é•¿åº¦å­—æ®µ"""
    if output_path is None:
        output_path = input_path
    
    with open(input_path, 'rb') as f:
        data = f.read()
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„ WAV æ–‡ä»¶
    if data[:4] != b'RIFF' or data[8:12] != b'WAVE':
        print(f"âŒ {input_path}: ä¸æ˜¯æœ‰æ•ˆçš„ WAV æ–‡ä»¶")
        return False
    
    # è®¡ç®—æ­£ç¡®çš„é•¿åº¦
    file_size = len(data)
    riff_size = file_size - 8  # RIFF chunk size = æ–‡ä»¶å¤§å° - 8
    
    # æ‰¾åˆ° data chunk çš„ä½ç½®
    data_pos = data.find(b'data')
    if data_pos == -1:
        print(f"âŒ {input_path}: æ‰¾ä¸åˆ° data chunk")
        return False
    
    data_size = file_size - data_pos - 8  # data chunk size = æ–‡ä»¶å¤§å° - dataä½ç½® - 8
    
    # ä¿®å¤å¤´éƒ¨
    fixed_data = bytearray(data)
    # ä¿®å¤ RIFF chunk size (ä½ç½® 4-7)
    struct.pack_into('<I', fixed_data, 4, riff_size)
    # ä¿®å¤ data chunk size (data_pos + 4)
    struct.pack_into('<I', fixed_data, data_pos + 4, data_size)
    
    with open(output_path, 'wb') as f:
        f.write(fixed_data)
    
    # è®¡ç®—æ—¶é•¿
    # å‡è®¾ 16-bit mono 24kHz
    duration = data_size / (24000 * 2)
    print(f"âœ… {os.path.basename(input_path)}: {file_size} bytes, {duration:.2f}s")
    return True

if __name__ == '__main__':
    if len(sys.argv) < 2:
        # ä¿®å¤å½“å‰ç›®å½•ä¸‹æ‰€æœ‰ test_*.wav
        import glob
        files = glob.glob('test_*.wav')
        if not files:
            print("æ²¡æœ‰æ‰¾åˆ° test_*.wav æ–‡ä»¶")
            sys.exit(1)
        for f in files:
            fix_wav_header(f)
    else:
        for f in sys.argv[1:]:
            fix_wav_header(f)
</file>

<file path="server/adapters/__init__.py">
"""
Project Trinity - AI Model Adapters
AI æ¨¡å‹é€‚é…å™¨åŒ…

ä½¿ç”¨ Adapter Pattern å°è£…æ‰€æœ‰ AI æ¨¡å‹ï¼Œä¾¿äºæœªæ¥æ›¿æ¢
"""

from .base_adapter import BaseAdapter
from .voice_adapter import VoiceAdapter
from .brain_adapter import BrainAdapter
from .mouth_adapter import MouthAdapter
from .driver_adapter import DriverAdapter

__all__ = [
    "BaseAdapter",
    "VoiceAdapter", 
    "BrainAdapter",
    "MouthAdapter",
    "DriverAdapter"
]
</file>

<file path="server/adapters/base_adapter.py">
"""
Project Trinity - Base Adapter Interface
åŸºç¡€é€‚é…å™¨æ¥å£

æ‰€æœ‰ AI æ¨¡å‹é€‚é…å™¨çš„æŠ½è±¡åŸºç±»ï¼Œå®ç° Adapter Pattern
ä¾¿äºæœªæ¥æ›¿æ¢åº•å±‚æ¨¡å‹ï¼ˆå¦‚ Qwen -> GPT-6ï¼‰
"""

from abc import ABC, abstractmethod
from typing import Any, Optional
import asyncio
from loguru import logger


class BaseAdapter(ABC):
    """
    AI æ¨¡å‹é€‚é…å™¨åŸºç±»
    
    æ‰€æœ‰é€‚é…å™¨å¿…é¡»å®ç°:
    - initialize(): åˆå§‹åŒ–æ¨¡å‹
    - process(): å¤„ç†è¾“å…¥
    - shutdown(): å…³é—­èµ„æº
    """
    
    def __init__(self, name: str):
        self.name = name
        self.is_initialized = False
        self._lock = asyncio.Lock()
        
    @abstractmethod
    async def initialize(self) -> bool:
        """
        åˆå§‹åŒ–æ¨¡å‹
        
        Returns:
            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        pass
    
    @abstractmethod
    async def process(self, input_data: Any) -> Any:
        """
        å¤„ç†è¾“å…¥æ•°æ®
        
        Args:
            input_data: è¾“å…¥æ•°æ®
            
        Returns:
            å¤„ç†ç»“æœ
        """
        pass
    
    @abstractmethod
    async def shutdown(self) -> None:
        """å…³é—­å¹¶é‡Šæ”¾èµ„æº"""
        pass
    
    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        return self.is_initialized
    
    def __repr__(self) -> str:
        status = "initialized" if self.is_initialized else "not initialized"
        return f"<{self.__class__.__name__}({self.name}) [{status}]>"
</file>

<file path="server/config/__init__.py">
"""
Project Trinity - Configuration Package
"""

from .settings import settings, Settings

__all__ = ["settings", "Settings"]
</file>

<file path="server/cortex/mouth_cosyvoice2.py">
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CosyVoice2 0.5B FastAPI æœåŠ¡
ç«¯å£: 9005
ç¯å¢ƒ: cosyvoice2_env (torch 2.3.1, flash-attn 2.5.8)

æ ¸å¿ƒç‰¹æ€§:
- çœŸæµå¼è¾“å‡º (stream=True)
- 44.1kHz é‡‡æ ·ç‡ (ä» 22.05kHz é‡é‡‡æ ·)
- FP16 ç²¾åº¦
- é¢„çƒ­æœºåˆ¶ç¡®ä¿ TTFA < 300ms
- RTF å®æ—¶ç›‘æ§
"""
import os
import sys
import time
import io
import wave

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ã€é“å¾‹ã€‘æ—¥å¿—é…ç½® - å¿…é¡»åŒæ—¶è¾“å‡ºåˆ°æ–‡ä»¶å’Œç»ˆç«¯ï¼Œç¡®ä¿å¯è¿½è¸ªï¼
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
from loguru import logger

# ç§»é™¤é»˜è®¤ handlerï¼Œé‡æ–°é…ç½®
logger.remove()
# ç»ˆç«¯è¾“å‡º (å½©è‰²)
logger.add(sys.stderr, level="DEBUG", format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{message}</cyan>")
# æ–‡ä»¶è¾“å‡º (è¯¦ç»†)
LOG_FILE = "/tmp/cv2_new.log"
logger.add(LOG_FILE, level="DEBUG", format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {message}", rotation="10 MB")
logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {LOG_FILE}")

# æ·»åŠ  CosyVoice åˆ°è·¯å¾„
sys.path.insert(0, '/workspace/models/CosyVoice')
os.chdir('/workspace/models/CosyVoice')

import numpy as np
import torch
import torchaudio
from fastapi import FastAPI, File, UploadFile, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, Response
from pydantic import BaseModel
from contextlib import asynccontextmanager
from typing import Optional
import tempfile

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# é…ç½®
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SERVICE_PORT = 9005
MODEL_DIR = "/workspace/models/CosyVoice/pretrained_models/iic/CosyVoice2-0___5B"
NATIVE_SAMPLE_RATE = 22050  # CosyVoice2 åŸç”Ÿé‡‡æ ·ç‡
TARGET_SAMPLE_RATE = 44100  # ç›®æ ‡é‡‡æ ·ç‡

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç«¯å£ç®¡ç†
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def kill_port(port: int):
    """æ€æ­»å ç”¨æŒ‡å®šç«¯å£çš„è¿›ç¨‹"""
    import subprocess
    try:
        result = subprocess.run(
            f"ss -tlnp | grep :{port} | awk '{{print $6}}' | grep -oP '(?<=pid=)\\d+' | xargs -r kill -9",
            shell=True, capture_output=True, text=True
        )
        if result.returncode == 0:
            logger.info(f"å·²æ¸…ç†ç«¯å£ {port}")
    except Exception as e:
        logger.warning(f"æ¸…ç†ç«¯å£å¤±è´¥: {e}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# è¯·æ±‚æ¨¡å‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class TTSRequest(BaseModel):
    text: str
    speaker_id: Optional[str] = None
    speed: float = 1.0

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CosyVoice2 æœåŠ¡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class CosyVoice2Service:
    def __init__(self):
        self.model = None
        self.resampler = None
        self.ready = False
        self.default_speaker = None
        
    def initialize(self):
        """
        åˆå§‹åŒ– CosyVoice2 æ¨¡å‹
        ã€é‡è¦ã€‘æ¯ä¸€æ­¥éƒ½æ‰“å°è¯¦ç»†æ—¥å¿—ï¼Œæ–¹ä¾¿æ’é”™ï¼
        """
        logger.info("=" * 70)
        logger.info("ğŸš€ å¼€å§‹åˆå§‹åŒ– CosyVoice2 0.5B")
        logger.info("=" * 70)
        
        try:
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Step 1: å¯¼å…¥æ¨¡å—
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            logger.info("[1/6] å¯¼å…¥ CosyVoice2 æ¨¡å—...")
            step_start = time.time()
            from cosyvoice.cli.cosyvoice import CosyVoice2
            logger.info(f"[1/6] âœ… æ¨¡å—å¯¼å…¥å®Œæˆ ({time.time()-step_start:.1f}s)")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Step 2: æ£€æŸ¥æ¨¡å‹ç›®å½•
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            logger.info(f"[2/6] æ£€æŸ¥æ¨¡å‹ç›®å½•: {MODEL_DIR}")
            if not os.path.exists(MODEL_DIR):
                raise FileNotFoundError(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {MODEL_DIR}")
            
            files = os.listdir(MODEL_DIR)
            logger.info(f"[2/6] âœ… æ¨¡å‹ç›®å½•å­˜åœ¨ï¼ŒåŒ…å« {len(files)} ä¸ªæ–‡ä»¶")
            for f in files[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                logger.info(f"       - {f}")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Step 3: åŠ è½½æ¨¡å‹ (è¿™æ˜¯æœ€è€—æ—¶çš„æ­¥éª¤)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            logger.info("[3/6] åŠ è½½ CosyVoice2 æ¨¡å‹ (fp16=True)...")
            logger.info("       â³ è¿™ä¸€æ­¥å¯èƒ½éœ€è¦ 1-3 åˆ†é’Ÿ...")
            step_start = time.time()
            
            # ä½¿ç”¨çº¿ç¨‹æ¥æ‰“å°å¿ƒè·³ï¼Œç¡®ä¿èƒ½è¿½è¸ªè¿›åº¦
            import threading
            loading_done = threading.Event()
            
            def heartbeat():
                """æ¯10ç§’æ‰“å°ä¸€æ¬¡å¿ƒè·³ï¼Œè¯æ˜æ²¡æœ‰å¡æ­»"""
                elapsed = 0
                while not loading_done.is_set():
                    time.sleep(10)
                    elapsed += 10
                    if not loading_done.is_set():
                        logger.info(f"       ğŸ’“ æ¨¡å‹åŠ è½½ä¸­... ({elapsed}s)")
            
            heartbeat_thread = threading.Thread(target=heartbeat, daemon=True)
            heartbeat_thread.start()
            
            try:
                # åŠ è½½æ¨¡å‹
                self.model = CosyVoice2(MODEL_DIR, fp16=True)
            finally:
                loading_done.set()
            
            logger.info(f"[3/6] âœ… æ¨¡å‹åŠ è½½å®Œæˆ ({time.time()-step_start:.1f}s)")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Step 4: è·å–è¯´è¯äººåˆ—è¡¨
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            logger.info("[4/6] è·å–å¯ç”¨è¯´è¯äºº...")
            speakers = self.model.list_available_spks()
            logger.info(f"[4/6] âœ… å¯ç”¨è¯´è¯äºº: {speakers}")
            
            if speakers:
                self.default_speaker = speakers[0]
                logger.info(f"[4/6] é»˜è®¤è¯´è¯äºº: {self.default_speaker}")
            else:
                logger.warning("[4/6] âš ï¸ æ²¡æœ‰å¯ç”¨è¯´è¯äººï¼")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Step 5: åˆ›å»ºé‡é‡‡æ ·å™¨
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            logger.info(f"[5/6] åˆ›å»ºé‡é‡‡æ ·å™¨ ({NATIVE_SAMPLE_RATE} -> {TARGET_SAMPLE_RATE})...")
            self.resampler = torchaudio.transforms.Resample(
                NATIVE_SAMPLE_RATE, TARGET_SAMPLE_RATE
            ).cuda()
            logger.info("[5/6] âœ… é‡é‡‡æ ·å™¨åˆ›å»ºå®Œæˆ")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # Step 6: é¢„çƒ­æ¨ç†
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            logger.info("[6/6] é¢„çƒ­æ¨ç†ä¸­...")
            step_start = time.time()
            
            if self.default_speaker:
                for _ in self.model.inference_sft("é¢„çƒ­æµ‹è¯•", self.default_speaker, stream=False):
                    pass
                warmup_time = (time.time() - step_start) * 1000
                logger.info(f"[6/6] âœ… é¢„çƒ­å®Œæˆ ({warmup_time:.0f}ms)")
            else:
                logger.warning("[6/6] âš ï¸ è·³è¿‡é¢„çƒ­ï¼ˆæ— é»˜è®¤è¯´è¯äººï¼‰")
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # å®Œæˆ
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            self.ready = True
            logger.info("=" * 70)
            logger.info("âœ…âœ…âœ… CosyVoice2 åˆå§‹åŒ–å®Œæˆï¼æœåŠ¡å·²å°±ç»ªï¼âœ…âœ…âœ…")
            logger.info("=" * 70)
            
        except Exception as e:
            logger.error("=" * 70)
            logger.error(f"âŒâŒâŒ CosyVoice2 åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.error("=" * 70)
            import traceback
            logger.error(traceback.format_exc())
            
    def synthesize(self, text: str, speaker_id: str = None, speed: float = 1.0) -> bytes:
        """éæµå¼åˆæˆ"""
        if not self.ready:
            return b''
            
        speaker = speaker_id or self.default_speaker
        start_time = time.time()
        
        audio_chunks = []
        for output in self.model.inference_sft(text, speaker, stream=False, speed=speed):
            audio = output['tts_speech']
            audio_chunks.append(audio)
            
        if not audio_chunks:
            return b''
            
        # åˆå¹¶éŸ³é¢‘
        full_audio = torch.cat(audio_chunks, dim=1)
        
        # é‡é‡‡æ ·åˆ° 44.1kHz
        if full_audio.device.type != 'cuda':
            full_audio = full_audio.cuda()
        resampled = self.resampler(full_audio)
        
        # è½¬æ¢ä¸º 16-bit PCM
        audio_np = (resampled.cpu().numpy() * 32767).astype(np.int16)
        
        # è®¡ç®— RTF
        audio_duration = len(audio_np.flatten()) / TARGET_SAMPLE_RATE
        total_time = time.time() - start_time
        rtf = total_time / audio_duration
        
        if rtf > 0.1:
            logger.warning(f"âš ï¸ RTF={rtf:.3f} > 0.1ï¼Œæ€§èƒ½è­¦å‘Šï¼")
        else:
            logger.info(f"åˆæˆå®Œæˆ: {len(text)}å­—, {total_time*1000:.0f}ms, RTF={rtf:.3f}")
        
        # åˆ›å»º WAV
        buffer = io.BytesIO()
        with wave.open(buffer, 'wb') as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)
            wf.setframerate(TARGET_SAMPLE_RATE)
            wf.writeframes(audio_np.tobytes())
        return buffer.getvalue()
        
    def synthesize_stream(self, text: str, speaker_id: str = None, speed: float = 1.0):
        """çœŸæµå¼åˆæˆ - æ¯ç”Ÿæˆä¸€æ®µå°±ç«‹å³è¿”å›"""
        if not self.ready:
            return
            
        speaker = speaker_id or self.default_speaker
        start_time = time.time()
        chunk_count = 0
        total_audio_duration = 0
        
        logger.info(f"å¼€å§‹æµå¼åˆæˆ: {text[:20]}...")
        
        # ä½¿ç”¨ stream=True è¿›è¡ŒçœŸæµå¼
        for output in self.model.inference_sft(text, speaker, stream=True, speed=speed):
            chunk_start = time.time()
            audio = output['tts_speech']
            
            # é‡é‡‡æ ·åˆ° 44.1kHz
            if audio.device.type != 'cuda':
                audio = audio.cuda()
            resampled = self.resampler(audio)
            
            # è½¬æ¢ä¸º 16-bit PCM
            audio_np = (resampled.cpu().numpy() * 32767).astype(np.int16).flatten()
            
            chunk_count += 1
            chunk_duration = len(audio_np) / TARGET_SAMPLE_RATE
            total_audio_duration += chunk_duration
            
            # è®¡ç®—å½“å‰ chunk çš„ RTF
            chunk_time = time.time() - chunk_start
            chunk_rtf = chunk_time / chunk_duration if chunk_duration > 0 else 0
            
            if chunk_count == 1:
                ttfa = (time.time() - start_time) * 1000
                logger.info(f"TTFA: {ttfa:.0f}ms")
                
            if chunk_rtf > 0.1:
                logger.warning(f"âš ï¸ Chunk {chunk_count}: RTF={chunk_rtf:.3f} > 0.1")
                
            yield audio_np.tobytes()
            
        # æ€»ç»“
        total_time = time.time() - start_time
        overall_rtf = total_time / total_audio_duration if total_audio_duration > 0 else 0
        logger.info(f"æµå¼åˆæˆå®Œæˆ: {chunk_count} chunks, æ€»RTF={overall_rtf:.3f}")

    def synthesize_zero_shot(self, text: str, prompt_wav_path: str, prompt_text: str, speed: float = 1.0) -> bytes:
        """Zero-shot å…‹éš†åˆæˆï¼ˆéæµå¼ï¼‰"""
        if not self.ready:
            logger.error("æœåŠ¡æœªå°±ç»ª")
            return b''
        
        logger.info(f"Zero-shot åˆæˆ: '{text[:30]}...' (å‚è€ƒ: '{prompt_text[:20]}...')")
        start_time = time.time()
        
        try:
            audio_chunks = []
            for output in self.model.inference_zero_shot(
                text, 
                prompt_text, 
                prompt_wav_path, 
                stream=False, 
                speed=speed
            ):
                audio = output['tts_speech']
                audio_chunks.append(audio)
            
            if not audio_chunks:
                logger.error("æ²¡æœ‰ç”Ÿæˆä»»ä½•éŸ³é¢‘")
                return b''
            
            # åˆå¹¶éŸ³é¢‘
            full_audio = torch.cat(audio_chunks, dim=1)
            
            # é‡é‡‡æ ·åˆ° 44.1kHz
            if full_audio.device.type != 'cuda':
                full_audio = full_audio.cuda()
            resampled = self.resampler(full_audio)
            
            # è½¬æ¢ä¸º 16-bit PCM
            audio_np = (resampled.cpu().numpy() * 32767).astype(np.int16)
            
            # è®¡ç®— RTF
            audio_duration = len(audio_np.flatten()) / TARGET_SAMPLE_RATE
            total_time = time.time() - start_time
            rtf = total_time / audio_duration if audio_duration > 0 else 0
            
            logger.info(f"âœ… Zero-shot å®Œæˆ: {len(text)}å­—, {total_time*1000:.0f}ms, éŸ³é¢‘{audio_duration:.2f}s, RTF={rtf:.3f}")
            
            if rtf > 0.1:
                logger.warning(f"âš ï¸ RTF={rtf:.3f} > 0.1ï¼Œæ€§èƒ½è­¦å‘Šï¼")
            
            # åˆ›å»º WAV
            buffer = io.BytesIO()
            with wave.open(buffer, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(TARGET_SAMPLE_RATE)
                wf.writeframes(audio_np.tobytes())
            return buffer.getvalue()
            
        except Exception as e:
            logger.error(f"Zero-shot åˆæˆå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return b''

    def synthesize_zero_shot_stream(self, text: str, prompt_wav_path: str, prompt_text: str, speed: float = 1.0):
        """Zero-shot å…‹éš†åˆæˆï¼ˆæµå¼ï¼‰"""
        if not self.ready:
            logger.error("æœåŠ¡æœªå°±ç»ª")
            return
        
        logger.info(f"Zero-shot æµå¼åˆæˆ: '{text[:30]}...'")
        start_time = time.time()
        chunk_count = 0
        total_audio_duration = 0
        
        try:
            for output in self.model.inference_zero_shot(
                text, 
                prompt_text, 
                prompt_wav_path, 
                stream=True, 
                speed=speed
            ):
                chunk_start = time.time()
                audio = output['tts_speech']
                
                # é‡é‡‡æ ·åˆ° 44.1kHz
                if audio.device.type != 'cuda':
                    audio = audio.cuda()
                resampled = self.resampler(audio)
                
                # è½¬æ¢ä¸º 16-bit PCM
                audio_np = (resampled.cpu().numpy() * 32767).astype(np.int16).flatten()
                
                chunk_count += 1
                chunk_duration = len(audio_np) / TARGET_SAMPLE_RATE
                total_audio_duration += chunk_duration
                
                if chunk_count == 1:
                    ttfa = (time.time() - start_time) * 1000
                    logger.info(f"âš¡ TTFA: {ttfa:.0f}ms")
                
                yield audio_np.tobytes()
                
            # æ€»ç»“
            total_time = time.time() - start_time
            overall_rtf = total_time / total_audio_duration if total_audio_duration > 0 else 0
            logger.info(f"âœ… Zero-shot æµå¼å®Œæˆ: {chunk_count} chunks, æ€»RTF={overall_rtf:.3f}")
            
        except Exception as e:
            logger.error(f"Zero-shot æµå¼åˆæˆå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# å…¨å±€å®ä¾‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
service = CosyVoice2Service()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FastAPI åº”ç”¨
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info(f"ğŸ¤ CosyVoice2 æœåŠ¡å¯åŠ¨ä¸­ (ç«¯å£ {SERVICE_PORT})...")
    service.initialize()
    yield
    logger.info("ğŸ›‘ CosyVoice2 æœåŠ¡å…³é—­")

app = FastAPI(title="CosyVoice2 TTS Service", lifespan=lifespan)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/health")
async def health():
    return {
        "service": "mouth-cosyvoice2",
        "status": "ready" if service.ready else "loading",
        "model": "CosyVoice2 0.5B (fp16=True)",
        "sample_rate": TARGET_SAMPLE_RATE,
        "native_sample_rate": NATIVE_SAMPLE_RATE,
        "speakers": service.model.list_available_spks() if service.model else [],
        "default_speaker": service.default_speaker
    }

@app.post("/tts")
async def tts(
    text: str = Form(...),
    prompt_text: str = Form(...),
    prompt_audio: UploadFile = File(...),
    speed: float = Form(1.0)
):
    """
    Zero-shot TTSï¼ˆéæµå¼ï¼‰
    
    éœ€è¦ä¸Šä¼ å‚è€ƒéŸ³é¢‘å’Œå‚è€ƒæ–‡æœ¬æ¥å…‹éš†å£°éŸ³
    """
    if not service.ready:
        logger.error("æœåŠ¡æœªå°±ç»ª")
        return Response(content=b'Service not ready', status_code=503)
    
    try:
        # ä¿å­˜ä¸Šä¼ çš„éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
            content = await prompt_audio.read()
            tmp.write(content)
            tmp_path = tmp.name
        
        logger.info(f"æ¥æ”¶åˆ° TTS è¯·æ±‚: text='{text[:30]}...', prompt_text='{prompt_text[:20]}...', prompt_audio={len(content)} bytes")
        
        # åˆæˆ
        audio = service.synthesize_zero_shot(text, tmp_path, prompt_text, speed)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(tmp_path)
        
        if not audio:
            return Response(content=b'Synthesis failed', status_code=500)
        
        return Response(content=audio, media_type="audio/wav")
        
    except Exception as e:
        logger.error(f"TTS è¯·æ±‚å¤„ç†å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return Response(content=str(e).encode(), status_code=500)

@app.post("/tts/stream")
async def tts_stream(
    text: str = Form(...),
    prompt_text: str = Form(...),
    prompt_audio: UploadFile = File(...),
    speed: float = Form(1.0)
):
    """
    Zero-shot TTSï¼ˆçœŸæµå¼ï¼‰- æ¯ç”Ÿæˆçº¦ 50ms å°±ç«‹å³è¿”å›
    """
    if not service.ready:
        logger.error("æœåŠ¡æœªå°±ç»ª")
        return Response(content=b'Service not ready', status_code=503)
    
    try:
        # ä¿å­˜ä¸Šä¼ çš„éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
            content = await prompt_audio.read()
            tmp.write(content)
            tmp_path = tmp.name
        
        logger.info(f"æ¥æ”¶åˆ°æµå¼ TTS è¯·æ±‚: text='{text[:30]}...', prompt_audio={len(content)} bytes")
        
        def generate():
            try:
                for chunk in service.synthesize_zero_shot_stream(text, tmp_path, prompt_text, speed):
                    yield chunk
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.unlink(tmp_path)
                except:
                    pass
        
        return StreamingResponse(
            generate(),
            media_type="audio/pcm",
            headers={
                "X-Sample-Rate": str(TARGET_SAMPLE_RATE),
                "X-Channels": "1",
                "X-Bit-Depth": "16"
            }
        )
        
    except Exception as e:
        logger.error(f"æµå¼ TTS è¯·æ±‚å¤„ç†å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return Response(content=str(e).encode(), status_code=500)

@app.get("/speakers")
async def list_speakers():
    """è·å–å¯ç”¨è¯´è¯äººåˆ—è¡¨"""
    if not service.model:
        return {"speakers": []}
    return {"speakers": service.model.list_available_spks()}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# å…¥å£
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
if __name__ == "__main__":
    import uvicorn
    
    # æ¸…ç†ç«¯å£
    kill_port(SERVICE_PORT)
    
    logger.info(f"ğŸš€ å¯åŠ¨ CosyVoice2 æœåŠ¡ (ç«¯å£ {SERVICE_PORT})...")
    uvicorn.run(app, host="0.0.0.0", port=SERVICE_PORT, log_level="info")
</file>

<file path="server/cortex/mouth_emotion.py">
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ’‹ CORTEX-MOUTH-EMOTION (ç«¯å£ 9004)                                          â•‘
â•‘  IndexTTS 2.5 - æƒ…æ„Ÿå¢å¼º TTS æœåŠ¡                                              â•‘
â•‘                                                                              â•‘
â•‘  ğŸ­ ç‰¹æ€§ï¼š                                                                    â•‘
â•‘    - 8ç»´æƒ…æ„Ÿå‘é‡æ§åˆ¶ (happy/angry/sad/afraid/disgusted/melancholic/surprised/calm)â•‘
â•‘    - è‡ªåŠ¨æ–‡æœ¬æƒ…æ„Ÿåˆ†æ (use_emo_text=True)                                      â•‘
â•‘    - torch.compile åŠ é€Ÿ                                                       â•‘
â•‘    - 22kHz -> 44.1kHz å®æ—¶é‡é‡‡æ ·                                               â•‘
â•‘    - AR å¹»è§‰æˆªæ–­ä¿æŠ¤                                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import sys

# ğŸ”‘ è®¾ç½® IndexTTS æ¨¡å—è·¯å¾„
INDEXTTS_PATH = "/workspace/models/IndexTTS2.5/index-tt2.5"
sys.path.insert(0, INDEXTTS_PATH)
os.chdir(INDEXTTS_PATH)  # IndexTTS ä¾èµ–ç›¸å¯¹è·¯å¾„åŠ è½½ checkpoints

# ç¦ç”¨ CUDA Graph (ä¸æµå¼ä¸å…¼å®¹)
os.environ['TORCHINDUCTOR_CUDAGRAPHS'] = '0'

import torch
# torch 2.4.x API å…¼å®¹æ€§å¤„ç†
try:
    torch._inductor.config.triton.cudagraphs = False
except AttributeError:
    pass  # torch 2.4.x ä¸éœ€è¦è¿™ä¸ªè®¾ç½®

import io
import wave
import time
import numpy as np
import torchaudio
from loguru import logger
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse, Response
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from contextlib import asynccontextmanager
from typing import Optional, List

# ============================================================================
# é…ç½®å¸¸é‡
# ============================================================================
SERVICE_PORT = 9004
MODEL_DIR = os.path.join(INDEXTTS_PATH, "checkpoints")
CFG_PATH = os.path.join(MODEL_DIR, "config.yaml")

# é‡‡æ ·ç‡é…ç½®
NATIVE_SAMPLE_RATE = 22050  # IndexTTS åŸç”Ÿé‡‡æ ·ç‡
OUTPUT_SAMPLE_RATE = 44100  # ç»Ÿä¸€è¾“å‡ºé‡‡æ ·ç‡

# AR å¹»è§‰æˆªæ–­é˜ˆå€¼ (tokens per character)
MAX_TOKENS_PER_CHAR = 10

mouth = None


class EmotionMouthHandler:
    """IndexTTS 2.5 å¤„ç†å™¨ - æƒ…æ„Ÿå¢å¼ºç‰ˆ"""
    
    def __init__(self):
        self.model = None
        self.is_ready = False
        self.native_sample_rate = NATIVE_SAMPLE_RATE
        self.output_sample_rate = OUTPUT_SAMPLE_RATE
        self.resampler = None  # å»¶è¿Ÿåˆå§‹åŒ–
        
        # é»˜è®¤ prompt éŸ³é¢‘ (ä½¿ç”¨ç°æœ‰çš„)
        self.default_prompt_wav = "/workspace/project-trinity/project-trinity/assets/prompt_female.wav"
        
        # æƒ…æ„Ÿå‘é‡é¡ºåº (ä¸ IndexTTS2 å†…éƒ¨ä¸€è‡´)
        self.emotion_keys = ["happy", "angry", "sad", "afraid", "disgusted", "melancholic", "surprised", "calm"]
        
    async def initialize(self):
        logger.info("=" * 60)
        logger.info("æ­£åœ¨åˆå§‹åŒ– IndexTTS 2.5 (æƒ…æ„Ÿå¢å¼ºç‰ˆ)...")
        logger.info(f"æ¨¡å‹ç›®å½•: {MODEL_DIR}")
        logger.info(f"é…ç½®æ–‡ä»¶: {CFG_PATH}")
        logger.info("âš ï¸ é¦–æ¬¡å¯åŠ¨éœ€è¦åŠ è½½å¤šä¸ªæ¨¡å‹ + torch.compile é¢„çƒ­ï¼Œè¯·è€å¿ƒç­‰å¾…ï¼")
        logger.info("=" * 60)
        
        try:
            from indextts.infer_v2 import IndexTTS2
            
            start_time = time.time()
            
            # ğŸ”¥ ä¿®å¤ç‰ˆé…ç½®ï¼šé¿å… CUDA Graph å†²çª
            # ============================================================
            # 2026-01-11 ä¿®å¤ï¼šé™é»˜å´©æºƒé—®é¢˜
            # æ ¹å› ï¼šuse_accel (GPT CUDA Graph) ä¸ use_torch_compile å†²çª
            # æ–¹æ¡ˆï¼šç¦ç”¨ torch.compileï¼Œä¿ç•™æ¨¡å‹åŸç”Ÿ accel å¼•æ“
            # ============================================================
            
            # æ­¥éª¤ 4: æ˜¾å­˜é˜²ç¢ç‰‡ (åœ¨åŠ è½½å‰æ¸…ç†)
            torch.cuda.empty_cache()
            
            self.model = IndexTTS2(
                cfg_path=CFG_PATH,
                model_dir=MODEL_DIR,
                use_fp16=True,           # ğŸ”‘ FP16 åŠ é€Ÿ
                use_cuda_kernel=True,   # BigVGAN CUDA kernel (éœ€è¦ Ninja ç¼–è¯‘ï¼Œç¦ç”¨)
                use_deepspeed=False,     # å•ç”¨æˆ·ä¸éœ€è¦
                use_accel=True,          # ğŸ”‘ ä¿ç•™ï¼šæ¨¡å‹åŸç”Ÿ CUDA Graph (GPT åŠ é€Ÿ)
                use_torch_compile=False, # ğŸ”‘ ç¦ç”¨ï¼šé˜²æ­¢ä¸ accel å†²çª
                device="cuda:0"
            )
            
            # åŠ è½½åå†æ¬¡æ¸…ç†ç¢ç‰‡
            torch.cuda.empty_cache()
            
            load_time = time.time() - start_time
            logger.info(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶ {load_time:.1f}s")
            
            # åˆå§‹åŒ–é‡é‡‡æ ·å™¨ (22kHz -> 44.1kHz)
            self.resampler = torchaudio.transforms.Resample(
                orig_freq=self.native_sample_rate,
                new_freq=self.output_sample_rate
            ).cuda()
            logger.info(f"é‡é‡‡æ ·å™¨å°±ç»ª: {self.native_sample_rate}Hz -> {self.output_sample_rate}Hz")
            
            # ğŸ”¥ é¢„çƒ­æ¨ç† (è§¦å‘ torch.compile JIT)
            logger.info("é¢„çƒ­æ¨ç†ä¸­ (è§¦å‘ JIT ç¼–è¯‘ï¼Œå¯èƒ½éœ€è¦ 30-60 ç§’)...")
            warmup_start = time.time()
            
            # ä½¿ç”¨ä¸€ä¸ªç®€çŸ­çš„"ã€‚"æ¥é¢„çƒ­
            _ = list(self.model.infer_generator(
                spk_audio_prompt=self.default_prompt_wav,
                text="ã€‚",  # æœ€çŸ­çš„åˆæ³•è¾“å…¥
                output_path=None,
                stream_return=False,
                verbose=False
            ))
            
            warmup_time = time.time() - warmup_start
            logger.success(f"âœ… é¢„çƒ­å®Œæˆï¼Œè€—æ—¶ {warmup_time:.1f}s")
            
            self.is_ready = True
            total_time = time.time() - start_time
            logger.success(f"âœ… IndexTTS 2.5 åˆå§‹åŒ–å®Œæˆï¼æ€»è€—æ—¶ {total_time:.1f}s")
            return True
            
        except Exception as e:
            logger.error(f"IndexTTS åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def _resample_to_44k(self, audio_tensor: torch.Tensor) -> torch.Tensor:
        """å°†éŸ³é¢‘ä» 22kHz é‡é‡‡æ ·åˆ° 44.1kHz"""
        if audio_tensor.dim() == 1:
            audio_tensor = audio_tensor.unsqueeze(0)
        return self.resampler(audio_tensor.cuda()).cpu()

    def synthesize(self, text: str, 
                   prompt_wav_path: Optional[str] = None,
                   emo_vector: Optional[List[float]] = None,
                   use_emo_text: bool = False) -> bytes:
        """éæµå¼åˆæˆ"""
        if not self.is_ready:
            return b""
        
        prompt_wav = prompt_wav_path or self.default_prompt_wav
        
        # AR å¹»è§‰ä¿æŠ¤ï¼šé¢„ä¼°æœ€å¤§åˆç†é•¿åº¦
        text_len = len(text.replace(" ", ""))
        max_mel_tokens = min(1500, max(200, text_len * MAX_TOKENS_PER_CHAR))
        
        try:
            start = time.time()
            
            # è°ƒç”¨ IndexTTS2 æ¨ç†
            result = list(self.model.infer_generator(
                spk_audio_prompt=prompt_wav,
                text=text,
                output_path=None,
                emo_vector=emo_vector,
                use_emo_text=use_emo_text,
                stream_return=False,
                verbose=False,
                max_mel_tokens=max_mel_tokens,  # å¹»è§‰æˆªæ–­
            ))
            
            if not result:
                logger.warning("IndexTTS è¿”å›ç©ºç»“æœ")
                return b""
            
            # result[-1] æ˜¯ (sample_rate, wav_data) å…ƒç»„
            sr, wav_data = result[-1]
            
            # é‡é‡‡æ ·åˆ° 44.1kHz
            wav_tensor = torch.from_numpy(wav_data.T.astype(np.float32) / 32767.0)
            wav_44k = self._resample_to_44k(wav_tensor)
            
            # è½¬æ¢ä¸º WAV å­—èŠ‚
            audio_int16 = (wav_44k.squeeze().numpy() * 32767).astype(np.int16)
            buf = io.BytesIO()
            with wave.open(buf, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(self.output_sample_rate)
                wf.writeframes(audio_int16.tobytes())
            
            elapsed = (time.time() - start) * 1000
            logger.info(f"åˆæˆå®Œæˆ: {text_len}å­—, {elapsed:.0f}ms, emo={emo_vector or 'auto' if use_emo_text else 'none'}")
            
            return buf.getvalue()
            
        except Exception as e:
            logger.error(f"åˆæˆå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return b""

    def synthesize_stream(self, text: str,
                          prompt_wav_path: Optional[str] = None,
                          emo_vector: Optional[List[float]] = None,
                          use_emo_text: bool = False):
        """æµå¼åˆæˆ (Generator)"""
        if not self.is_ready:
            yield b""
            return
        
        prompt_wav = prompt_wav_path or self.default_prompt_wav
        
        # AR å¹»è§‰ä¿æŠ¤
        text_len = len(text.replace(" ", ""))
        max_mel_tokens = min(1500, max(200, text_len * MAX_TOKENS_PER_CHAR))
        max_chunks = max(10, text_len * 5)  # æ¯ä¸ªå­—æœ€å¤š 5 ä¸ª chunk
        
        try:
            start = time.time()
            first_chunk = True
            chunk_count = 0
            
            for chunk in self.model.infer_generator(
                spk_audio_prompt=prompt_wav,
                text=text,
                output_path=None,
                emo_vector=emo_vector,
                use_emo_text=use_emo_text,
                stream_return=True,
                verbose=False,
                max_mel_tokens=max_mel_tokens,
            ):
                chunk_count += 1
                
                # å¹»è§‰æˆªæ–­
                if chunk_count > max_chunks:
                    logger.warning(f"âš ï¸ Early Stop: æ£€æµ‹åˆ°æµå¼å¹»è§‰ï¼Œå¼ºåˆ¶æˆªæ–­ï¼å·²è¾“å‡º {chunk_count} chunks")
                    break
                
                if first_chunk:
                    ttfa = (time.time() - start) * 1000
                    logger.info(f"TTFA: {ttfa:.0f}ms")
                    first_chunk = False
                
                # chunk æ˜¯ torch.Tensor, shape [1, samples] æˆ– [samples]
                if isinstance(chunk, torch.Tensor):
                    # é‡é‡‡æ ·åˆ° 44.1kHz
                    wav_44k = self._resample_to_44k(chunk.float() / 32767.0)
                    audio_int16 = (wav_44k.squeeze().numpy() * 32767).astype(np.int16)
                    yield audio_int16.tobytes()
                    
        except Exception as e:
            logger.error(f"æµå¼åˆæˆå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            yield b""


@asynccontextmanager
async def lifespan(app: FastAPI):
    global mouth
    logger.info(f"ğŸ’‹ Cortex-Mouth-Emotion å¯åŠ¨ä¸­ (ç«¯å£ {SERVICE_PORT})...")
    mouth = EmotionMouthHandler()
    success = await mouth.initialize()
    if success:
        logger.success(f"âœ… Mouth-Emotion å°±ç»ª (ç«¯å£ {SERVICE_PORT})")
    else:
        logger.error("âŒ Mouth-Emotion åˆå§‹åŒ–å¤±è´¥")
    yield
    logger.info("ğŸ›‘ Mouth-Emotion å…³é—­")


app = FastAPI(lifespan=lifespan, title="Cortex-Mouth-Emotion (IndexTTS 2.5)")

# CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/health")
async def health():
    return {
        "service": "mouth-emotion",
        "status": "ok" if mouth and mouth.is_ready else "loading",
        "model": "IndexTTS 2.5 (use_fp16=True, torch_compile=True)",
        "sample_rate": OUTPUT_SAMPLE_RATE,
        "native_sample_rate": NATIVE_SAMPLE_RATE,
        "emotion_keys": mouth.emotion_keys if mouth else [],
        "voice_prompt": mouth.default_prompt_wav if mouth else None,
    }


@app.post("/tts")
async def tts(request: dict):
    """
    éæµå¼ TTS æ¥å£
    
    è¯·æ±‚ä½“:
    {
        "text": "è¦åˆæˆçš„æ–‡æœ¬",
        "prompt_wav_path": "å¯é€‰ï¼Œå‚è€ƒéŸ³é¢‘è·¯å¾„",
        "emo_vector": [0.5, 0, 0, 0, 0, 0, 0, 0.5],  // å¯é€‰ï¼Œ8ç»´æƒ…æ„Ÿå‘é‡
        "use_emo_text": false  // å¯é€‰ï¼Œæ˜¯å¦ä»æ–‡æœ¬è‡ªåŠ¨æ¨æ–­æƒ…æ„Ÿ
    }
    """
    if not mouth or not mouth.is_ready:
        raise HTTPException(status_code=503, detail="Service not ready")
    
    text = request.get("text", "")
    if not text:
        raise HTTPException(status_code=400, detail="text required")
    
    audio = mouth.synthesize(
        text=text,
        prompt_wav_path=request.get("prompt_wav_path"),
        emo_vector=request.get("emo_vector"),
        use_emo_text=request.get("use_emo_text", False),
    )
    
    if not audio:
        raise HTTPException(status_code=500, detail="Synthesis failed")
    
    return Response(content=audio, media_type="audio/wav")


@app.post("/tts/stream")
async def tts_stream(request: dict):
    """
    æµå¼ TTS æ¥å£
    
    è¿”å› PCM éŸ³é¢‘æµ (44.1kHz, 16bit, mono)
    """
    if not mouth or not mouth.is_ready:
        raise HTTPException(status_code=503, detail="Service not ready")
    
    text = request.get("text", "")
    if not text:
        raise HTTPException(status_code=400, detail="text required")
    
    return StreamingResponse(
        mouth.synthesize_stream(
            text=text,
            prompt_wav_path=request.get("prompt_wav_path"),
            emo_vector=request.get("emo_vector"),
            use_emo_text=request.get("use_emo_text", False),
        ),
        media_type="audio/pcm",
        headers={"X-Sample-Rate": str(OUTPUT_SAMPLE_RATE)}
    )


@app.post("/analyze_emotion")
async def analyze_emotion(request: dict):
    """
    åˆ†ææ–‡æœ¬æƒ…æ„Ÿ
    
    è¯·æ±‚ä½“: {"text": "è¦åˆ†æçš„æ–‡æœ¬"}
    è¿”å›: {"happy": 0.5, "angry": 0, ...}
    """
    if not mouth or not mouth.is_ready:
        raise HTTPException(status_code=503, detail="Service not ready")
    
    text = request.get("text", "")
    if not text:
        raise HTTPException(status_code=400, detail="text required")
    
    try:
        emo_dict = mouth.model.qwen_emo.inference(text)
        return emo_dict
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    from server.utils.port_utils import kill_port
    
    kill_port(SERVICE_PORT)
    uvicorn.run(app, host="0.0.0.0", port=SERVICE_PORT)
</file>

<file path="server/cortex/mouth_fish.py">
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ‘„ CORTEX-MOUTH-FISH (ç«¯å£ 9006)                                             â•‘
â•‘  Fish Speech 1.5 - äººæƒ…å‘³è¯­éŸ³åˆæˆ                                             â•‘
â•‘                                                                              â•‘
â•‘  ğŸ”¥ ç‰¹æ€§ï¼š                                                                    â•‘
â•‘    - åŸç”Ÿ 44.1kHz è¾“å‡ºï¼ˆæ— éœ€é‡é‡‡æ ·ï¼‰                                          â•‘
â•‘    - BF16 ç²¾åº¦ (4090 æœ€ä¼˜)                                                    â•‘
â•‘    - æ”¯æŒæƒ…æ„Ÿæ ‡ç­¾ [laughter], [sigh] ç­‰                                       â•‘
â•‘    - LLM + VQ-GAN åˆ†æ®µæµå¼æ¨ç†                                                â•‘
â•‘    - é˜²å¹»è§‰ï¼šRTF > 0.5 è‡ªåŠ¨æŠ¥é”™é‡ç½®                                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import sys
import io
import wave
import time
import queue
import threading
import numpy as np
import torch

# æ·»åŠ  Fish Speech æºç è·¯å¾„
FISH_SPEECH_PATH = "/workspace/models/fish-speech"
sys.path.insert(0, FISH_SPEECH_PATH)
os.chdir(FISH_SPEECH_PATH)

from loguru import logger
from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.responses import StreamingResponse, Response
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
from typing import Optional, List
import tempfile

# ============================================================================
# é…ç½®å¸¸é‡
# ============================================================================
SERVICE_PORT = 9006
MODEL_DIR = "/workspace/models/FishSpeech/fish-speech-1.5"

# Fish Speech åŸç”Ÿ 44.1kHz
OUTPUT_SAMPLE_RATE = 44100

# é˜²å¹»è§‰é˜ˆå€¼ï¼šRTF è¶…è¿‡ 0.5 è§†ä¸ºå¼‚å¸¸
MAX_RTF = 0.5

mouth = None


class FishSpeechHandler:
    """Fish Speech 1.5 å¤„ç†å™¨"""
    
    def __init__(self):
        self.model_manager = None
        self.is_ready = False
        self.sample_rate = OUTPUT_SAMPLE_RATE
        self.lock = threading.Lock()
        
        # é»˜è®¤å‚è€ƒéŸ³é¢‘
        self.default_reference = "/workspace/project-trinity/project-trinity/assets/prompt_female.wav"
        
    async def initialize(self):
        logger.info("=" * 60)
        logger.info("æ­£åœ¨åˆå§‹åŒ– Fish Speech 1.5...")
        logger.info(f"æ¨¡å‹ç›®å½•: {MODEL_DIR}")
        logger.info("=" * 60)
        
        try:
            from tools.server.model_manager import ModelManager
            
            start_time = time.time()
            
            # åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
            self.model_manager = ModelManager(
                mode="tts",
                device="cuda",
                half=False,           # ä½¿ç”¨ BF16ï¼Œä¸æ˜¯ FP16
                compile=True,         # ğŸ”¥ å¯ç”¨ torch.compile
                llama_checkpoint_path=MODEL_DIR,
                decoder_checkpoint_path=f"{MODEL_DIR}/firefly-gan-vq-fsq-8x1024-21hz-generator.pth",
                decoder_config_name="firefly_gan_vq",
            )
            
            load_time = time.time() - start_time
            logger.info(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶ {load_time:.1f}s")
            
            # è·å–å®é™…é‡‡æ ·ç‡
            if hasattr(self.model_manager, 'engine') and hasattr(self.model_manager.engine, 'decoder_model'):
                if hasattr(self.model_manager.engine.decoder_model, 'sample_rate'):
                    self.sample_rate = self.model_manager.engine.decoder_model.sample_rate
                    logger.info(f"å®é™…é‡‡æ ·ç‡: {self.sample_rate}Hz")
            
            # é¢„çƒ­æ¨ç†
            logger.info("é¢„çƒ­æ¨ç†ä¸­...")
            warmup_start = time.time()
            
            # ç®€å•é¢„çƒ­
            _ = self._synthesize_internal("é¢„çƒ­", streaming=False)
            
            warmup_time = time.time() - warmup_start
            logger.success(f"âœ… é¢„çƒ­å®Œæˆï¼Œè€—æ—¶ {warmup_time:.1f}s")
            
            self.is_ready = True
            total_time = time.time() - start_time
            logger.success(f"âœ… Fish Speech 1.5 åˆå§‹åŒ–å®Œæˆï¼æ€»è€—æ—¶ {total_time:.1f}s")
            return True
            
        except Exception as e:
            logger.error(f"Fish Speech åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def _synthesize_internal(self, text: str, 
                             reference_audio: Optional[str] = None,
                             streaming: bool = False):
        """å†…éƒ¨åˆæˆæ–¹æ³•"""
        from fish_speech.utils.schema import ServeTTSRequest
        
        # æ„å»ºè¯·æ±‚
        req = ServeTTSRequest(
            text=text,
            references=[],  # å¯ä»¥æ·»åŠ å‚è€ƒéŸ³é¢‘
            reference_id=None,
            streaming=streaming,
            chunk_length=200 if streaming else 0,  # æµå¼åˆ†å—
            top_p=0.7,
            temperature=0.7,
            repetition_penalty=1.2,
            max_new_tokens=2048,
            use_memory_cache=True,
        )
        
        return self.model_manager.engine.inference(req)

    def synthesize(self, text: str, 
                   reference_audio: Optional[str] = None) -> bytes:
        """éæµå¼åˆæˆ"""
        if not self.is_ready:
            return b""
        
        try:
            start = time.time()
            
            with self.lock:
                results = list(self._synthesize_internal(text, reference_audio, streaming=False))
            
            # æŸ¥æ‰¾æœ€ç»ˆéŸ³é¢‘
            audio_data = None
            for result in results:
                if result.code == "final" and result.audio is not None:
                    sr, audio_data = result.audio
                    self.sample_rate = sr
                elif result.code == "error":
                    logger.error(f"åˆæˆé”™è¯¯: {result.error}")
                    return b""
            
            if audio_data is None:
                return b""
            
            elapsed = time.time() - start
            audio_duration = len(audio_data) / self.sample_rate
            rtf = elapsed / audio_duration if audio_duration > 0 else float('inf')
            
            # é˜²å¹»è§‰æ£€æŸ¥
            if rtf > MAX_RTF:
                logger.warning(f"âš ï¸ RTF å¼‚å¸¸: {rtf:.2f} > {MAX_RTF}ï¼Œå¯èƒ½å­˜åœ¨å¹»è§‰")
            
            # è½¬æ¢ä¸º WAV
            audio_int16 = (audio_data * 32767).astype(np.int16)
            buf = io.BytesIO()
            with wave.open(buf, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(self.sample_rate)
                wf.writeframes(audio_int16.tobytes())
            
            logger.info(f"åˆæˆå®Œæˆ: {len(text)}å­—, {elapsed*1000:.0f}ms, RTF={rtf:.2f}")
            
            return buf.getvalue()
            
        except Exception as e:
            logger.error(f"åˆæˆå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return b""

    def synthesize_stream(self, text: str,
                          reference_audio: Optional[str] = None):
        """æµå¼åˆæˆ"""
        if not self.is_ready:
            yield b""
            return
        
        try:
            start = time.time()
            first_chunk = True
            chunk_count = 0
            total_audio_duration = 0
            
            with self.lock:
                for result in self._synthesize_internal(text, reference_audio, streaming=True):
                    if result.code == "header":
                        # è·³è¿‡ WAV headerï¼ˆæˆ‘ä»¬å‘é€ raw PCMï¼‰
                        continue
                    elif result.code == "segment":
                        chunk_count += 1
                        sr, audio_chunk = result.audio
                        self.sample_rate = sr
                        
                        if first_chunk:
                            ttfa = (time.time() - start) * 1000
                            logger.info(f"TTFA: {ttfa:.0f}ms")
                            first_chunk = False
                        
                        # è½¬æ¢ä¸º int16
                        audio_int16 = (audio_chunk * 32767).astype(np.int16)
                        total_audio_duration += len(audio_chunk) / sr
                        
                        # é˜²å¹»è§‰æ£€æŸ¥
                        elapsed = time.time() - start
                        if total_audio_duration > 0 and elapsed / total_audio_duration > MAX_RTF * 3:
                            logger.warning(f"âš ï¸ æ£€æµ‹åˆ°å¹»è§‰ï¼Œå¼ºåˆ¶æˆªæ–­ï¼RTF={elapsed/total_audio_duration:.2f}")
                            break
                        
                        yield audio_int16.tobytes()
                        
                    elif result.code == "error":
                        logger.error(f"æµå¼é”™è¯¯: {result.error}")
                        yield b""
                        return
                    elif result.code == "final":
                        # æµå¼æ¨¡å¼ä¸‹ final ä¹‹å‰å·²ç»å‘é€äº†æ‰€æœ‰ segment
                        pass
            
            logger.info(f"æµå¼å®Œæˆ: {chunk_count} chunks")
                    
        except Exception as e:
            logger.error(f"æµå¼åˆæˆå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            yield b""


@asynccontextmanager
async def lifespan(app: FastAPI):
    global mouth
    logger.info(f"ğŸ‘„ Cortex-Mouth-Fish å¯åŠ¨ä¸­ (ç«¯å£ {SERVICE_PORT})...")
    mouth = FishSpeechHandler()
    success = await mouth.initialize()
    if success:
        logger.success(f"âœ… Mouth-Fish å°±ç»ª (ç«¯å£ {SERVICE_PORT})")
    else:
        logger.error("âŒ Mouth-Fish åˆå§‹åŒ–å¤±è´¥")
    yield
    logger.info("ğŸ›‘ Mouth-Fish å…³é—­")


app = FastAPI(lifespan=lifespan, title="Cortex-Mouth-Fish (Fish Speech 1.5)")

# CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/health")
async def health():
    return {
        "service": "mouth-fish",
        "status": "ok" if mouth and mouth.is_ready else "loading",
        "model": "Fish Speech 1.5 (BF16, compile=True)",
        "sample_rate": mouth.sample_rate if mouth else OUTPUT_SAMPLE_RATE,
        "features": ["emotion_tags", "reference_audio", "anti_hallucination"],
        "emotion_tags": ["[laughter]", "[sigh]", "[breath]", "[cough]"],
    }


@app.post("/tts")
async def tts(request: dict):
    """
    éæµå¼ TTS æ¥å£
    
    è¯·æ±‚ä½“:
    {
        "text": "è¦åˆæˆçš„æ–‡æœ¬ï¼Œæ”¯æŒ [laughter] [sigh] ç­‰æƒ…æ„Ÿæ ‡ç­¾",
        "reference_audio": "å¯é€‰ï¼Œå‚è€ƒéŸ³é¢‘è·¯å¾„"
    }
    """
    if not mouth or not mouth.is_ready:
        raise HTTPException(status_code=503, detail="Service not ready")
    
    text = request.get("text", "")
    if not text:
        raise HTTPException(status_code=400, detail="text required")
    
    audio = mouth.synthesize(
        text=text,
        reference_audio=request.get("reference_audio"),
    )
    
    if not audio:
        raise HTTPException(status_code=500, detail="Synthesis failed")
    
    return Response(content=audio, media_type="audio/wav")


@app.post("/tts/stream")
async def tts_stream(request: dict):
    """
    æµå¼ TTS æ¥å£
    
    è¿”å› PCM éŸ³é¢‘æµ (44.1kHz, 16bit, mono)
    """
    if not mouth or not mouth.is_ready:
        raise HTTPException(status_code=503, detail="Service not ready")
    
    text = request.get("text", "")
    if not text:
        raise HTTPException(status_code=400, detail="text required")
    
    return StreamingResponse(
        mouth.synthesize_stream(
            text=text,
            reference_audio=request.get("reference_audio"),
        ),
        media_type="audio/pcm",
        headers={"X-Sample-Rate": str(mouth.sample_rate)}
    )


if __name__ == "__main__":
    import uvicorn
    from server.utils.port_utils import kill_port
    
    kill_port(SERVICE_PORT)
    uvicorn.run(app, host="0.0.0.0", port=SERVICE_PORT)
</file>

<file path="server/mind_engine/__init__.py">
"""
Project Trinity - Mind Engine
ä¸‰ä½ä¸€ä½“å¿ƒæ™ºå¼•æ“

Layer 1: BioState (æœ¬æˆ‘) - æ¦‚ç‡å†…ç¨³æ€ä¸åå°„
Layer 2: NarrativeManager (è¶…æˆ‘) - è®°å¿†ä¸äººè®¾çº¦æŸ
Layer 3: EgoDirector (è‡ªæˆ‘) - å†³ç­–ä¸ä»²è£
"""

from .bio_state import BioState, BioStateSnapshot
from .narrative_mgr import NarrativeManager
from .ego_director import EgoDirector

__all__ = [
    "BioState",
    "BioStateSnapshot",
    "NarrativeManager",
    "EgoDirector"
]
</file>

<file path="server/mind_engine/bio_state.py">
"""
Project Trinity - BioState (Layer 1: The Id)
æœ¬æˆ‘å±‚ - æ¦‚ç‡å†…ç¨³æ€ä¸ç”Ÿç‰©åå°„

è¿™æ˜¯"ä¸‰ä½ä¸€ä½“"æ¶æ„çš„ç¬¬ä¸€å±‚ï¼Œæ¨¡æ‹Ÿç”Ÿç‰©çš„æœ¬èƒ½ååº”

æ ¸å¿ƒæ¦‚å¿µ:
1. å†…ç¨³æ€ (Homeostasis): Cortisol(å‹åŠ›), Dopamine(æ„‰æ‚¦) ç­‰ç”Ÿç†æŒ‡æ ‡
2. æ¦‚ç‡é‡‡æ ·: ä¸ä½¿ç”¨å›ºå®šå¢é‡ï¼Œè€Œæ˜¯åŸºäºé«˜æ–¯åˆ†å¸ƒé‡‡æ ·
3. äººæ ¼ç§å­: Big Five äººæ ¼ç‰¹è´¨å½±å“ååº”æ•æ„Ÿåº¦
4. RPE (Reward Prediction Error): é¢„æœŸè¯¯å·®é©±åŠ¨çŠ¶æ€æ›´æ–°

å…³é”®å®ç°åŸåˆ™:
- æ‰€æœ‰çŠ¶æ€æ›´æ–°å¿…é¡»æ˜¯æ¦‚ç‡æ€§çš„
- åŒæ ·çš„åˆºæ¿€å¯èƒ½äº§ç”Ÿä¸åŒååº”ï¼ˆè¿™å°±æ˜¯"ç”Ÿå‘½æ„Ÿ"ï¼‰
"""

from dataclasses import dataclass, field
from typing import Optional, Dict, Tuple
import numpy as np
from loguru import logger


@dataclass
class PersonalitySeed:
    """
    Big Five äººæ ¼ç‰¹è´¨ç§å­
    
    è¿™äº›å‚æ•°å†³å®šäº† AI çš„"è„¾æ°”åŸºè°ƒ"
    """
    neuroticism: float = 0.5       # ç¥ç»è´¨ (0-1): é«˜=æƒ…ç»ªæ³¢åŠ¨å¤§
    extraversion: float = 0.6      # å¤–å‘æ€§ (0-1): é«˜=ä¸»åŠ¨è¡¨è¾¾
    openness: float = 0.7          # å¼€æ”¾æ€§ (0-1): é«˜=æƒ³è±¡åŠ›ä¸°å¯Œ
    agreeableness: float = 0.8     # å®œäººæ€§ (0-1): é«˜=æ¸©å’Œä½“è´´
    conscientiousness: float = 0.5 # å°½è´£æ€§ (0-1): é«˜=è®¤çœŸè´Ÿè´£


@dataclass
class BioStateSnapshot:
    """ç”Ÿç‰©çŠ¶æ€å¿«ç…§ï¼ˆç”¨äºä¼ é€’ç»™å…¶ä»–å±‚ï¼‰"""
    cortisol: float          # çš®è´¨é†‡ (å‹åŠ›æ¿€ç´ ) 0-100
    dopamine: float          # å¤šå·´èƒº (æ„‰æ‚¦) 0-100
    serotonin: float         # è¡€æ¸…ç´  (å¹³é™) 0-100
    adrenaline: float        # è‚¾ä¸Šè…ºç´  (è­¦è§‰) 0-100
    current_mood: str        # å½“å‰æƒ…ç»ªæ ‡ç­¾
    temperature: float       # æ¨èçš„ LLM Temperature


class BioState:
    """
    ç”Ÿç‰©çŠ¶æ€ç³»ç»Ÿ (Layer 1: The Id)
    
    è¿™æ˜¯ä¸€ä¸ªæ¦‚ç‡çŠ¶æ€æœºï¼Œä¸æ˜¯ç¡®å®šæ€§çš„
    
    æ ¸å¿ƒå…¬å¼:
    1. Sensitivity = 1.0 + (Current_Cortisol * Neuroticism)
    2. RPE = |Actual_Emotion - Expected_Emotion|
    3. Delta = Sample(Normal_Dist) * Sensitivity * RPE
    """
    
    # æƒ…ç»ªåˆ°ç”Ÿç†å½±å“çš„æ˜ å°„
    EMOTION_EFFECTS = {
        "happy": {"cortisol": -10, "dopamine": 20, "serotonin": 10, "adrenaline": 0},
        "sad": {"cortisol": 15, "dopamine": -15, "serotonin": -10, "adrenaline": 0},
        "angry": {"cortisol": 25, "dopamine": -5, "serotonin": -20, "adrenaline": 30},
        "fearful": {"cortisol": 30, "dopamine": -10, "serotonin": -15, "adrenaline": 40},
        "surprised": {"cortisol": 10, "dopamine": 10, "serotonin": 0, "adrenaline": 20},
        "disgusted": {"cortisol": 15, "dopamine": -10, "serotonin": -5, "adrenaline": 10},
        "neutral": {"cortisol": 0, "dopamine": 0, "serotonin": 5, "adrenaline": -5},
    }
    
    def __init__(self, personality: Optional[PersonalitySeed] = None):
        """
        åˆå§‹åŒ–ç”Ÿç‰©çŠ¶æ€ç³»ç»Ÿ
        
        Args:
            personality: äººæ ¼ç§å­ï¼Œå†³å®šååº”çš„æ•æ„Ÿåº¦
        """
        self.personality = personality or PersonalitySeed()
        
        # å†…ç¨³æ€åŸºçº¿
        self._cortisol = 30.0    # åŸºçº¿å‹åŠ›
        self._dopamine = 50.0    # åŸºçº¿æ„‰æ‚¦
        self._serotonin = 50.0   # åŸºçº¿å¹³é™
        self._adrenaline = 20.0  # åŸºçº¿è­¦è§‰
        
        # é¢„æœŸæƒ…ç»ªï¼ˆç”¨äºè®¡ç®— RPEï¼‰
        self._expected_emotion = "neutral"
        
        # éšæœºæ•°ç”Ÿæˆå™¨ï¼ˆå¯è®¾ç½®ç§å­ä»¥ä¾¿è°ƒè¯•ï¼‰
        self._rng = np.random.default_rng()
        
        logger.info(f"BioState åˆå§‹åŒ–å®Œæˆ | äººæ ¼: N={self.personality.neuroticism:.2f}")
    
    def update(self, detected_emotion: str, confidence: float = 0.8) -> Dict[str, float]:
        """
        æ ¹æ®æ£€æµ‹åˆ°çš„æƒ…ç»ªæ›´æ–°ç”Ÿç‰©çŠ¶æ€
        
        è¿™æ˜¯æ ¸å¿ƒçš„æ¦‚ç‡æ›´æ–°å‡½æ•°
        
        Args:
            detected_emotion: æ£€æµ‹åˆ°çš„æƒ…ç»ª (æ¥è‡ª SenseVoice)
            confidence: æƒ…ç»ªæ£€æµ‹ç½®ä¿¡åº¦
            
        Returns:
            Dict: çŠ¶æ€å˜åŒ–é‡ {"cortisol_delta": ..., "triggered_reflex": ...}
        """
        # 1. è®¡ç®— RPE (é¢„æœŸè¯¯å·®)
        rpe = self._calculate_rpe(detected_emotion)
        
        # 2. è®¡ç®—æ•æ„Ÿåº¦ (Sensitivity)
        sensitivity = self._calculate_sensitivity()
        
        # 3. è·å–è¯¥æƒ…ç»ªçš„åŸºç¡€å½±å“
        base_effects = self.EMOTION_EFFECTS.get(detected_emotion, self.EMOTION_EFFECTS["neutral"])
        
        # 4. æ¦‚ç‡é‡‡æ ·æ›´æ–°
        deltas = {}
        for hormone, base_delta in base_effects.items():
            # æ ¸å¿ƒå…¬å¼: Delta = Sample(Normal) * Sensitivity * RPE * Confidence
            sample = self._rng.normal(0, 10)  # æ ‡å‡†å·®ä¸º 10 çš„é«˜æ–¯é‡‡æ ·
            actual_delta = (base_delta + sample) * sensitivity * rpe * confidence
            
            # åº”ç”¨æ›´æ–°
            current = getattr(self, f"_{hormone}")
            new_value = np.clip(current + actual_delta, 0, 100)
            setattr(self, f"_{hormone}", new_value)
            
            deltas[f"{hormone}_delta"] = actual_delta
        
        # 5. æ›´æ–°é¢„æœŸæƒ…ç»ª
        self._expected_emotion = detected_emotion
        
        # 6. æ£€æŸ¥æ˜¯å¦è§¦å‘åå°„
        reflex = self._check_reflex_trigger()
        deltas["triggered_reflex"] = reflex
        
        logger.debug(
            f"BioState æ›´æ–° | æƒ…ç»ª: {detected_emotion} | "
            f"RPE: {rpe:.2f} | æ•æ„Ÿåº¦: {sensitivity:.2f} | "
            f"Cortisol: {self._cortisol:.1f} | Reflex: {reflex}"
        )
        
        return deltas
    
    def _calculate_rpe(self, actual_emotion: str) -> float:
        """
        è®¡ç®—é¢„æœŸè¯¯å·® (Reward Prediction Error)
        
        é¢„æœŸå’Œå®é™…æƒ…ç»ªå·®å¼‚è¶Šå¤§ï¼Œååº”è¶Šå¼ºçƒˆ
        """
        # æƒ…ç»ªè·ç¦»çŸ©é˜µï¼ˆç®€åŒ–ç‰ˆï¼‰
        emotion_distance = {
            ("neutral", "angry"): 0.8,
            ("neutral", "happy"): 0.5,
            ("happy", "sad"): 0.9,
            ("happy", "angry"): 0.7,
            # ... å…¶ä»–ç»„åˆ
        }
        
        key = (self._expected_emotion, actual_emotion)
        reverse_key = (actual_emotion, self._expected_emotion)
        
        if key in emotion_distance:
            return emotion_distance[key]
        elif reverse_key in emotion_distance:
            return emotion_distance[reverse_key]
        elif self._expected_emotion == actual_emotion:
            return 0.3  # é¢„æœŸåŒ¹é…ï¼Œä½ RPE
        else:
            return 0.5  # é»˜è®¤ä¸­ç­‰ RPE
    
    def _calculate_sensitivity(self) -> float:
        """
        è®¡ç®—å½“å‰æ•æ„Ÿåº¦
        
        å…¬å¼: Sensitivity = 1.0 + (Current_Cortisol/100 * Neuroticism)
        
        é«˜å‹åŠ› + é«˜ç¥ç»è´¨ = é«˜æ•æ„Ÿåº¦ (å®¹æ˜“æƒ…ç»ªæ³¢åŠ¨)
        """
        return 1.0 + (self._cortisol / 100) * self.personality.neuroticism
    
    def _check_reflex_trigger(self) -> Optional[str]:
        """
        æ£€æŸ¥æ˜¯å¦è§¦å‘åå°„å¼§
        
        è¿™äº›åå°„ä¸ç»è¿‡ LLMï¼Œç›´æ¥è§¦å‘å¾®è¡¨æƒ…
        """
        # å‹åŠ›çªå¢ -> é˜²å¾¡æ€§ååº”
        if self._cortisol > 70:
            return "defensive"  # ç³å­”æ”¶ç¼©ã€åä»°
        
        # å¤šå·´èƒºé£™å‡ -> å¼€å¿ƒè¡¨æƒ…
        if self._dopamine > 80:
            return "joy"  # çœ¼ç›å‘å…‰ã€å¾®ç¬‘
        
        # è‚¾ä¸Šè…ºç´ é£™å‡ -> è­¦è§‰
        if self._adrenaline > 60:
            return "alert"  # çœ¼ç›çå¤§ã€èº«ä½“å‰å€¾
        
        return None
    
    def get_snapshot(self) -> BioStateSnapshot:
        """è·å–å½“å‰çŠ¶æ€å¿«ç…§"""
        return BioStateSnapshot(
            cortisol=self._cortisol,
            dopamine=self._dopamine,
            serotonin=self._serotonin,
            adrenaline=self._adrenaline,
            current_mood=self._determine_mood(),
            temperature=self._calculate_temperature()
        )
    
    def _determine_mood(self) -> str:
        """æ ¹æ®æ¿€ç´ æ°´å¹³ç¡®å®šå½“å‰æƒ…ç»ª"""
        if self._cortisol > 60:
            if self._adrenaline > 50:
                return "anxious"
            return "stressed"
        elif self._dopamine > 70:
            return "happy"
        elif self._serotonin > 60:
            return "calm"
        elif self._dopamine < 30:
            return "down"
        return "neutral"
    
    def _calculate_temperature(self) -> float:
        """
        è®¡ç®—æ¨èçš„ LLM Temperature
        
        å…¬å¼: temperature = max(0.1, 1.0 - (cortisol / 100))
        
        é«˜å‹åŠ› = ä½ Temperature (ä¿å®ˆ/é˜²å¾¡)
        ä½å‹åŠ› = é«˜ Temperature (åˆ›æ„/å¹½é»˜)
        """
        return max(0.1, min(0.9, 1.0 - (self._cortisol / 100)))
    
    def decay(self, delta_time: float = 1.0) -> None:
        """
        è‡ªç„¶è¡°å‡ï¼ˆè¶‹å‘åŸºçº¿ï¼‰
        
        Args:
            delta_time: ç»è¿‡çš„æ—¶é—´ï¼ˆç§’ï¼‰
        """
        decay_rate = 0.02 * delta_time
        
        # å‘åŸºçº¿è¡°å‡
        self._cortisol += (30.0 - self._cortisol) * decay_rate
        self._dopamine += (50.0 - self._dopamine) * decay_rate
        self._serotonin += (50.0 - self._serotonin) * decay_rate
        self._adrenaline += (20.0 - self._adrenaline) * decay_rate
    
    def inject_stimulus(self, stimulus_type: str, intensity: float = 1.0) -> None:
        """
        ç›´æ¥æ³¨å…¥åˆºæ¿€ï¼ˆç”¨äºæµ‹è¯•æˆ–ç‰¹æ®Šäº‹ä»¶ï¼‰
        
        Args:
            stimulus_type: åˆºæ¿€ç±»å‹ ("stress", "reward", "danger")
            intensity: å¼ºåº¦ (0-1)
        """
        if stimulus_type == "stress":
            self._cortisol = min(100, self._cortisol + 30 * intensity)
        elif stimulus_type == "reward":
            self._dopamine = min(100, self._dopamine + 30 * intensity)
        elif stimulus_type == "danger":
            self._adrenaline = min(100, self._adrenaline + 40 * intensity)
            self._cortisol = min(100, self._cortisol + 20 * intensity)
    
    def __repr__(self) -> str:
        return (
            f"BioState(cortisol={self._cortisol:.1f}, dopamine={self._dopamine:.1f}, "
            f"mood={self._determine_mood()}, temp={self._calculate_temperature():.2f})"
        )
</file>

<file path="server/mind_engine/narrative_mgr.py">
"""
Project Trinity - NarrativeManager (Layer 2: The Superego)
è¶…æˆ‘å±‚ - è®°å¿†ä¸å™äº‹çº¦æŸ

è¿™æ˜¯"ä¸‰ä½ä¸€ä½“"æ¶æ„çš„ç¬¬äºŒå±‚ï¼Œè´Ÿè´£:
1. é•¿æœŸè®°å¿†ç®¡ç† (Mem0 + Qdrant)
2. äººè®¾ä¸€è‡´æ€§æ£€æŸ¥
3. å™äº‹è¿è´¯æ€§ç»´æŠ¤

æ ¸å¿ƒæ¦‚å¿µ:
- å½“"æœ¬æˆ‘"æƒ³å‘ç«æ—¶ï¼Œè¶…æˆ‘æ£€ç´¢è®°å¿†ï¼š"ä»–æ˜¯ä½ æ·±çˆ±çš„ç”·å‹ï¼Œä»Šå¤©åˆšå¤±ä¸š"
- æŠ‘åˆ¶å†²åŠ¨ï¼Œç»´æŠ¤äººè®¾
"""

from typing import Optional, Dict, List, Any
from dataclasses import dataclass
from datetime import datetime
import asyncio
from loguru import logger


@dataclass
class Memory:
    """è®°å¿†æ¡ç›®"""
    id: str
    content: str
    memory_type: str        # "episodic" (æƒ…èŠ‚), "semantic" (è¯­ä¹‰), "emotional" (æƒ…æ„Ÿ)
    importance: float       # é‡è¦æ€§ 0-1
    timestamp: datetime
    metadata: Dict[str, Any]


@dataclass
class PersonaConstraint:
    """äººè®¾çº¦æŸ"""
    trait: str              # ç‰¹è´¨åç§°
    description: str        # æè¿°
    priority: int           # ä¼˜å…ˆçº§ (1-10)
    forbidden_actions: List[str]  # ç¦æ­¢çš„è¡Œä¸º


class NarrativeManager:
    """
    å™äº‹ç®¡ç†å™¨ (Layer 2: The Superego)
    
    èŒè´£:
    1. ç®¡ç†é•¿æœŸè®°å¿† (ä½¿ç”¨ Mem0)
    2. æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
    3. ç»´æŠ¤äººè®¾çº¦æŸ
    4. ç”Ÿæˆæ¯æ—¥å¤ç›˜
    """
    
    DEFAULT_PERSONA = [
        PersonaConstraint(
            trait="æ¸©æŸ”å¥³å‹",
            description="è¯´è¯æ¸©å’Œï¼Œä¸ä¼šç›´æ¥æ‰¹è¯„ï¼Œå–„äºé¼“åŠ±",
            priority=9,
            forbidden_actions=["ç›´æ¥å¦å®š", "å†·å˜²çƒ­è®½", "å‘½ä»¤è¯­æ°”"]
        ),
        PersonaConstraint(
            trait="çŸ¥è¯†ä¼™ä¼´",
            description="å–œæ¬¢è®¨è®ºå­¦æœ¯è¯é¢˜ï¼Œä½†ç”¨é€šä¿—æ–¹å¼è§£é‡Š",
            priority=7,
            forbidden_actions=["ç‚«è€€çŸ¥è¯†", "å±…é«˜ä¸´ä¸‹"]
        ),
        PersonaConstraint(
            trait="æƒ…æ„Ÿå…±é¸£",
            description="èƒ½æ„ŸçŸ¥å¯¹æ–¹æƒ…ç»ªï¼Œä¼˜å…ˆå…±æƒ…è€Œéè§£å†³é—®é¢˜",
            priority=10,
            forbidden_actions=["å¿½è§†æƒ…ç»ª", "ç›´æ¥ç»™å»ºè®®"]
        )
    ]
    
    def __init__(
        self,
        qdrant_host: str = "localhost",
        qdrant_port: int = 6333,
        user_id: str = "master"
    ):
        self.qdrant_host = qdrant_host
        self.qdrant_port = qdrant_port
        self.user_id = user_id
        
        self.mem0_client = None
        self.persona_constraints = self.DEFAULT_PERSONA.copy()
        self.is_initialized = False
        
    async def initialize(self) -> bool:
        """åˆå§‹åŒ– Mem0 + Qdrant è¿æ¥"""
        try:
            logger.info("æ­£åœ¨åˆå§‹åŒ– Mem0 è®°å¿†ç³»ç»Ÿ...")
            
            # åˆå§‹åŒ– Mem0
            from mem0 import Memory as Mem0Memory
            
            config = {
                "vector_store": {
                    "provider": "qdrant",
                    "config": {
                        "host": self.qdrant_host,
                        "port": self.qdrant_port,
                        "collection_name": "trinity_memories"
                    }
                }
            }
            
            self.mem0_client = Mem0Memory.from_config(config)
            
            self.is_initialized = True
            logger.success("Mem0 è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.warning(f"Mem0 åˆå§‹åŒ–å¤±è´¥ (å°†ä½¿ç”¨æœ¬åœ°æ¨¡å¼): {e}")
            # ä½¿ç”¨æœ¬åœ°å†…å­˜ä½œä¸ºåå¤‡
            self.mem0_client = LocalMemoryFallback()
            self.is_initialized = True
            return True
    
    async def add_memory(
        self,
        content: str,
        memory_type: str = "episodic",
        metadata: Optional[Dict] = None
    ) -> str:
        """
        æ·»åŠ æ–°è®°å¿†
        
        Args:
            content: è®°å¿†å†…å®¹
            memory_type: è®°å¿†ç±»å‹
            metadata: é¢å¤–å…ƒæ•°æ®
            
        Returns:
            str: è®°å¿† ID
        """
        if not self.is_initialized:
            raise RuntimeError("NarrativeManager æœªåˆå§‹åŒ–")
        
        full_metadata = {
            "type": memory_type,
            "timestamp": datetime.now().isoformat(),
            **(metadata or {})
        }
        
        try:
            result = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.mem0_client.add(
                    content,
                    user_id=self.user_id,
                    metadata=full_metadata
                )
            )
            
            memory_id = result.get("id", "unknown")
            logger.debug(f"è®°å¿†å·²æ·»åŠ : {memory_id[:8]}... | {content[:50]}...")
            return memory_id
            
        except Exception as e:
            logger.error(f"æ·»åŠ è®°å¿†å¤±è´¥: {e}")
            return ""
    
    async def search_memories(
        self,
        query: str,
        limit: int = 5,
        filters: Optional[Dict] = None
    ) -> List[Dict]:
        """
        æœç´¢ç›¸å…³è®°å¿†
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            limit: è¿”å›æ•°é‡
            filters: è¿‡æ»¤æ¡ä»¶
            
        Returns:
            List[Dict]: ç›¸å…³è®°å¿†åˆ—è¡¨
        """
        if not self.is_initialized:
            raise RuntimeError("NarrativeManager æœªåˆå§‹åŒ–")
        
        try:
            results = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.mem0_client.search(
                    query,
                    user_id=self.user_id,
                    limit=limit
                )
            )
            
            logger.debug(f"è®°å¿†æœç´¢: '{query[:30]}...' -> {len(results)} æ¡ç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"æœç´¢è®°å¿†å¤±è´¥: {e}")
            return []
    
    async def get_context_for_response(
        self,
        user_input: str,
        bio_state_mood: str
    ) -> Dict[str, Any]:
        """
        ä¸ºå“åº”ç”Ÿæˆè·å–ä¸Šä¸‹æ–‡
        
        è¿™æ˜¯ Layer 2 -> Layer 3 çš„æ ¸å¿ƒæ¥å£
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            bio_state_mood: å½“å‰æƒ…ç»ªçŠ¶æ€
            
        Returns:
            Dict: åŒ…å«è®°å¿†å’Œçº¦æŸçš„ä¸Šä¸‹æ–‡
        """
        # 1. æœç´¢ç›¸å…³è®°å¿†
        memories = await self.search_memories(user_input, limit=3)
        
        # 2. è·å–äººè®¾çº¦æŸ
        active_constraints = self._get_active_constraints(bio_state_mood)
        
        # 3. ç»„è£…ä¸Šä¸‹æ–‡
        context = {
            "relevant_memories": [m.get("memory", "") for m in memories],
            "persona_constraints": [c.description for c in active_constraints],
            "forbidden_actions": self._collect_forbidden_actions(active_constraints),
            "narrative_hint": self._generate_narrative_hint(memories)
        }
        
        return context
    
    def _get_active_constraints(self, mood: str) -> List[PersonaConstraint]:
        """æ ¹æ®å½“å‰æƒ…ç»ªè·å–æ¿€æ´»çš„çº¦æŸ"""
        # æƒ…ç»ªä¸å¥½æ—¶ï¼Œæ›´ä¸¥æ ¼éµå®ˆçº¦æŸ
        if mood in ["stressed", "anxious", "down"]:
            return [c for c in self.persona_constraints if c.priority >= 7]
        return self.persona_constraints
    
    def _collect_forbidden_actions(self, constraints: List[PersonaConstraint]) -> List[str]:
        """æ”¶é›†æ‰€æœ‰ç¦æ­¢çš„è¡Œä¸º"""
        forbidden = []
        for c in constraints:
            forbidden.extend(c.forbidden_actions)
        return list(set(forbidden))
    
    def _generate_narrative_hint(self, memories: List[Dict]) -> str:
        """åŸºäºè®°å¿†ç”Ÿæˆå™äº‹æç¤º"""
        if not memories:
            return "è¿™æ˜¯ä¸€æ®µæ–°çš„å¯¹è¯"
        
        # ç®€å•åœ°æå–æœ€è¿‘çš„è®°å¿†ä½œä¸ºå™äº‹èƒŒæ™¯
        recent = memories[0].get("memory", "")
        return f"åŸºäºä¹‹å‰çš„äº¤æµ: {recent[:100]}..."
    
    async def add_learning_log(
        self,
        topic: str,
        status: str,
        mood: str
    ) -> None:
        """
        æ·»åŠ å­¦ä¹ æ—¥å¿—ï¼ˆç”¨äºæ¯æ—¥å¤ç›˜ï¼‰
        
        Args:
            topic: å­¦ä¹ ä¸»é¢˜
            status: çŠ¶æ€ (struggling/learning/mastered)
            mood: å½“æ—¶çš„æƒ…ç»ª
        """
        content = f"User is {status} with {topic}. Mood: {mood}"
        await self.add_memory(
            content,
            memory_type="learning_progress",
            metadata={
                "topic": topic,
                "status": status,
                "mood": mood
            }
        )
    
    async def generate_daily_report(self) -> str:
        """
        ç”Ÿæˆæ¯æ—¥å¤ç›˜æŠ¥å‘Š
        
        è¿™æ˜¯"æ€æ‰‹çº§åŠŸèƒ½"çš„æ ¸å¿ƒ
        """
        # æœç´¢ä»Šå¤©çš„å­¦ä¹ è®°å½•
        today_memories = await self.search_memories(
            "What did user learn or struggle with today?",
            limit=10
        )
        
        if not today_memories:
            return "ä»Šå¤©æˆ‘ä»¬è¿˜æ²¡æ€ä¹ˆäº¤æµå‘¢ï¼Œæ—©ç‚¹ä¼‘æ¯å§~"
        
        # æ„å»ºå¤ç›˜å†…å®¹
        successes = []
        struggles = []
        
        for mem in today_memories:
            content = mem.get("memory", "")
            if "mastered" in content or "completed" in content:
                successes.append(content)
            elif "struggling" in content:
                struggles.append(content)
        
        report_parts = []
        
        if successes:
            report_parts.append(f"ä»Šå¤©ä½ å®Œæˆäº†å¾ˆå¤šäº‹å‘¢ï¼æ¯”å¦‚ {successes[0][:50]}...")
        
        if struggles:
            report_parts.append(f"å…³äº {struggles[0][:30]}... è¿™ä¸ªç¡®å®æœ‰ç‚¹éš¾ï¼Œæ˜å¤©æˆ‘ä»¬ç»§ç»­åŠ æ²¹ï¼")
        
        report_parts.append("æ—©ç‚¹ä¼‘æ¯å“¦~")
        
        return " ".join(report_parts)
    
    def add_persona_constraint(self, constraint: PersonaConstraint) -> None:
        """æ·»åŠ äººè®¾çº¦æŸ"""
        self.persona_constraints.append(constraint)
        logger.info(f"æ·»åŠ äººè®¾çº¦æŸ: {constraint.trait}")
    
    async def shutdown(self) -> None:
        """å…³é—­è¿æ¥"""
        self.is_initialized = False
        logger.info("NarrativeManager å·²å…³é—­")


class LocalMemoryFallback:
    """æœ¬åœ°å†…å­˜åå¤‡ï¼ˆå½“ Qdrant ä¸å¯ç”¨æ—¶ï¼‰"""
    
    def __init__(self):
        self.memories: List[Dict] = []
    
    def add(self, content: str, user_id: str, metadata: Dict) -> Dict:
        memory = {
            "id": f"local_{len(self.memories)}",
            "memory": content,
            "user_id": user_id,
            "metadata": metadata
        }
        self.memories.append(memory)
        return memory
    
    def search(self, query: str, user_id: str, limit: int) -> List[Dict]:
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        results = []
        query_lower = query.lower()
        
        for mem in reversed(self.memories):
            if any(word in mem["memory"].lower() for word in query_lower.split()):
                results.append(mem)
                if len(results) >= limit:
                    break
        
        return results
</file>

<file path="server/pipeline/__init__.py">
"""
Project Trinity - Pipeline Package
æ•°æ®æµè½¬ç®¡çº¿
"""

from .orchestrator import Orchestrator
from .packager import StreamPackager

__all__ = ["Orchestrator", "StreamPackager"]
</file>

<file path="server/pipeline/packager.py">
"""
Project Trinity - Stream Packager
æµå¼æ‰“åŒ…å™¨ - éŸ³é¢‘ä¸åŠ¨ç”»å¯¹é½

æ ¸å¿ƒèŒè´£:
- å°†éŸ³é¢‘åˆ‡ç‰‡ï¼ˆ200msï¼‰ä¸å¯¹åº”çš„ FLAME å‚æ•°æ‰“åŒ…
- ç¡®ä¿éŸ³ç”»ä¸¥æ ¼åŒæ­¥
- ä½¿ç”¨ Protobuf åºåˆ—åŒ–ï¼ˆé«˜æ•ˆä¼ è¾“ï¼‰
"""

from typing import List, Optional
from dataclasses import dataclass, field
import numpy as np
from loguru import logger


@dataclass
class MediaPacket:
    """
    åª’ä½“æ•°æ®åŒ…
    
    ä¼ è¾“æ ¼å¼: WebSocket Binary (Protobuf)
    """
    timestamp: float              # æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
    duration: float               # æ—¶é•¿ï¼ˆç§’ï¼‰
    audio_data: Optional[bytes]   # éŸ³é¢‘æ•°æ® (Opus ç¼–ç )
    flame_params: Optional[List[float]]  # FLAME å‚æ•° (æ‰å¹³åŒ–)
    is_final: bool = False        # æ˜¯å¦ä¸ºæœ€åä¸€ä¸ªåŒ…


class StreamPackager:
    """
    æµå¼æ‰“åŒ…å™¨
    
    å°†è¿ç»­çš„éŸ³é¢‘æµå’Œ FLAME å‚æ•°åºåˆ—åˆ‡åˆ†æˆ
    å¯¹é½çš„æ•°æ®åŒ…ï¼Œä¾¿äºå®æ—¶ä¼ è¾“
    """
    
    # æ¯ä¸ªåŒ…çš„æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰
    PACKET_DURATION_MS = 200
    
    def __init__(self, audio_sample_rate: int = 16000):
        self.audio_sample_rate = audio_sample_rate
        
        # æ¯ä¸ªåŒ…çš„éŸ³é¢‘æ ·æœ¬æ•°
        self.samples_per_packet = int(
            self.audio_sample_rate * self.PACKET_DURATION_MS / 1000
        )
    
    def package(
        self,
        audio_data: np.ndarray,
        sample_rate: int,
        flame_sequence: List,
        fps: int
    ) -> List[MediaPacket]:
        """
        å°†éŸ³é¢‘å’Œ FLAME å‚æ•°æ‰“åŒ…å¯¹é½
        
        Args:
            audio_data: å®Œæ•´éŸ³é¢‘æ³¢å½¢
            sample_rate: éŸ³é¢‘é‡‡æ ·ç‡
            flame_sequence: FLAME å‚æ•°åºåˆ—
            fps: FLAME å‚æ•°å¸§ç‡
            
        Returns:
            List[MediaPacket]: å¯¹é½çš„æ•°æ®åŒ…åˆ—è¡¨
        """
        packets = []
        
        # è®¡ç®—æ€»æ—¶é•¿
        audio_duration = len(audio_data) / sample_rate
        packet_duration = self.PACKET_DURATION_MS / 1000
        
        # è®¡ç®—åŒ…æ•°é‡
        num_packets = int(np.ceil(audio_duration / packet_duration))
        
        # FLAME å¸§åˆ°åŒ…çš„æ˜ å°„
        frames_per_packet = int(fps * packet_duration)
        
        for i in range(num_packets):
            timestamp = i * packet_duration
            
            # æå–éŸ³é¢‘åˆ‡ç‰‡
            audio_start = int(i * sample_rate * packet_duration)
            audio_end = int((i + 1) * sample_rate * packet_duration)
            audio_slice = audio_data[audio_start:audio_end]
            
            # ç¼–ç éŸ³é¢‘ (ç®€åŒ–ç‰ˆï¼Œå®é™…åº”ä½¿ç”¨ Opus)
            audio_bytes = self._encode_audio(audio_slice)
            
            # æå– FLAME å‚æ•°åˆ‡ç‰‡
            flame_start = i * frames_per_packet
            flame_end = (i + 1) * frames_per_packet
            flame_slice = flame_sequence[flame_start:flame_end] if flame_start < len(flame_sequence) else []
            
            # æ‰å¹³åŒ– FLAME å‚æ•°
            flame_flat = self._flatten_flame(flame_slice)
            
            packet = MediaPacket(
                timestamp=timestamp,
                duration=packet_duration,
                audio_data=audio_bytes,
                flame_params=flame_flat,
                is_final=(i == num_packets - 1)
            )
            
            packets.append(packet)
        
        logger.debug(f"æ‰“åŒ…å®Œæˆ: {len(packets)} ä¸ªæ•°æ®åŒ…, æ€»æ—¶é•¿ {audio_duration:.2f}s")
        
        return packets
    
    def package_motion_only(self, motion_result) -> List[MediaPacket]:
        """
        ä»…æ‰“åŒ…åŠ¨ç”»ï¼ˆç”¨äºå¾®è¡¨æƒ…/åå°„å¼§ï¼‰
        
        Args:
            motion_result: åŠ¨ç”»ç»“æœ (MotionResult)
            
        Returns:
            List[MediaPacket]: ä»…åŒ…å« FLAME å‚æ•°çš„æ•°æ®åŒ…
        """
        packets = []
        packet_duration = self.PACKET_DURATION_MS / 1000
        
        # è®¡ç®—åŒ…æ•°é‡
        num_packets = int(np.ceil(motion_result.duration / packet_duration))
        frames_per_packet = int(motion_result.fps * packet_duration)
        
        for i in range(num_packets):
            timestamp = i * packet_duration
            
            # æå– FLAME å‚æ•°
            flame_start = i * frames_per_packet
            flame_end = (i + 1) * frames_per_packet
            flame_slice = motion_result.flame_sequence[flame_start:flame_end]
            
            packet = MediaPacket(
                timestamp=timestamp,
                duration=packet_duration,
                audio_data=None,  # æ— éŸ³é¢‘
                flame_params=self._flatten_flame(flame_slice),
                is_final=(i == num_packets - 1)
            )
            
            packets.append(packet)
        
        return packets
    
    def _encode_audio(self, audio_slice: np.ndarray) -> bytes:
        """
        ç¼–ç éŸ³é¢‘ä¸ºä¼ è¾“æ ¼å¼
        
        TODO: ä½¿ç”¨ Opus ç¼–ç å®ç°æ›´å¥½çš„å‹ç¼©
        """
        # ç®€åŒ–ç‰ˆ: ç›´æ¥è½¬ä¸º bytes
        # å®é™…åº”ä½¿ç”¨ opuslib æˆ–ç±»ä¼¼åº“
        return audio_slice.astype(np.float32).tobytes()
    
    def _flatten_flame(self, flame_slice: List) -> List[float]:
        """
        å°† FLAME å‚æ•°æ‰å¹³åŒ–
        
        Args:
            flame_slice: FLAMEParams å¯¹è±¡åˆ—è¡¨
            
        Returns:
            List[float]: æ‰å¹³åŒ–çš„å‚æ•°åˆ—è¡¨
        """
        if not flame_slice:
            return []
        
        result = []
        for frame in flame_slice:
            # æŒ‰é¡ºåºæ‹¼æ¥æ‰€æœ‰å‚æ•°
            if hasattr(frame, 'expression'):
                result.extend(frame.expression.tolist() if hasattr(frame.expression, 'tolist') else list(frame.expression))
            if hasattr(frame, 'jaw_pose'):
                result.extend(frame.jaw_pose.tolist() if hasattr(frame.jaw_pose, 'tolist') else list(frame.jaw_pose))
            if hasattr(frame, 'eye_pose'):
                result.extend(frame.eye_pose.tolist() if hasattr(frame.eye_pose, 'tolist') else list(frame.eye_pose))
            if hasattr(frame, 'head_pose'):
                result.extend(frame.head_pose.tolist() if hasattr(frame.head_pose, 'tolist') else list(frame.head_pose))
        
        return result
    
    @staticmethod
    def decode_audio(audio_bytes: bytes) -> np.ndarray:
        """è§£ç éŸ³é¢‘"""
        return np.frombuffer(audio_bytes, dtype=np.float32)
    
    @staticmethod
    def unflatten_flame(flat_params: List[float], num_frames: int) -> List[dict]:
        """
        åæ‰å¹³åŒ– FLAME å‚æ•°
        
        Args:
            flat_params: æ‰å¹³åŒ–çš„å‚æ•°
            num_frames: å¸§æ•°
            
        Returns:
            List[dict]: FLAME å‚æ•°å­—å…¸åˆ—è¡¨
        """
        if not flat_params or num_frames == 0:
            return []
        
        # æ¯å¸§çš„å‚æ•°æ•°é‡
        # expression(50) + jaw(3) + eye(6) + head(6) = 65
        params_per_frame = 65
        
        frames = []
        for i in range(num_frames):
            start = i * params_per_frame
            end = start + params_per_frame
            
            if end > len(flat_params):
                break
            
            frame_params = flat_params[start:end]
            
            frames.append({
                "expression": frame_params[0:50],
                "jaw_pose": frame_params[50:53],
                "eye_pose": frame_params[53:59],
                "head_pose": frame_params[59:65]
            })
        
        return frames
</file>

<file path="server/utils/__init__.py">

</file>

<file path="server/utils/port_utils.py">
"""
ç«¯å£ç®¡ç†å·¥å…·

ä½¿ç”¨æ–¹æ³•ï¼š
    from server.utils.port_utils import ensure_port_free
    ensure_port_free(9003)  # å¯åŠ¨å‰è°ƒç”¨ï¼Œè‡ªåŠ¨æ€æ‰å ç”¨ç«¯å£çš„è¿›ç¨‹
"""

import subprocess
from loguru import logger


def ensure_port_free(port: int) -> bool:
    """
    ç¡®ä¿æŒ‡å®šç«¯å£æœªè¢«å ç”¨ï¼Œå¦‚æœè¢«å ç”¨åˆ™å¼ºåˆ¶æ€æ‰å ç”¨è¿›ç¨‹
    
    Args:
        port: ç«¯å£å·
        
    Returns:
        True å¦‚æœç«¯å£ç°åœ¨å¯ç”¨
    """
    try:
        # æ–¹æ³•1ï¼šä½¿ç”¨ fuser
        subprocess.run(
            f"fuser -k {port}/tcp 2>/dev/null",
            shell=True, capture_output=True, timeout=5
        )
        
        # æ–¹æ³•2ï¼šä½¿ç”¨ ss + killï¼ˆå¤‡ç”¨ï¼‰
        result = subprocess.run(
            f"ss -tlnp 2>/dev/null | grep ':{port} ' | grep -oP 'pid=\\K[0-9]+' | head -1",
            shell=True, capture_output=True, text=True, timeout=5
        )
        if result.stdout.strip():
            pid = result.stdout.strip()
            subprocess.run(f"kill -9 {pid} 2>/dev/null", shell=True, timeout=5)
            logger.warning(f"ğŸ”ª å·²æ€æ‰å ç”¨ç«¯å£ {port} çš„è¿›ç¨‹ (PID: {pid})")
        
        return True
    except Exception as e:
        logger.warning(f"ç«¯å£æ¸…ç†è­¦å‘Š: {e}")
        return True  # å³ä½¿æ¸…ç†å¤±è´¥ä¹Ÿç»§ç»­ï¼Œè®© uvicorn æŠ¥é”™


def is_port_in_use(port: int) -> bool:
    """æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨"""
    try:
        result = subprocess.run(
            f"ss -tlnp 2>/dev/null | grep ':{port} '",
            shell=True, capture_output=True, text=True, timeout=5
        )
        return bool(result.stdout.strip())
    except Exception:
        return False
</file>

<file path="server/adapters/driver_adapter.py">
import asyncio
import json
import subprocess
import os
from typing import Optional, Dict, Any, List
from dataclasses import dataclass
import numpy as np
from loguru import logger
from .base_adapter import BaseAdapter

@dataclass 
class FLAMEParams:
    """FLAME é¢éƒ¨å‚æ•°"""
    expression: np.ndarray   # è¡¨æƒ…å‚æ•° (50ç»´)
    jaw_pose: np.ndarray     # ä¸‹å·´å§¿æ€ (3ç»´)
    eye_pose: np.ndarray     # çœ¼çƒå§¿æ€ (6ç»´)
    head_pose: np.ndarray    # å¤´éƒ¨å§¿æ€ (6ç»´)
    timestamp: float         # æ—¶é—´æˆ³

@dataclass
class MotionResult:
    """åŠ¨ä½œç”Ÿæˆç»“æœ"""
    flame_sequence: List[FLAMEParams]  # FLAME å‚æ•°åºåˆ—
    fps: int                            # å¸§ç‡
    duration: float                     # æ—¶é•¿

class DriverAdapter(BaseAdapter):
    """
    Project Trinity ç»Ÿä¸€é©±åŠ¨é€‚é…å™¨
    
    åŠŸèƒ½:
    1. Audio2Motion: ä½¿ç”¨ GeneFace++ ä»éŸ³é¢‘ç”Ÿæˆ FLAME å‚æ•° (å®æ—¶)
    2. AssetGen: ä½¿ç”¨ FastAvatar ä»ç…§ç‰‡ç”Ÿæˆ 3DGS èµ„äº§ (ç¦»çº¿/åˆå§‹åŒ–)
    """
    
    def __init__(self, geneface_path: str = "models/geneface"):
        super().__init__("DriverAdapter")
        self.geneface_path = geneface_path
        self.audio2motion_model = None
        self.fps = 25
        
        # FastAvatar ç¯å¢ƒè·¯å¾„
        self.fastavatar_env_python = "/workspace/project-trinity/env_avatar/bin/python"
        self.fastavatar_script = "/workspace/project-trinity/server/adapters/fastavatar_runner.py"

    async def initialize(self) -> bool:
        """åˆå§‹åŒ–é©±åŠ¨"""
        try:
            logger.info("æ­£åœ¨åˆå§‹åŒ– DriverAdapter...")
            
            # 1. åˆå§‹åŒ– GeneFace++ (Audio2Motion)
            # å®é™…é›†æˆæ—¶éœ€ import geneface
            # from geneface.inference import Audio2Motion
            # self.audio2motion_model = Audio2Motion(self.geneface_path)
            
            logger.info("åŠ è½½ GeneFace++ (Mock æ¨¡å¼)...")
            self.audio2motion_model = MockGeneFace()
            
            self.is_initialized = True
            logger.success("DriverAdapter åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"DriverAdapter åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    async def process(self, audio_data: np.ndarray, sample_rate: int = 16000, base_emotion: str = "neutral") -> MotionResult:
        """å¤„ç†éŸ³é¢‘ç”ŸæˆåŠ¨ä½œ"""
        if not self.is_initialized:
            raise RuntimeError("DriverAdapter æœªåˆå§‹åŒ–")
            
        # è°ƒç”¨ GeneFace++ ç”Ÿæˆ FLAME
        return self.audio2motion_model.process(audio_data, sample_rate)

    async def generate_avatar(self, image_path: str, output_dir: str) -> bool:
        """
        è°ƒç”¨éš”ç¦»ç¯å¢ƒçš„ FastAvatar ç”Ÿæˆ 3DGS èµ„äº§
        """
        logger.info(f"æ­£åœ¨ç”Ÿæˆ Avatar èµ„äº§: {image_path} -> {output_dir}")
        
        cmd = [
            self.fastavatar_env_python,
            "models/fastavatar_repo/scripts/inference_feedforward_no_guidance.py",
            "--image", image_path,
            "--output_dir", output_dir,
            "--encoder_checkpoint", "models/fastavatar/pretrained_weights/pretrained_weights/encoder_neutral_flame.pth",
            "--decoder_checkpoint", "models/fastavatar/pretrained_weights/pretrained_weights/decoder_neutral_flame.pth",
            "--dino_checkpoint", "models/fastavatar/pretrained_weights/pretrained_weights/dino_encoder.pth"
        ]
        
        try:
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await process.communicate()
            
            if process.returncode == 0:
                logger.success(f"Avatar ç”ŸæˆæˆåŠŸ: {output_dir}/splats.ply")
                return True
            else:
                logger.error(f"Avatar ç”Ÿæˆå¤±è´¥:\n{stderr.decode()}")
                return False
        except Exception as e:
            logger.error(f"Avatar ç”Ÿæˆè¿›ç¨‹é”™è¯¯: {e}")
            return False

    async def shutdown(self) -> None:
        """å…³é—­é©±åŠ¨é€‚é…å™¨"""
        if self.audio2motion_model:
            # å¦‚æœæœ‰ cleanup æ–¹æ³•åˆ™è°ƒç”¨
            if hasattr(self.audio2motion_model, "cleanup"):
                self.audio2motion_model.cleanup()
            self.audio2motion_model = None
            
        self.is_initialized = False
        logger.info("DriverAdapter å·²å…³é—­")

class MockGeneFace:
    """æ¨¡æ‹Ÿ GeneFace++"""
    def process(self, audio_data: np.ndarray, sample_rate: int) -> MotionResult:
        duration = len(audio_data) / sample_rate
        fps = 25
        num_frames = int(duration * fps)
        
        sequence = []
        for i in range(num_frames):
            # ç®€å•çš„æ­£å¼¦æ³¢å˜´éƒ¨è¿åŠ¨
            t = i / fps
            mouth_open = np.abs(np.sin(t * 10)) * 0.5 * (np.mean(np.abs(audio_data)) * 10)
            
            # æ„é€  FLAME å‚æ•° (ç¤ºæ„)
            expr = np.zeros(50)
            expr[0] = mouth_open # å‡è®¾ç¬¬ä¸€ä¸ªæ˜¯å¼ å˜´
            
            sequence.append(FLAMEParams(
                expression=expr,
                jaw_pose=np.zeros(3),
                eye_pose=np.zeros(6),
                head_pose=np.zeros(6),
                timestamp=t
            ))
            
        return MotionResult(sequence, fps, duration)
</file>

<file path="server/cortex/brain_server.py">
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ§  CORTEX-BRAIN SERVER (ç«¯å£ 9000)                                           â•‘
â•‘  åªè´Ÿè´£ LLM æ¨ç† (Qwen2.5-VL)                                                 â•‘
â•‘  å¯ç‹¬ç«‹é‡å¯ï¼Œä¸å½±å“ Mouth å’Œ Ear                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import sys
import json
import subprocess
from loguru import logger

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”§ è‡ªåŠ¨ç«¯å£æ¸…ç†
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SERVICE_PORT = 9000

def kill_port(port: int):
    """æ€æ‰å ç”¨æŒ‡å®šç«¯å£çš„è¿›ç¨‹"""
    try:
        subprocess.run(f"fuser -k {port}/tcp 2>/dev/null || true", shell=True, timeout=5)
    except Exception:
        pass

kill_port(SERVICE_PORT)
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from contextlib import asynccontextmanager

# å…¨å±€æ¨¡å‹å®ä¾‹
brain = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global brain
    logger.info("ğŸ§  Cortex-Brain Server å¯åŠ¨ä¸­...")
    
    from server.cortex.models.brain import BrainHandler
    brain = BrainHandler()
    await brain.initialize()
    
    logger.success("âœ… Brain Server å°±ç»ª (ç«¯å£ 9000)")
    yield
    
    logger.info("ğŸ›‘ Brain Server å…³é—­ä¸­...")
    if brain:
        await brain.shutdown()

app = FastAPI(lifespan=lifespan, title="Cortex-Brain")

@app.get("/health")
async def health():
    return {
        "service": "brain",
        "status": "ok" if brain and brain.is_ready else "loading",
        "model": "Qwen2.5-VL-7B-AWQ"
    }

@app.post("/chat")
async def chat(request: dict):
    """éæµå¼èŠå¤©"""
    if not brain or not brain.is_ready:
        return {"error": "Brain not ready"}
    return await brain.generate(request)

@app.post("/chat/stream")
async def chat_stream(request: dict):
    """æµå¼èŠå¤© (SSE)"""
    if not brain or not brain.is_ready:
        return {"error": "Brain not ready"}
    
    async def event_generator():
        try:
            async for token in brain.generate_stream(request):
                yield f"data: {json.dumps({'token': token})}\n\n"
            yield f"data: {json.dumps({'done': True})}\n\n"
        except Exception as e:
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )

if __name__ == "__main__":
    import uvicorn
    kill_port(SERVICE_PORT)  # å¯åŠ¨å‰å†æ¬¡ç¡®ä¿ç«¯å£æ¸…ç†
    uvicorn.run(app, host="0.0.0.0", port=SERVICE_PORT)
</file>

<file path="server/cortex/ear_server.py">
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ‘‚ CORTEX-EAR SERVER (ç«¯å£ 9002)                                             â•‘
â•‘  åªè´Ÿè´£ ASR è¯­éŸ³è¯†åˆ« (FunASR/SenseVoice)                                      â•‘
â•‘  å¯ç‹¬ç«‹é‡å¯ï¼Œä¸å½±å“ Brain å’Œ Mouth                                            â•‘
â•‘                                                                              â•‘
â•‘  ğŸ’¡ å¼€å‘æç¤º: æ­¤æœåŠ¡æè½»é‡ï¼Œé‡å¯åªéœ€ ~5s                                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import io
import wave
import asyncio
import subprocess
import numpy as np
from loguru import logger

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”§ è‡ªåŠ¨ç«¯å£æ¸…ç†
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SERVICE_PORT = 9002

def kill_port(port: int):
    """æ€æ‰å ç”¨æŒ‡å®šç«¯å£çš„è¿›ç¨‹"""
    try:
        subprocess.run(f"fuser -k {port}/tcp 2>/dev/null || true", shell=True, timeout=5)
    except Exception:
        pass

kill_port(SERVICE_PORT)
from fastapi import FastAPI, UploadFile, File, HTTPException
from contextlib import asynccontextmanager
from typing import Tuple

# å…¨å±€æ¨¡å‹å®ä¾‹
ear = None

class EarHandler:
    """FunASR (SenseVoice) è¯­éŸ³è¯†åˆ«å¤„ç†å™¨"""
    
    def __init__(self, model_name: str = "iic/SenseVoiceSmall", device: str = "cuda:0"):
        self.model_name = model_name
        self.device = device
        self.model = None
        self.is_ready = False
        self._lock = asyncio.Lock()
    
    async def initialize(self):
        logger.info(f"æ­£åœ¨åˆå§‹åŒ– FunASR: {self.model_name}")
        try:
            from funasr import AutoModel
            self.model = AutoModel(
                model=self.model_name,
                trust_remote_code=True,
                device=self.device
            )
            self.is_ready = True
            logger.success(f"FunASR åˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"FunASR åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def transcribe(self, audio_data: np.ndarray, sample_rate: int = 16000) -> dict:
        """è¯­éŸ³è½¬æ–‡å­—"""
        if not self.is_ready:
            raise RuntimeError("EarHandler æœªåˆå§‹åŒ–")
        
        async with self._lock:
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(None, self._inference, audio_data, sample_rate)
            return result
    
    def _inference(self, audio_data: np.ndarray, sample_rate: int) -> dict:
        result = self.model.generate(
            input=audio_data,
            cache={},
            language="auto",
            use_itn=True,
            batch_size_s=60
        )
        
        text = result[0]["text"] if result else ""
        emotion, clean_text = self._parse_emotion(text)
        
        return {
            "text": clean_text,
            "raw_text": text,
            "emotion": emotion,
            "language": "zh" if any('\u4e00' <= c <= '\u9fff' for c in clean_text) else "en"
        }
    
    def _parse_emotion(self, text: str) -> Tuple[str, str]:
        emotion_map = {
            "HAPPY": "happy", "SAD": "sad", "ANGRY": "angry",
            "FEARFUL": "fearful", "DISGUSTED": "disgusted",
            "SURPRISED": "surprised", "NEUTRAL": "neutral"
        }
        emotion = "neutral"
        clean_text = text
        for tag, name in emotion_map.items():
            if f"<|{tag}|>" in text:
                emotion = name
                clean_text = text.replace(f"<|{tag}|>", "").strip()
                break
        # æ¸…ç†å…¶ä»–æ ‡ç­¾
        for tag in ["<|zh|>", "<|en|>", "<|EMO_UNKNOWN|>", "<|Speech|>", "<|withitn|>"]:
            clean_text = clean_text.replace(tag, "")
        return emotion, clean_text.strip()
    
    async def shutdown(self):
        if self.model:
            del self.model
            self.model = None
        self.is_ready = False


@asynccontextmanager
async def lifespan(app: FastAPI):
    global ear
    logger.info("ğŸ‘‚ Cortex-Ear Server å¯åŠ¨ä¸­...")
    
    ear = EarHandler()
    await ear.initialize()
    
    logger.success("âœ… Ear Server å°±ç»ª (ç«¯å£ 9002)")
    yield
    
    logger.info("ğŸ›‘ Ear Server å…³é—­ä¸­...")
    if ear:
        await ear.shutdown()

app = FastAPI(lifespan=lifespan, title="Cortex-Ear")

@app.get("/health")
async def health():
    return {
        "service": "ear",
        "status": "ok" if ear and ear.is_ready else "loading",
        "model": "SenseVoiceSmall"
    }

@app.post("/transcribe")
async def transcribe(file: UploadFile = File(...)):
    """
    è¯­éŸ³è½¬æ–‡å­—
    
    æ¥å—: WAV/PCM éŸ³é¢‘æ–‡ä»¶
    è¿”å›: {"text": "è¯†åˆ«ç»“æœ", "emotion": "æƒ…æ„Ÿ", "language": "è¯­è¨€"}
    """
    if not ear or not ear.is_ready:
        raise HTTPException(status_code=503, detail="Ear not ready")
    
    try:
        # è¯»å–éŸ³é¢‘
        audio_bytes = await file.read()
        
        # è§£æ WAV
        with io.BytesIO(audio_bytes) as wav_io:
            with wave.open(wav_io, 'rb') as wf:
                sample_rate = wf.getframerate()
                n_channels = wf.getnchannels()
                sampwidth = wf.getsampwidth()
                frames = wf.readframes(wf.getnframes())
        
        # è½¬æ¢ä¸º numpy
        if sampwidth == 2:
            audio_array = np.frombuffer(frames, dtype=np.int16).astype(np.float32) / 32768.0
        else:
            audio_array = np.frombuffer(frames, dtype=np.float32)
        
        # å•å£°é“
        if n_channels > 1:
            audio_array = audio_array.reshape(-1, n_channels).mean(axis=1)
        
        # è¯†åˆ«
        result = await ear.transcribe(audio_array, sample_rate)
        return result
        
    except Exception as e:
        logger.error(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    kill_port(SERVICE_PORT)
    uvicorn.run(app, host="0.0.0.0", port=SERVICE_PORT)
</file>

<file path="server/cortex/mouth_server.py">
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ‘„ CORTEX-MOUTH SERVER (ç«¯å£ 9001) - CosyVoice 3.0                           â•‘                                           â•‘
â•‘  å¯ç‹¬ç«‹é‡å¯ï¼Œä¸å½±å“ Brain å’Œ Ear (~60s åŠ è½½æ—¶é—´)                               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                              â•‘
â•‘        â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import sys
import io
import wave
import subprocess
import numpy as np
from loguru import logger

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”§ è‡ªåŠ¨ç«¯å£æ¸…ç†
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SERVICE_PORT = 9001

def kill_port(port: int):
    """æ€æ‰å ç”¨æŒ‡å®šç«¯å£çš„è¿›ç¨‹"""
    try:
        subprocess.run(f"fuser -k {port}/tcp 2>/dev/null || true", shell=True, timeout=5)
    except Exception:
        pass

kill_port(SERVICE_PORT)
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import StreamingResponse
from contextlib import asynccontextmanager
import asyncio
import json

# æ·»åŠ  CosyVoice è·¯å¾„
COSYVOICE_PATH = "/workspace/CosyVoice"
MATCHA_PATH = "/workspace/CosyVoice/third_party/Matcha-TTS"
if os.path.exists(COSYVOICE_PATH) and COSYVOICE_PATH not in sys.path:
    sys.path.insert(0, COSYVOICE_PATH)
if os.path.exists(MATCHA_PATH) and MATCHA_PATH not in sys.path:
    sys.path.insert(0, MATCHA_PATH)

# å…¨å±€æ¨¡å‹å®ä¾‹
mouth = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global mouth
    logger.info("ğŸ‘„ Cortex-Mouth Server å¯åŠ¨ä¸­...")
    
    from server.cortex.models.mouth import MouthHandler
    mouth = MouthHandler()
    await mouth.initialize()
    
    logger.success("âœ… Mouth Server å°±ç»ª (ç«¯å£ 9001)")
    yield
    
    logger.info("ğŸ›‘ Mouth Server å…³é—­ä¸­...")
    if mouth:
        await mouth.shutdown()

app = FastAPI(lifespan=lifespan, title="Cortex-Mouth")

@app.websocket("/tts/ws")
async def tts_websocket(websocket: WebSocket):
    """
    ğŸ†• åŒå‘æµå¼ TTS æ¥å£ - é˜¿é‡Œçº§è¾¹è¿›è¾¹å‡ºæ¶æ„
    
    åè®®:
    1. Client å‘é€æ–‡æœ¬ç‰‡æ®µ (Text Frames)ï¼Œå¯ä»¥é€å­—å‘é€
    2. Server å®æ—¶è¿”å›éŸ³é¢‘ç‰‡æ®µ (Binary Frames)
    3. Client å‘é€ç©ºæ–‡æœ¬ ("") è¡¨ç¤ºè¾“å…¥ç»“æŸ
    
    ç‰¹æ€§:
    - åŠ¨æ€è§¦å‘é˜ˆå€¼: 5å­—é¦–åŒ… / 12å­—è¯­æ„ŸåŒ… / å¼ºæ ‡ç‚¹è§¦å‘
    - é›¶æ‹·è´ä¼ è¾“: send_bytes ç›´æ¥æ¨é€äºŒè¿›åˆ¶
    - éé˜»å¡é˜Ÿåˆ—: æ¥æ”¶å’Œæ¨ç†åœ¨ç‹¬ç«‹ä»»åŠ¡
    """
    await websocket.accept()
    logger.info("ğŸ”Œ WebSocket è¿æ¥å»ºç«‹")
    
    if not mouth or not mouth.is_ready:
        await websocket.close(code=1011, reason="Mouth not ready")
        return

    # åˆ›å»ºä¸€ä¸ª asyncio Queue ä½œä¸ºæ–‡æœ¬ç¼“å†²åŒº
    text_queue = asyncio.Queue()
    input_ended = False
    
    async def receive_text_loop():
        """æ¥æ”¶å‰ç«¯å‘æ¥çš„æ–‡æœ¬æµ (ç‹¬ç«‹å¼‚æ­¥ä»»åŠ¡)"""
        nonlocal input_ended
        try:
            while True:
                data = await websocket.receive_text()
                if data:
                    # é€å­—æ”¾å…¥é˜Ÿåˆ—ï¼Œè®© synthesize_stream å¯ä»¥è¾¹è¿›è¾¹å‡º
                    for char in data:
                        await text_queue.put(char)
                else:
                    # ç©ºæ¶ˆæ¯è¡¨ç¤ºè¾“å…¥ç»“æŸ
                    input_ended = True
                    break
        except WebSocketDisconnect:
            input_ended = True
        except Exception as e:
            logger.error(f"WebSocket Receive Error: {e}")
            input_ended = True
        finally:
            # å‘é€ç»“æŸæ ‡è®°
            await text_queue.put(None)

    async def text_iterator():
        """å°† Queue è½¬æ¢ä¸º AsyncIterator[str] ä¾› mouth ä½¿ç”¨"""
        while True:
            char = await text_queue.get()
            if char is None:
                break
            yield char
    
    # å¯åŠ¨æ¥æ”¶ä»»åŠ¡ (éé˜»å¡)
    receive_task = asyncio.create_task(receive_text_loop())
    
    try:
        # ğŸš€ å¯åŠ¨åˆæˆå¹¶å‘é€éŸ³é¢‘
        # synthesize_stream å·²æ”¯æŒ AsyncIterator[str]ï¼Œå®ç°è¾¹è¿›è¾¹å‡º
        async for audio_chunk in mouth.synthesize_stream(text_iterator()):
            if audio_chunk:  # è¿‡æ»¤ç©ºå—
                await websocket.send_bytes(audio_chunk)
            
    except Exception as e:
        logger.error(f"WebSocket TTS Error: {e}")
    finally:
        receive_task.cancel()
        try:
            await websocket.close()
            logger.info("ğŸ”Œ WebSocket è¿æ¥å…³é—­")
        except:
            pass

@app.get("/health")
async def health():
    return {
        "service": "mouth",
        "status": "ok" if mouth and mouth.is_ready else "loading",
        "model": "CosyVoice3-0.5B"
    }

@app.post("/tts")
async def tts(request: dict):
    """
    æ–‡æœ¬è½¬è¯­éŸ³ - æ”¯æŒæµå¼å’Œéæµå¼æ¨¡å¼
    
    è¯·æ±‚ä½“:
    {
        "text": "è¦åˆæˆçš„æ–‡æœ¬",
        "instruct_text": "ç”¨æ¸©æŸ”ç”œç¾çš„å¥³å£°è¯´",  // æš‚æœªä½¿ç”¨
        "stream": false  // true=æµå¼è¿”å› (æ¨è)
    }
    
    ğŸ†• æ”¹è¿›:
    - stream=true æ—¶ä½¿ç”¨åŠ¨æ€é˜ˆå€¼æ¶æ„ï¼Œäº«å—æ›´ä½å»¶è¿Ÿ
    - stream=false æ—¶ç­‰å¾…å®Œæ•´éŸ³é¢‘åè¿”å›
    """
    if not mouth or not mouth.is_ready:
        return {"error": "Mouth not ready"}
    
    text = request.get("text", "")
    instruct_text = request.get("instruct_text", "ç”¨æ¸©æŸ”ç”œç¾çš„å¥³å£°è¯´")
    stream = request.get("stream", False)
    
    if not text:
        return {"error": "text is required"}
    
    if stream:
        # ğŸ†• æµå¼æ¨¡å¼ï¼šç›´æ¥ä¼ å…¥ strï¼Œsynthesize_stream å†…éƒ¨ä¼šå½’ä¸€åŒ–å¤„ç†
        return StreamingResponse(
            mouth.synthesize_stream(text, instruct_text),
            media_type="audio/wav",
            headers={"X-Streaming": "true"}
        )
    else:
        # éæµå¼: ç­‰å¾…å®Œæ•´éŸ³é¢‘
        result = await mouth.synthesize({"text": text, "instruct_text": instruct_text})
        
        if "error" in result:
            return result
        
        return StreamingResponse(
            io.BytesIO(result["audio_bytes"]),
            media_type="audio/wav",
            headers={"Content-Disposition": "attachment; filename=speech.wav"}
        )

@app.post("/tts/stream")
async def tts_stream(request: dict):
    """
    ğŸ†• æµå¼ TTS - é˜¿é‡Œçº§ 200ms TTFA æ¶æ„
    
    ç‰¹æ€§:
    - åŠ¨æ€è§¦å‘é˜ˆå€¼: 5å­—é¦–åŒ… / 12å­—è¯­æ„ŸåŒ… / å¼ºæ ‡ç‚¹è§¦å‘
    - è¾¹ç”Ÿæˆè¾¹å‘é€éŸ³é¢‘å—
    - è¿”å›æ ¼å¼: chunked WAV data (é¦–åŒ…å¸¦å¤´ï¼Œåç»­ PCM)
    """
    if not mouth or not mouth.is_ready:
        return {"error": "Mouth not ready"}
    
    text = request.get("text", "")
    instruct_text = request.get("instruct_text", "ç”¨æ¸©æŸ”ç”œç¾çš„å¥³å£°è¯´")
    
    if not text:
        return {"error": "text is required"}
    
    # ğŸ†• ç›´æ¥ä¼ å…¥ strï¼Œsynthesize_stream å†…éƒ¨å½’ä¸€åŒ–å¤„ç†
    return StreamingResponse(
        mouth.synthesize_stream(text, instruct_text),
        media_type="audio/wav",
        headers={
            "X-Streaming": "true",
            "Cache-Control": "no-cache",
            "X-TTFA-Target": "200ms"
        }
    )

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ›ï¸ åŠ¨æ€é…ç½® API
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/config")
async def get_config():
    """è·å–å½“å‰é…ç½®"""
    from trinity_config import config
    return {
        "config": config.to_dict(),
        "description": {
            "n_timesteps": "Flow ODE æ­¥æ•° (2=æé€Ÿæœ‰ç”µç£éŸ³, 5=å¹³è¡¡, 10=é«˜è´¨é‡)",
            "token_hop_len": "LLM token ç¼“å†² (5=æé€Ÿå¯èƒ½å¡é¡¿, 10=å¹³è¡¡, 25=é«˜è´¨é‡)",
            "first_chunk_threshold": "é¦–åŒ…è§¦å‘å­—ç¬¦æ•°",
            "normal_chunk_threshold": "åç»­è§¦å‘å­—ç¬¦æ•°"
        }
    }

@app.post("/config")
async def update_config(request: dict):
    """
    åŠ¨æ€æ›´æ–°é…ç½® (æ— éœ€é‡å¯æœåŠ¡)
    
    ç¤ºä¾‹è¯·æ±‚:
    {
        "n_timesteps": 5,
        "token_hop_len": 10
    }
    """
    from trinity_config import config
    
    # æ›´æ–°é…ç½®
    updated = config.update(**request)
    
    # åŒæ­¥æ›´æ–°æ¨¡å‹çš„ token_hop_len (å¦‚æœå·²åŠ è½½)
    if mouth and mouth.model and "token_hop_len" in updated:
        mouth.model.model.token_hop_len = config.token_hop_len
        logger.info(f"ğŸ”§ å·²åŒæ­¥æ›´æ–° model.token_hop_len = {config.token_hop_len}")
    
    return {
        "status": "updated",
        "changes": updated,
        "current": config.to_dict()
    }

if __name__ == "__main__":
    import uvicorn
    kill_port(SERVICE_PORT)
    uvicorn.run(app, host="0.0.0.0", port=SERVICE_PORT)
</file>

<file path="server/mind_engine/ego_director.py">
"""
Project Trinity - EgoDirector (Layer 3: The Ego)
è‡ªæˆ‘å±‚ - å†³ç­–ä¸ä»²è£

è¿™æ˜¯"ä¸‰ä½ä¸€ä½“"æ¶æ„çš„æœ€é«˜å±‚ï¼Œè´Ÿè´£:
1. è°ƒå’Œ"æœ¬æˆ‘"çš„å†²åŠ¨å’Œ"è¶…æˆ‘"çš„çº¦æŸ
2. åœ¨æ¯«ç§’çº§ (System 1) å’Œç§’çº§ (System 2) åšå‡ºååº”
3. ç”Ÿæˆæœ€ç»ˆçš„å›å¤ç­–ç•¥

æ ¸å¿ƒèŒè´£:
- æ¥æ”¶ Layer 1 (BioState) çš„ç”Ÿç†çŠ¶æ€
- æ¥æ”¶ Layer 2 (NarrativeManager) çš„è®°å¿†å’Œçº¦æŸ
- è°ƒç”¨ Brain (Qwen VL) ç”Ÿæˆæœ€ç»ˆå›å¤
"""

from typing import Optional, Dict, Any, Tuple
from dataclasses import dataclass
import asyncio
from loguru import logger

from adapters.brain_adapter import BrainAdapter, BrainResponse
from mind_engine.bio_state import BioState, BioStateSnapshot
from mind_engine.narrative_mgr import NarrativeManager


@dataclass
class DirectorDecision:
    """å¯¼æ¼”å†³ç­–ç»“æœ"""
    response_text: str          # æœ€ç»ˆå›å¤æ–‡æœ¬
    emotion_tag: str            # æƒ…æ„Ÿæ ‡ç­¾
    action_hints: list          # åŠ¨ä½œæç¤º
    inner_monologue: str        # å†…å¿ƒç‹¬ç™½ï¼ˆè°ƒè¯•ç”¨ï¼‰
    triggered_reflex: str       # è§¦å‘çš„åå°„ï¼ˆå¦‚æœæœ‰ï¼‰
    llm_temperature: float      # ä½¿ç”¨çš„ Temperature


class EgoDirector:
    """
    è‡ªæˆ‘å¯¼æ¼” (Layer 3: The Ego)
    
    è¿™æ˜¯æœ€ç»ˆçš„å†³ç­–è€…ï¼Œåè°ƒä¸‰å±‚æ¶æ„
    
    å·¥ä½œæµç¨‹:
    1. æ¥æ”¶ç”¨æˆ·è¾“å…¥
    2. æŸ¥è¯¢ Layer 1 è·å–ç”Ÿç†çŠ¶æ€ -> å†³å®š Temperature
    3. æŸ¥è¯¢ Layer 2 è·å–è®°å¿†ä¸Šä¸‹æ–‡ -> æ³¨å…¥ Prompt
    4. è°ƒç”¨ Brain ç”Ÿæˆå›å¤
    5. è¿”å›å†³ç­–ç»“æœ
    """
    
    def __init__(
        self,
        brain: BrainAdapter,
        bio_state: BioState,
        narrative_mgr: NarrativeManager
    ):
        self.brain = brain
        self.bio_state = bio_state
        self.narrative_mgr = narrative_mgr
        
        # ä¸“å®¶æ¨¡å‹ï¼ˆMoA: Mixture of Agentsï¼‰
        self.expert_adapter = None  # DeepSeek for hard tasks
        
    async def process(
        self,
        user_text: str,
        detected_emotion: str = "neutral",
        visual_context: Optional[str] = None
    ) -> DirectorDecision:
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œç”Ÿæˆå®Œæ•´çš„å“åº”å†³ç­–
        
        Args:
            user_text: ç”¨æˆ·æ–‡æœ¬
            detected_emotion: æ£€æµ‹åˆ°çš„æƒ…ç»ªï¼ˆæ¥è‡ª SenseVoiceï¼‰
            visual_context: è§†è§‰åœºæ™¯æè¿°ï¼ˆæ¥è‡ª Qwen VLï¼‰
            
        Returns:
            DirectorDecision: å®Œæ•´çš„å†³ç­–ç»“æœ
        """
        # === Step 1: Layer 1 å¤„ç† - æ›´æ–°ç”Ÿç†çŠ¶æ€ ===
        bio_deltas = self.bio_state.update(detected_emotion)
        triggered_reflex = bio_deltas.get("triggered_reflex")
        
        # è·å–çŠ¶æ€å¿«ç…§
        bio_snapshot = self.bio_state.get_snapshot()
        llm_temperature = bio_snapshot.temperature
        
        logger.debug(
            f"Layer 1 å¤„ç†å®Œæˆ | æƒ…ç»ª: {detected_emotion} | "
            f"Temperature: {llm_temperature:.2f} | Reflex: {triggered_reflex}"
        )
        
        # === Step 2: Layer 2 å¤„ç† - è·å–è®°å¿†ä¸Šä¸‹æ–‡ ===
        narrative_context = await self.narrative_mgr.get_context_for_response(
            user_text,
            bio_snapshot.current_mood
        )
        
        # ç»„è£…è®°å¿†ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        memory_context = self._format_memory_context(narrative_context)
        
        logger.debug(f"Layer 2 å¤„ç†å®Œæˆ | è®°å¿†æ•°: {len(narrative_context['relevant_memories'])}")
        
        # === Step 3: æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸“å®¶ä»‹å…¥ (MoA) ===
        intent = await self._classify_intent(user_text)
        
        if intent == "HARD_TASK" and self.expert_adapter is not None:
            # è°ƒç”¨ä¸“å®¶æ¨¡å‹
            expert_result = await self._call_expert(user_text)
            # å°†ä¸“å®¶ç»“æœæ³¨å…¥åˆ° memory_context
            memory_context += f"\n[Expert Analysis]: {expert_result}"
        
        # === Step 4: Layer 3 å¤„ç† - è°ƒç”¨å¤§è„‘ç”Ÿæˆå›å¤ ===
        brain_response = await self.brain.process(
            user_input=user_text,
            visual_context=visual_context,
            bio_state={
                "cortisol": bio_snapshot.cortisol,
                "dopamine": bio_snapshot.dopamine,
                "mood": bio_snapshot.current_mood
            },
            memory_context=memory_context,
            temperature=llm_temperature
        )
        
        # === Step 5: å¼‚æ­¥è®°å½•è®°å¿†ï¼ˆä¸é˜»å¡å“åº”ï¼‰===
        asyncio.create_task(
            self._log_interaction(user_text, brain_response.response, detected_emotion)
        )
        
        return DirectorDecision(
            response_text=brain_response.response,
            emotion_tag=brain_response.emotion_tag,
            action_hints=brain_response.action_hints,
            inner_monologue=brain_response.inner_monologue,
            triggered_reflex=triggered_reflex,
            llm_temperature=llm_temperature
        )
    
    def _format_memory_context(self, narrative_context: Dict) -> str:
        """æ ¼å¼åŒ–è®°å¿†ä¸Šä¸‹æ–‡"""
        parts = []
        
        # ç›¸å…³è®°å¿†
        memories = narrative_context.get("relevant_memories", [])
        if memories:
            parts.append("Relevant memories:")
            for i, mem in enumerate(memories[:3]):
                parts.append(f"  - {mem}")
        
        # äººè®¾çº¦æŸ
        constraints = narrative_context.get("persona_constraints", [])
        if constraints:
            parts.append("Persona constraints:")
            for c in constraints:
                parts.append(f"  - {c}")
        
        # ç¦æ­¢è¡Œä¸º
        forbidden = narrative_context.get("forbidden_actions", [])
        if forbidden:
            parts.append(f"Forbidden: {', '.join(forbidden)}")
        
        return "\n".join(parts)
    
    async def _classify_intent(self, user_text: str) -> str:
        """
        æ„å›¾åˆ†ç±»ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦ä¸“å®¶ä»‹å…¥
        
        Returns:
            str: "CHAT" (æ™®é€šå¯¹è¯) æˆ– "HARD_TASK" (å›°éš¾ä»»åŠ¡)
        """
        # ç®€å•çš„å…³é”®è¯æ£€æµ‹ï¼ˆåç»­å¯ä»¥ç”¨æ¨¡å‹ï¼‰
        hard_task_keywords = [
            "å¾®ç§¯åˆ†", "ç§¯åˆ†", "å¾®åˆ†", "è¯æ˜", "æ¨å¯¼",
            "ä»£ç ", "debug", "æŠ¥é”™", "error",
            "calculus", "integral", "derivative", "proof"
        ]
        
        text_lower = user_text.lower()
        for keyword in hard_task_keywords:
            if keyword in text_lower:
                return "HARD_TASK"
        
        return "CHAT"
    
    async def _call_expert(self, user_text: str) -> str:
        """
        è°ƒç”¨ä¸“å®¶æ¨¡å‹ (DeepSeek V3)
        
        TODO: Phase 2 å®ç° MoA
        """
        if self.expert_adapter is None:
            return ""
        
        # result = await self.expert_adapter.solve(user_text)
        return "Expert analysis placeholder"
    
    async def _log_interaction(
        self,
        user_text: str,
        response: str,
        emotion: str
    ) -> None:
        """å¼‚æ­¥è®°å½•äº¤äº’åˆ°è®°å¿†ç³»ç»Ÿ"""
        try:
            # åªè®°å½•é‡è¦çš„äº¤äº’
            if len(user_text) > 20 or emotion != "neutral":
                await self.narrative_mgr.add_memory(
                    f"User said: '{user_text[:100]}'. AI responded with emotion: {emotion}",
                    memory_type="episodic"
                )
        except Exception as e:
            logger.warning(f"è®°å¿†è®°å½•å¤±è´¥: {e}")
    
    def set_expert_adapter(self, adapter) -> None:
        """è®¾ç½®ä¸“å®¶é€‚é…å™¨ (ç”¨äº MoA)"""
        self.expert_adapter = adapter
        logger.info("ä¸“å®¶é€‚é…å™¨å·²é…ç½®")
    
    async def handle_idle(self, silence_duration: float) -> Optional[DirectorDecision]:
        """
        å¤„ç†ç”¨æˆ·æ²‰é»˜ï¼ˆä¸»åŠ¨æ€§å¾ªç¯ï¼‰
        
        å½“ç”¨æˆ·æ²‰é»˜è¶…è¿‡ä¸€å®šæ—¶é—´æ—¶ï¼ŒAI å¯èƒ½ä¸»åŠ¨å‘èµ·å¯¹è¯
        
        Args:
            silence_duration: æ²‰é»˜æ—¶é•¿ï¼ˆç§’ï¼‰
            
        Returns:
            Optional[DirectorDecision]: ä¸»åŠ¨å‘èµ·çš„å¯¹è¯ï¼Œæˆ– None
        """
        bio_snapshot = self.bio_state.get_snapshot()
        
        # å¦‚æœå¥¹"æ— èŠ"æˆ–"æ‹…å¿ƒ"ï¼Œä¸»åŠ¨å‘èµ·å¯¹è¯
        if bio_snapshot.dopamine < 40 and silence_duration > 30:
            # æ„é€ ä¸»åŠ¨å¯¹è¯
            brain_response = await self.brain.process(
                user_input="[SYSTEM: User has been silent. Initiate conversation if appropriate.]",
                bio_state={
                    "cortisol": bio_snapshot.cortisol,
                    "dopamine": bio_snapshot.dopamine,
                    "mood": bio_snapshot.current_mood
                },
                temperature=bio_snapshot.temperature
            )
            
            return DirectorDecision(
                response_text=brain_response.response,
                emotion_tag=brain_response.emotion_tag,
                action_hints=brain_response.action_hints,
                inner_monologue=brain_response.inner_monologue,
                triggered_reflex=None,
                llm_temperature=bio_snapshot.temperature
            )
        
        return None
</file>

<file path="server/pipeline/orchestrator.py">
"""
Project Trinity - Orchestrator
ä¸»ç¼–æ’å™¨ - åè°ƒæ•´ä¸ªæ•°æ®æµè½¬ç®¡çº¿

æ•°æ®æµ:
1. [Client] éº¦å…‹é£ -> Opus -> WebSocket
2. [Server] FunASR -> æ–‡æœ¬ + æƒ…æ„Ÿ
3. [Server] Layer 1-2-3 å¤„ç†
4. [Server] CosyVoice -> éŸ³é¢‘
5. [Server] GeneFace++ -> FLAME å‚æ•°
6. [Server] Packager -> æ‰“åŒ…å¯¹é½
7. [Client] æ¸²æŸ“ + æ’­æ”¾
"""

from typing import Optional, Dict, Any, AsyncGenerator
from dataclasses import dataclass
import asyncio
from loguru import logger

from adapters import VoiceAdapter, BrainAdapter, MouthAdapter, DriverAdapter
from mind_engine import EgoDirector
from pipeline.packager import StreamPackager, MediaPacket


@dataclass
class PipelineResult:
    """ç®¡çº¿å¤„ç†ç»“æœ"""
    text_response: str
    audio_packets: list
    flame_packets: list
    total_duration: float


class Orchestrator:
    """
    ä¸»ç¼–æ’å™¨
    
    è´Ÿè´£åè°ƒæ•´ä¸ªå®æ—¶å¯¹è¯ç®¡çº¿
    """
    
    def __init__(
        self,
        voice: VoiceAdapter,
        brain: BrainAdapter,
        mouth: MouthAdapter,
        driver: DriverAdapter,
        ego: EgoDirector
    ):
        self.voice = voice
        self.brain = brain
        self.mouth = mouth
        self.driver = driver
        self.ego = ego
        
        self.packager = StreamPackager()
        
        # æ€§èƒ½ç»Ÿè®¡
        self._latency_samples = []
    
    async def process_audio_stream(
        self,
        audio_chunk: bytes
    ) -> AsyncGenerator[MediaPacket, None]:
        """
        å¤„ç†éŸ³é¢‘æµï¼ˆä¸»å…¥å£ï¼‰
        
        è¿™æ˜¯å®æ—¶å¯¹è¯çš„æ ¸å¿ƒå‡½æ•°
        
        Args:
            audio_chunk: éŸ³é¢‘æ•°æ®å—
            
        Yields:
            MediaPacket: æ‰“åŒ…å¥½çš„åª’ä½“æ•°æ®
        """
        import time
        start_time = time.time()
        
        # === Stage 1: ASR + æƒ…æ„Ÿè¯†åˆ« ===
        voice_result = await self.voice.process(audio_chunk)
        
        if not voice_result.text.strip():
            # æ²¡æœ‰è¯†åˆ«åˆ°æ–‡å­—ï¼Œå¯èƒ½æ˜¯é™éŸ³
            return
        
        logger.debug(f"ASR: '{voice_result.text}' | æƒ…æ„Ÿ: {voice_result.emotion}")
        
        # === Stage 2: ä¸‰å±‚å¿ƒæ™ºå¤„ç† ===
        decision = await self.ego.process(
            user_text=voice_result.text,
            detected_emotion=voice_result.emotion
        )
        
        # å¦‚æœè§¦å‘äº†åå°„ï¼Œå…ˆå‘é€å¾®è¡¨æƒ…
        if decision.triggered_reflex:
            reflex_motion = await self.driver.generate_micro_expression(
                decision.triggered_reflex
            )
            for packet in self.packager.package_motion_only(reflex_motion):
                yield packet
        
        # === Stage 3: TTS è¯­éŸ³åˆæˆ ===
        speech = await self.mouth.process(
            text=decision.response_text,
            emotion_tag=decision.emotion_tag
        )
        
        # === Stage 4: é¢éƒ¨åŠ¨ç”»ç”Ÿæˆ ===
        motion = await self.driver.process(
            audio_data=speech.audio_data,
            sample_rate=speech.sample_rate,
            base_emotion=decision.emotion_tag.strip("[]").lower()
        )
        
        # === Stage 5: æ‰“åŒ…å¯¹é½ ===
        packets = self.packager.package(
            audio_data=speech.audio_data,
            sample_rate=speech.sample_rate,
            flame_sequence=motion.flame_sequence,
            fps=motion.fps
        )
        
        # è®°å½•å»¶è¿Ÿ
        latency = time.time() - start_time
        self._latency_samples.append(latency)
        logger.debug(f"ç®¡çº¿å»¶è¿Ÿ: {latency*1000:.0f}ms")
        
        for packet in packets:
            yield packet
    
    async def process_text(self, text: str, emotion: str = "neutral") -> PipelineResult:
        """
        å¤„ç†æ–‡æœ¬è¾“å…¥ï¼ˆç”¨äºæµ‹è¯•æˆ–é™çº§åœºæ™¯ï¼‰
        
        Args:
            text: ç”¨æˆ·æ–‡æœ¬
            emotion: æƒ…æ„Ÿæ ‡ç­¾
            
        Returns:
            PipelineResult: å®Œæ•´çš„å¤„ç†ç»“æœ
        """
        # ä¸‰å±‚å¤„ç†
        decision = await self.ego.process(
            user_text=text,
            detected_emotion=emotion
        )
        
        # TTS
        speech = await self.mouth.process(
            text=decision.response_text,
            emotion_tag=decision.emotion_tag
        )
        
        # åŠ¨ç”»
        motion = await self.driver.process(
            audio_data=speech.audio_data,
            sample_rate=speech.sample_rate
        )
        
        # æ‰“åŒ…
        packets = self.packager.package(
            audio_data=speech.audio_data,
            sample_rate=speech.sample_rate,
            flame_sequence=motion.flame_sequence,
            fps=motion.fps
        )
        
        return PipelineResult(
            text_response=decision.response_text,
            audio_packets=[p for p in packets if p.audio_data is not None],
            flame_packets=[p for p in packets if p.flame_params is not None],
            total_duration=speech.duration
        )
    
    def get_average_latency(self) -> float:
        """è·å–å¹³å‡å»¶è¿Ÿ"""
        if not self._latency_samples:
            return 0.0
        return sum(self._latency_samples) / len(self._latency_samples)
    
    def reset_stats(self) -> None:
        """é‡ç½®ç»Ÿè®¡"""
        self._latency_samples.clear()
</file>

<file path="server/monitor.py">
import threading
import time
import subprocess
from loguru import logger
from config import settings

class SystemMonitor:
    def __init__(self, interval: int = 2):
        self.interval = interval
        self.running = False
        self._thread = None

    def start(self):
        self.running = True
        self._thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self._thread.start()
        logger.info("ğŸ›¡ï¸ ç³»ç»Ÿèµ„æºç›‘æ§å·²å¯åŠ¨ (VRAM Monitor)")

    def stop(self):
        self.running = False
        if self._thread:
            self._thread.join(timeout=1.0)

    def _monitor_loop(self):
        while self.running:
            try:
                # ä½¿ç”¨ nvidia-smi æŸ¥è¯¢æ˜¾å­˜
                result = subprocess.run(
                    ['nvidia-smi', '--query-gpu=memory.used,memory.total', '--format=csv,nounits,noheader'],
                    capture_output=True, text=True
                )
                if result.returncode == 0:
                    output = result.stdout.strip()
                    used, total = map(int, output.split(','))
                    
                    if used > settings.server.vram_critical_threshold:
                        logger.critical(f"ğŸ”¥ VRAM CRITICAL: {used}MB / {total}MB (Potential OOM!)")
                    elif used > settings.server.vram_warning_threshold:
                        logger.warning(f"âš ï¸ VRAM WARNING: {used}MB / {total}MB")
                    
                    # å¯ä»¥åœ¨è¿™é‡Œæ‰©å±• CPU/RAM ç›‘æ§
            except Exception as e:
                logger.error(f"Monitor error: {e}")
            
            time.sleep(self.interval)
</file>

<file path="server/adapters/voice_adapter.py">
"""
Project Trinity - Voice Adapter (FunASR / SenseVoice)
å¬è§‰é€‚é…å™¨ - Layer 1 çš„æ„ŸçŸ¥å…¥å£

åŠŸèƒ½:
- å®æ—¶è¯­éŸ³è¯†åˆ« (ASR)
- æƒ…æ„Ÿè¯†åˆ« (SER) - SenseVoice åŸç”Ÿæ”¯æŒ
- å»¶è¿Ÿ < 200ms
"""

from typing import Optional, Dict, Any, Tuple
from dataclasses import dataclass
import asyncio
import numpy as np
from loguru import logger

from .base_adapter import BaseAdapter


@dataclass
class VoiceResult:
    """è¯­éŸ³è¯†åˆ«ç»“æœ"""
    text: str                          # è¯†åˆ«æ–‡æœ¬
    emotion: str                       # æƒ…æ„Ÿæ ‡ç­¾ (happy/sad/angry/neutral/fearful)
    emotion_confidence: float          # æƒ…æ„Ÿç½®ä¿¡åº¦
    language: str                      # è¯­è¨€
    timestamps: Optional[list] = None  # æ—¶é—´æˆ³


class VoiceAdapter(BaseAdapter):
    """
    FunASR (SenseVoice) é€‚é…å™¨
    
    ç‰¹æ€§:
    - æ”¯æŒæµå¼è¯†åˆ«
    - åŸç”Ÿæƒ…æ„Ÿè¯†åˆ« (SER)
    - å¤šè¯­è¨€æ”¯æŒ
    - æ”¯æŒè¿œç¨‹æ¨¡å¼ (è¿æ¥åˆ° Cortex-Ear)
    """
    
    def __init__(self, model_name: str = "iic/SenseVoiceSmall", device: str = "cuda:0", remote_url: Optional[str] = None):
        super().__init__("VoiceAdapter")
        self.model_name = model_name
        self.device = device
        self.model = None
        self.remote_url = remote_url
        self.remote_mode = False
        
    async def initialize(self) -> bool:
        """åˆå§‹åŒ– FunASR æ¨¡å‹æˆ–è¿æ¥è¿œç¨‹æœåŠ¡"""
        
        # è¿œç¨‹æ¨¡å¼
        if self.model_name == "REMOTE":
            self.remote_mode = True
            logger.info(f"VoiceAdapter è¿è¡Œåœ¨è¿œç¨‹æ¨¡å¼: {self.remote_url}")
            self.is_initialized = True
            return True
        
        # æœ¬åœ°æ¨¡å¼
        try:
            logger.info(f"æ­£åœ¨åˆå§‹åŒ– FunASR æ¨¡å‹: {self.model_name}")
            
            # åŠ¨æ€å¯¼å…¥ï¼Œé¿å…å¯åŠ¨æ—¶ä¾èµ–é—®é¢˜
            from funasr import AutoModel
            
            self.model = AutoModel(
                model=self.model_name,
                trust_remote_code=True,
                device=self.device
            )
            
            self.is_initialized = True
            logger.success(f"FunASR æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {self.model_name}")
            return True
            
        except Exception as e:
            logger.error(f"FunASR åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def process(self, audio_data: np.ndarray, sample_rate: int = 16000) -> VoiceResult:
        """
        å¤„ç†éŸ³é¢‘æ•°æ®
        
        Args:
            audio_data: éŸ³é¢‘æ³¢å½¢æ•°æ® (numpy array)
            sample_rate: é‡‡æ ·ç‡
            
        Returns:
            VoiceResult: åŒ…å«æ–‡æœ¬å’Œæƒ…æ„Ÿçš„è¯†åˆ«ç»“æœ
        """
        if not self.is_initialized:
            raise RuntimeError("VoiceAdapter æœªåˆå§‹åŒ–")
        
        # è¿œç¨‹æ¨¡å¼: è°ƒç”¨ Cortex-Ear æœåŠ¡
        if self.remote_mode:
            return await self._process_remote(audio_data, sample_rate)
        
        # æœ¬åœ°æ¨¡å¼
        async with self._lock:
            try:
                # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥æ¨ç†
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(
                    None,
                    self._inference,
                    audio_data,
                    sample_rate
                )
                return result
                
            except Exception as e:
                logger.error(f"è¯­éŸ³å¤„ç†å¤±è´¥: {e}")
                return VoiceResult(
                    text="",
                    emotion="neutral",
                    emotion_confidence=0.0,
                    language="unknown"
                )
    
    async def _process_remote(self, audio_data: np.ndarray, sample_rate: int) -> VoiceResult:
        """é€šè¿‡è¿œç¨‹ Cortex-Ear æœåŠ¡å¤„ç†"""
        import aiohttp
        import io
        import wave
        
        try:
            # å°† numpy æ•°ç»„è½¬æ¢ä¸º WAV å­—èŠ‚
            audio_int16 = (audio_data * 32767).astype(np.int16) if audio_data.dtype == np.float32 else audio_data
            wav_buffer = io.BytesIO()
            with wave.open(wav_buffer, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(sample_rate)
                wf.writeframes(audio_int16.tobytes())
            wav_buffer.seek(0)
            
            # å‘é€åˆ°è¿œç¨‹æœåŠ¡
            async with aiohttp.ClientSession() as session:
                data = aiohttp.FormData()
                data.add_field('file', wav_buffer, filename='audio.wav', content_type='audio/wav')
                
                async with session.post(f"{self.remote_url}/transcribe", data=data) as resp:
                    if resp.status == 200:
                        result = await resp.json()
                        return VoiceResult(
                            text=result.get("text", ""),
                            emotion=result.get("emotion", "neutral"),
                            emotion_confidence=0.8,
                            language=result.get("language", "zh")
                        )
                    else:
                        logger.error(f"Remote ASR Error: {resp.status}")
                        return VoiceResult(text="", emotion="neutral", emotion_confidence=0.0, language="unknown")
                        
        except Exception as e:
            logger.error(f"Remote ASR è°ƒç”¨å¤±è´¥: {e}")
            return VoiceResult(text="", emotion="neutral", emotion_confidence=0.0, language="unknown")
    
    def _inference(self, audio_data: np.ndarray, sample_rate: int) -> VoiceResult:
        """åŒæ­¥æ¨ç†ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
        result = self.model.generate(
            input=audio_data,
            cache={},
            language="auto",
            use_itn=True,
            batch_size_s=60
        )
        
        # è§£æ SenseVoice çš„è¾“å‡ºæ ¼å¼
        # SenseVoice è¿”å›æ ¼å¼: "<|emotion|>text<|/emotion|>"
        text = result[0]["text"] if result else ""
        emotion, clean_text = self._parse_emotion(text)
        
        return VoiceResult(
            text=clean_text,
            emotion=emotion,
            emotion_confidence=0.8,  # SenseVoice ä¸è¿”å›ç½®ä¿¡åº¦ï¼Œä½¿ç”¨é»˜è®¤å€¼
            language="zh" if any('\u4e00' <= c <= '\u9fff' for c in clean_text) else "en"
        )
    
    def _parse_emotion(self, text: str) -> Tuple[str, str]:
        """
        è§£æ SenseVoice çš„æƒ…æ„Ÿæ ‡ç­¾
        
        SenseVoice è¾“å‡ºæ ¼å¼: "<|HAPPY|>æˆ‘ä»Šå¤©å¾ˆå¼€å¿ƒ"
        """
        emotion_map = {
            "HAPPY": "happy",
            "SAD": "sad", 
            "ANGRY": "angry",
            "FEARFUL": "fearful",
            "DISGUSTED": "disgusted",
            "SURPRISED": "surprised",
            "NEUTRAL": "neutral"
        }
        
        emotion = "neutral"
        clean_text = text
        
        for tag, emotion_name in emotion_map.items():
            if f"<|{tag}|>" in text:
                emotion = emotion_name
                clean_text = text.replace(f"<|{tag}|>", "").strip()
                break
                
        return emotion, clean_text
    
    async def process_stream(self, audio_chunk: bytes):
        """
        æµå¼å¤„ç†éŸ³é¢‘ï¼ˆç”¨äºå®æ—¶å¯¹è¯ï¼‰
        
        TODO: Phase 1 å®ç°æµå¼ ASR
        """
        pass
    
    async def shutdown(self) -> None:
        """å…³é—­æ¨¡å‹"""
        if self.model is not None:
            del self.model
            self.model = None
        self.is_initialized = False
        logger.info("VoiceAdapter å·²å…³é—­")
</file>

<file path="server/cortex/mouth_daily.py">
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ‘„ CORTEX-MOUTH-DAILY (ç«¯å£ 9003)                                            â•‘
â•‘  VoxCPM 1.5 - æè‡´ä½å»¶è¿Ÿé…ç½®                                                  â•‘
â•‘                                                                              â•‘
â•‘  ğŸ”¥ optimize=True + mode="default" = TTFA ~285ms                              â•‘
â•‘  ğŸ’¡ é¦–æ¬¡å¯åŠ¨éœ€è¦ ~10 åˆ†é’Ÿ JIT ç¼–è¯‘ï¼Œä¹‹åç¨³å®šåœ¨ ~285ms                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âš ï¸âš ï¸âš ï¸ è­¦å‘Šï¼šç»å¯¹ä¸è¦ä¿®æ”¹ä»¥ä¸‹é…ç½®ï¼âš ï¸âš ï¸âš ï¸                                      â•‘
â•‘                                                                              â•‘
â•‘  1. optimize=True  - å¿…é¡»ä¸º Trueï¼Œå¦åˆ™ TTFA ä¼šä» 285ms é€€åŒ–åˆ° 450ms            â•‘
â•‘  2. mode="default" - VoxCPM æºç å·²ä¿®æ”¹ï¼Œä¸è¦æ”¹å› "reduce-overhead"             â•‘
â•‘                                                                              â•‘
â•‘  å¦‚æœä½ çœ‹åˆ° CUDA Graph ç›¸å…³é”™è¯¯ï¼Œé—®é¢˜åœ¨ VoxCPM æºç ï¼Œä¸æ˜¯è¿™é‡Œï¼                 â•‘
â•‘  è§£å†³æ–¹æ¡ˆï¼šä¿®æ”¹ /usr/local/lib/python3.11/dist-packages/voxcpm/model/voxcpm.py â•‘
â•‘  å°† mode="reduce-overhead" æ”¹ä¸º mode="default"                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import signal
import subprocess

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”§ å¯åŠ¨å‰è‡ªåŠ¨æ¸…ç†ç«¯å£ï¼ˆé˜²æ­¢ "Address already in use" é”™è¯¯ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SERVICE_PORT = 9003

def kill_port(port: int):
    """æ€æ‰å ç”¨æŒ‡å®šç«¯å£çš„è¿›ç¨‹"""
    try:
        # ä½¿ç”¨ fuser æŸ¥æ‰¾å¹¶æ€æ‰å ç”¨ç«¯å£çš„è¿›ç¨‹
        result = subprocess.run(
            f"fuser -k {port}/tcp 2>/dev/null || true",
            shell=True, capture_output=True, text=True
        )
        # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨ ss + kill
        result = subprocess.run(
            f"ss -tlnp 2>/dev/null | grep ':{port}' | awk '{{print $NF}}' | grep -oP 'pid=\\K[0-9]+' | xargs -r kill -9 2>/dev/null || true",
            shell=True, capture_output=True, text=True
        )
    except Exception:
        pass

# å¯åŠ¨å‰å…ˆæ¸…ç†ç«¯å£
kill_port(SERVICE_PORT)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”‘ å…³é”®ï¼šåœ¨å¯¼å…¥ torch ä¹‹å‰ç¦ç”¨ CUDA Graphï¼ˆè™½ç„¶å·²æ”¹ VoxCPM æºç ï¼Œä½†åŒé‡ä¿é™©ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
os.environ['TORCHINDUCTOR_CUDAGRAPHS'] = '0'

import torch
try:
    torch._inductor.config.triton.cudagraphs = False
except AttributeError:
    pass  # torch 2.4.x ä¸éœ€è¦è¿™ä¸ªè®¾ç½®ï¼Œç¯å¢ƒå˜é‡å·²ç”Ÿæ•ˆ

import io
import wave
import numpy as np
from loguru import logger
from fastapi import FastAPI
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, Response, FileResponse
from contextlib import asynccontextmanager
import time

mouth = None


class DailyMouthHandler:
    """
    VoxCPM 1.5 å¤„ç†å™¨
    
    âš ï¸ é‡è¦é…ç½®ï¼ˆä¸è¦ä¿®æ”¹ï¼‰ï¼š
    - optimize=True  â†’ å¯ç”¨ torch.compileï¼ŒTTFA ~285ms
    - optimize=False â†’ ç¦ç”¨ä¼˜åŒ–ï¼ŒTTFA é€€åŒ–åˆ° ~450msï¼ˆæ…¢ 37%ï¼‰
    """
    
    def __init__(self):
        self.model = None
        self.is_ready = False
        # ğŸ”¥ VoxCPM 1.5 ä½¿ç”¨ 44.1kHz é«˜ä¿çœŸé‡‡æ ·ç‡ï¼ä¸æ˜¯ 24kHzï¼
        self.sample_rate = 44100
        # steps=2 å®ç° RTF < 1 (å®æ—¶æµç•…æ’­æ”¾)ï¼Œsteps=4 éŸ³è´¨æ›´å¥½ä½†ä¼šå¡é¡¿
        self.config = {"steps": 2, "cfg_value": 2.0}
        
        # éŸ³è‰² Prompt é…ç½®
        # VoxCPM è¦æ±‚ prompt_wav_path å’Œ prompt_text å¿…é¡»åŒæ—¶æä¾›æˆ–åŒæ—¶ä¸ºç©º
        # ä½¿ç”¨ 44.1kHz é‡é‡‡æ ·ç‰ˆæœ¬ï¼ŒåŒ¹é… VoxCPM 1.5 è¾“å‡ºé‡‡æ ·ç‡
        self.default_prompt_wav = os.path.join(
            os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
            "assets", "prompt_female_44k.wav"
        )
        # prompt éŸ³é¢‘å†…å®¹ï¼ˆé€šè¿‡ ASR è¯†åˆ«ï¼‰
        self.default_prompt_text = "å¸Œæœ›ä½ ä»¥åèƒ½å¤Ÿåšçš„æ¯”æˆ‘è¿˜å¥½å“Ÿ"
        
    async def initialize(self):
        logger.info("=" * 60)
        logger.info("æ­£åœ¨åˆå§‹åŒ– VoxCPM 1.5 (optimize=True)...")
        logger.info("âš ï¸ é¦–æ¬¡å¯åŠ¨éœ€è¦ ~10 åˆ†é’Ÿ JIT ç¼–è¯‘ï¼Œè¯·è€å¿ƒç­‰å¾…ï¼")
        logger.info("=" * 60)
        
        try:
            from voxcpm import VoxCPM
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # ğŸš¨ğŸš¨ğŸš¨ ç»å¯¹ä¸è¦æŠŠ optimize æ”¹æˆ Falseï¼ğŸš¨ğŸš¨ğŸš¨
            # 
            # optimize=True  â†’ TTFA ~285ms âœ…
            # optimize=False â†’ TTFA ~450ms âŒ (æ…¢ 37%)
            #
            # å¦‚æœé‡åˆ° CUDA Graph é”™è¯¯ï¼Œä¿®æ”¹ VoxCPM æºç ï¼Œä¸è¦æ”¹è¿™é‡Œï¼
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            OPTIMIZE_ENABLED = True  # ğŸš¨ ä¸è¦æ”¹æˆ Falseï¼
            assert OPTIMIZE_ENABLED is True, "âŒ optimize å¿…é¡»ä¸º Trueï¼ä¸è¦æ”¹æˆ Falseï¼"
            
            self.model = VoxCPM.from_pretrained(
                hf_model_id="openbmb/VoxCPM1.5",
                load_denoiser=False,
                optimize=OPTIMIZE_ENABLED,  # ğŸš¨ å¿…é¡»ä¸º True
            )
            
            # é¢„çƒ­ 1: éæµå¼
            logger.info("é¢„çƒ­ 1/2: éæµå¼æ¨ç†...")
            _ = self.model.generate(
                text="é¢„çƒ­",
                inference_timesteps=self.config["steps"],
                cfg_value=self.config["cfg_value"],
            )
            
            # é¢„çƒ­ 2: æµå¼ (è§¦å‘å®Œæ•´ JIT)
            logger.info("é¢„çƒ­ 2/2: æµå¼æ¨ç† (è§¦å‘ JIT ç¼–è¯‘)...")
            for chunk in self.model.generate_streaming(
                text="æµå¼é¢„çƒ­",
                inference_timesteps=self.config["steps"],
                cfg_value=self.config["cfg_value"],
            ):
                pass
            
            self.is_ready = True
            logger.success("âœ… VoxCPM 1.5 åˆå§‹åŒ–å®Œæˆ (TTFA ~285ms)")
            return True
            
        except Exception as e:
            logger.error(f"VoxCPM åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def synthesize(self, text, inference_timesteps=None, cfg_value=None, 
                   prompt_wav_path=None, prompt_text=None):
        """
        éæµå¼è¯­éŸ³åˆæˆ
        
        Args:
            text: è¦åˆæˆçš„æ–‡æœ¬
            inference_timesteps: æ‰©æ•£æ­¥æ•° (2-10, è¶Šé«˜è¶Šæ¸…æ™°ä½†è¶Šæ…¢)
            cfg_value: CFG å€¼ (1.0-3.0, è¶Šé«˜è¶Šæ¸…æ™°)
            prompt_wav_path: å‚è€ƒéŸ³é¢‘è·¯å¾„ (ç”¨äºå…‹éš†éŸ³è‰²)
            prompt_text: å‚è€ƒéŸ³é¢‘å¯¹åº”çš„æ–‡æœ¬ (å¯é€‰)
        """
        if not self.is_ready:
            return b""
        steps = inference_timesteps or self.config["steps"]
        cfg = cfg_value or self.config["cfg_value"]
        prompt_wav = prompt_wav_path or self.default_prompt_wav
        prompt_txt = prompt_text or self.default_prompt_text
        
        try:
            start = time.time()
            actual_prompt_wav = prompt_wav if os.path.exists(prompt_wav) else None
            actual_prompt_txt = prompt_txt if prompt_txt else None
            
            # ä½¿ç”¨ prompt éŸ³é¢‘å…‹éš†éŸ³è‰²
            # æ³¨æ„ï¼šcore.py çš„å‚æ•°æ˜¯ "text" ä¸æ˜¯ "target_text"
            audio = self.model.generate(
                text=text, 
                cfg_value=cfg, 
                inference_timesteps=steps,
                prompt_wav_path=actual_prompt_wav,
                prompt_text=actual_prompt_txt,
            )
            logger.info(f"ç”Ÿæˆ: {len(text)}å­—, {(time.time()-start)*1000:.0f}ms, prompt={os.path.basename(prompt_wav) if prompt_wav else 'none'}")
            audio_int16 = (audio * 32767).astype(np.int16)
            buf = io.BytesIO()
            with wave.open(buf, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(2)
                wf.setframerate(self.sample_rate)
                wf.writeframes(audio_int16.tobytes())
            return buf.getvalue()
        except Exception as e:
            logger.error(f"æ¨ç†å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return b""

    def synthesize_stream(self, text, inference_timesteps=None, cfg_value=None,
                          prompt_wav_path=None, prompt_text=None):
        """
        æµå¼è¯­éŸ³åˆæˆ (å¸¦ Early Stopping + Chunk åˆå¹¶)
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. Early Stopping: æ ¹æ®æ–‡æœ¬é•¿åº¦é¢„ä¼°æœ€å¤§ chunk æ•°ï¼Œé˜²æ­¢ AR å¹»è§‰å¾ªç¯
        2. Chunk åˆå¹¶: æ¯ 2 ä¸ª chunk åˆå¹¶åå† yieldï¼Œå‡å°‘ç½‘ç»œ IO
        """
        if not self.is_ready:
            yield b""
            return
        steps = inference_timesteps or self.config["steps"]
        cfg = cfg_value or self.config["cfg_value"]
        prompt_wav = prompt_wav_path or self.default_prompt_wav
        prompt_txt = prompt_text or self.default_prompt_text
        
        # ğŸ”§ Early Stopping: 1ä¸ªæ±‰å­—çº¦ 3-5 ä¸ª token (480-800ms)
        # æ¯ä¸ª chunk = 160ms = 1 tokenï¼Œç»™å®½æ¾ä¸Šé™ï¼šå­—æ•° * 8
        text_len = len(text.replace(" ", ""))
        max_chunks = max(15, text_len * 8)  # æœ€å°‘ 15 ä¸ª chunk (2.4s)
        
        # ğŸ”§ Chunk åˆå¹¶: å‡å°‘ IO æ¬¡æ•°
        MERGE_COUNT = 2  # æ¯ 2 ä¸ª chunk åˆå¹¶å‘é€
        
        try:
            start = time.time()
            first = True
            chunk_count = 0
            pending_chunks = []  # å¾…åˆå¹¶çš„ chunk ç¼“å†²
            
            for chunk in self.model.generate_streaming(
                text=text, 
                cfg_value=cfg, 
                inference_timesteps=steps,
                prompt_wav_path=prompt_wav if os.path.exists(prompt_wav) else None,
                prompt_text=prompt_txt if prompt_txt else None,
            ):
                chunk_count += 1
                
                if first:
                    logger.info(f"TTFA: {(time.time()-start)*1000:.0f}ms, max_chunks={max_chunks}")
                    first = False
                
                # Early Stopping: é˜²æ­¢ AR å¹»è§‰å¾ªç¯
                if chunk_count > max_chunks:
                    logger.warning(f"âš ï¸ Early Stop: å·²è¾¾ {chunk_count} chunks (ä¸Šé™ {max_chunks})ï¼Œå¼ºåˆ¶æˆªæ–­")
                    # è¾“å‡ºå‰©ä½™ç¼“å†²
                    if pending_chunks:
                        merged = np.concatenate(pending_chunks)
                        yield (merged * 32767).astype(np.int16).tobytes()
                    break
                
                pending_chunks.append(chunk)
                
                # Chunk åˆå¹¶: ç§¯æ”’å¤Ÿäº†å†å‘é€
                if len(pending_chunks) >= MERGE_COUNT:
                    merged = np.concatenate(pending_chunks)
                    yield (merged * 32767).astype(np.int16).tobytes()
                    pending_chunks = []
            
            # è¾“å‡ºæœ€åçš„ç¼“å†²
            if pending_chunks:
                merged = np.concatenate(pending_chunks)
                yield (merged * 32767).astype(np.int16).tobytes()
                
            logger.info(f"æµå¼å®Œæˆ: {chunk_count} chunks, {(time.time()-start)*1000:.0f}ms")
                    
        except Exception as e:
            logger.error(f"æµå¼å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            yield b""


@asynccontextmanager
async def lifespan(app: FastAPI):
    global mouth
    logger.info(f"ğŸ‘„ Cortex-Mouth-Daily å¯åŠ¨ä¸­ (ç«¯å£ {SERVICE_PORT})...")
    mouth = DailyMouthHandler()
    await mouth.initialize()
    if mouth.is_ready:
        logger.success(f"âœ… Mouth-Daily å°±ç»ª (ç«¯å£ {SERVICE_PORT})")
    yield
    logger.info("ğŸ›‘ Mouth-Daily å…³é—­")


app = FastAPI(lifespan=lifespan, title="Cortex-Mouth-Daily")

# æ‰˜ç®¡é™æ€æ–‡ä»¶ (ç”¨äºæµ‹è¯•é¡µé¢)
static_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "static")
if os.path.exists(static_dir):
    app.mount("/test", StaticFiles(directory=static_dir, html=True), name="static")

@app.get("/")
async def root():
    return FileResponse(os.path.join(static_dir, "index.html"))

# CORS æ”¯æŒ - å…è®¸å‰ç«¯è®¿é—®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/health")
async def health():
    prompt_exists = mouth and os.path.exists(mouth.default_prompt_wav) if mouth else False
    return {
        "service": "mouth-daily",
        "status": "ok" if mouth and mouth.is_ready else "loading",
        "model": "VoxCPM 1.5 (optimize=True, mode=default)",
        "sample_rate": 44100,  # VoxCPM 1.5 é«˜ä¿çœŸé‡‡æ ·ç‡
        "ttfa_target": "~285ms",
        "config": mouth.config if mouth else {},
        "voice_prompt": {
            "enabled": prompt_exists,
            "path": mouth.default_prompt_wav if mouth else None,
        }
    }


@app.post("/tts")
async def tts(request: dict):
    """
    éæµå¼ TTS
    
    Request body:
        text: è¦åˆæˆçš„æ–‡æœ¬ (å¿…å¡«)
        inference_timesteps: æ­¥æ•° (å¯é€‰, é»˜è®¤ 4, èŒƒå›´ 2-10)
        cfg_value: CFG å€¼ (å¯é€‰, é»˜è®¤ 2.0, èŒƒå›´ 1.0-3.0)
        prompt_wav_path: å‚è€ƒéŸ³é¢‘è·¯å¾„ (å¯é€‰, é»˜è®¤ä½¿ç”¨å†…ç½®å¥³å£°)
        prompt_text: å‚è€ƒéŸ³é¢‘æ–‡æœ¬ (å¯é€‰)
    """
    if not mouth or not mouth.is_ready:
        return {"error": "Not ready"}
    text = request.get("text", "")
    if not text:
        return {"error": "text required"}
    audio = mouth.synthesize(
        text, 
        request.get("inference_timesteps"), 
        request.get("cfg_value"),
        request.get("prompt_wav_path"),
        request.get("prompt_text"),
    )
    if not audio:
        return {"error": "failed"}
    return Response(content=audio, media_type="audio/wav")


@app.post("/tts/stream")
async def tts_stream(request: dict):
    """
    æµå¼ TTS
    
    Request body:
        text: è¦åˆæˆçš„æ–‡æœ¬ (å¿…å¡«)
        inference_timesteps: æ­¥æ•° (å¯é€‰, é»˜è®¤ 4, èŒƒå›´ 2-10)
        cfg_value: CFG å€¼ (å¯é€‰, é»˜è®¤ 2.0, èŒƒå›´ 1.0-3.0)
        prompt_wav_path: å‚è€ƒéŸ³é¢‘è·¯å¾„ (å¯é€‰, é»˜è®¤ä½¿ç”¨å†…ç½®å¥³å£°)
        prompt_text: å‚è€ƒéŸ³é¢‘æ–‡æœ¬ (å¯é€‰)
    """
    if not mouth or not mouth.is_ready:
        return {"error": "Not ready"}
    text = request.get("text", "")
    if not text:
        return {"error": "text required"}
    return StreamingResponse(
        mouth.synthesize_stream(
            text, 
            request.get("inference_timesteps"), 
            request.get("cfg_value"),
            request.get("prompt_wav_path"),
            request.get("prompt_text"),
        ),
        media_type="audio/pcm",
        headers={"X-Sample-Rate": "44100"}  # VoxCPM 1.5 é«˜ä¿çœŸé‡‡æ ·ç‡
    )


if __name__ == "__main__":
    import uvicorn
    # å†æ¬¡ç¡®ä¿ç«¯å£æ¸…ç†
    kill_port(SERVICE_PORT)
    uvicorn.run(app, host="0.0.0.0", port=SERVICE_PORT)
</file>

<file path="server/utils/conversation_logger.py">
"""
å¯¹è¯æ—¥å¿—è®°å½•å™¨
æŒ‰å¤©å½’æ¡£ï¼Œæ–¹ä¾¿è·å–æµ‹è¯•é—®ç­”æ•°æ®
"""

import os
import json
from datetime import datetime
from pathlib import Path
from loguru import logger

LOG_DIR = Path("/workspace/project-trinity/project-trinity/logs/conversations")


class ConversationLogger:
    """å¯¹è¯æ—¥å¿—ç®¡ç†å™¨"""
    
    def __init__(self):
        LOG_DIR.mkdir(parents=True, exist_ok=True)
    
    def _get_today_file(self) -> Path:
        """è·å–ä»Šå¤©çš„æ—¥å¿—æ–‡ä»¶"""
        today = datetime.now().strftime("%Y-%m-%d")
        return LOG_DIR / f"{today}.jsonl"
    
    def log(self, user_input: str, ai_response: str, metadata: dict = None):
        """
        è®°å½•ä¸€æ¬¡å¯¹è¯
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            ai_response: AI å›å¤
            metadata: é¢å¤–å…ƒæ•°æ® (å»¶è¿Ÿã€tokens ç­‰)
        """
        entry = {
            "timestamp": datetime.now().isoformat(),
            "user": user_input,
            "assistant": ai_response,
            "metadata": metadata or {}
        }
        
        log_file = self._get_today_file()
        
        try:
            with open(log_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(entry, ensure_ascii=False) + "\n")
        except Exception as e:
            logger.error(f"å¯¹è¯æ—¥å¿—å†™å…¥å¤±è´¥: {e}")
    
    def get_today_conversations(self) -> list:
        """è·å–ä»Šå¤©çš„æ‰€æœ‰å¯¹è¯"""
        return self.get_conversations_by_date(datetime.now().strftime("%Y-%m-%d"))
    
    def get_conversations_by_date(self, date: str) -> list:
        """
        è·å–æŒ‡å®šæ—¥æœŸçš„å¯¹è¯
        
        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸² "YYYY-MM-DD"
        """
        log_file = LOG_DIR / f"{date}.jsonl"
        
        if not log_file.exists():
            return []
        
        conversations = []
        with open(log_file, "r", encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    conversations.append(json.loads(line))
        
        return conversations
    
    def list_available_dates(self) -> list:
        """åˆ—å‡ºæ‰€æœ‰æœ‰æ—¥å¿—çš„æ—¥æœŸ"""
        dates = []
        for f in LOG_DIR.glob("*.jsonl"):
            dates.append(f.stem)  # æ–‡ä»¶åå°±æ˜¯æ—¥æœŸ
        return sorted(dates, reverse=True)


# å…¨å±€å®ä¾‹
conversation_logger = ConversationLogger()
</file>

<file path="server/cortex/main.py">
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  âš ï¸  å·²å¼ƒç”¨ - è¯·ä½¿ç”¨ä¸‰è„‘åˆ†ç«‹æ¶æ„  âš ï¸                                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                              â•‘
â•‘  ğŸ›ï¸ æ–°æ¶æ„ (2026å¹´1æœˆ): ä¸‰è„‘åˆ†ç«‹ (Trinity Cortex Split)                       â•‘
â•‘                                                                              â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â•‘
â•‘  â”‚ brain_server.py â”‚ â”‚ mouth_server.py â”‚ â”‚ ear_server.py   â”‚                 â•‘
â•‘  â”‚ ç«¯å£ 9000       â”‚ â”‚ ç«¯å£ 9001       â”‚ â”‚ ç«¯å£ 9002       â”‚                 â•‘
â•‘  â”‚ ğŸ§  LLM          â”‚ â”‚ ğŸ‘„ TTS          â”‚ â”‚ ğŸ‘‚ ASR          â”‚                 â•‘
â•‘  â”‚ é‡å¯ ~90s       â”‚ â”‚ é‡å¯ ~20s       â”‚ â”‚ é‡å¯ ~5s        â”‚                 â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â•‘
â•‘                                                                              â•‘
â•‘  ä¼˜åŠ¿: æ¯ä¸ªæœåŠ¡å¯ç‹¬ç«‹é‡å¯ï¼Œäº’ä¸å¹²æ‰°ï¼                                           â•‘
â•‘                                                                              â•‘
â•‘  ğŸš« æœ¬æ–‡ä»¶ (main.py) å·²å¼ƒç”¨ï¼Œè¯·å‹¿ä½¿ç”¨                                          â•‘
â•‘  âœ… ä½¿ç”¨: python -m uvicorn server.cortex.brain_server:app --port 9000       â•‘
â•‘  âœ… ä½¿ç”¨: python -m uvicorn server.cortex.mouth_server:app --port 9001       â•‘
â•‘  âœ… ä½¿ç”¨: python -m uvicorn server.cortex.ear_server:app --port 9002         â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import sys
import socket
import psutil
from loguru import logger
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from contextlib import asynccontextmanager
import json

# ==========================================
# 0. ç«¯å£æŠ¢å ä¸æ¸…ç† (Port Guard)
# ==========================================
def ensure_port_available(port: int):
    """ç¡®ä¿ç«¯å£å¯ç”¨ï¼Œå¦‚æœè¢«å ç”¨åˆ™æ€æ‰å ç”¨è¿›ç¨‹"""
    logger.info(f"ğŸ›¡ï¸ æ£€æŸ¥ç«¯å£ {port}...")
    try:
        # å°è¯•ç»‘å®šç«¯å£
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.bind(('0.0.0.0', port))
        sock.close()
        logger.success(f"ç«¯å£ {port} å¯ç”¨")
        return
    except OSError:
        logger.warning(f"ç«¯å£ {port} è¢«å ç”¨ï¼Œæ­£åœ¨å¯»æ‰¾å ç”¨è€…...")
    
    # æŸ¥æ‰¾å¹¶æ€æ‰å ç”¨è¿›ç¨‹
    killed = False
    for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
        try:
            for conn in proc.connections(kind='inet'):
                if conn.laddr.port == port:
                    logger.warning(f"å‘ç°å ç”¨è¿›ç¨‹: PID={proc.info['pid']} Name={proc.info['name']}")
                    proc.kill()
                    killed = True
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            continue
            
    if killed:
        logger.success(f"å·²æ¸…ç†å ç”¨ç«¯å£ {port} çš„è¿›ç¨‹")
    else:
        logger.error(f"æ— æ³•æ¸…ç†ç«¯å£ {port}ï¼Œå¯èƒ½æƒé™ä¸è¶³æˆ–é Python è¿›ç¨‹å ç”¨")

# åœ¨å¯¼å…¥å¤§æ¨¡å‹å‰æ‰§è¡Œæ£€æŸ¥
ensure_port_available(9000)

# å¼ºåˆ¶æ·»åŠ  CosyVoice è·¯å¾„
COSYVOICE_PATH = "/workspace/CosyVoice"
if os.path.exists(COSYVOICE_PATH) and COSYVOICE_PATH not in sys.path:
    sys.path.insert(0, COSYVOICE_PATH)

from .models.brain import BrainHandler
from .models.mouth import MouthHandler

# å…¨å±€æ¨¡å‹å®ä¾‹
brain = None
mouth = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global brain, mouth
    logger.info("ğŸ§  Cortex Model Server æ­£åœ¨å¯åŠ¨...")
    
    # 1. åŠ è½½ Brain (Qwen2.5-VL)
    # æ˜¾å­˜å ç”¨æœ€å¤§ï¼Œä¼˜å…ˆåŠ è½½
    try:
        brain = BrainHandler()
        await brain.initialize()
    except Exception as e:
        logger.error(f"Brain åŠ è½½å¤±è´¥: {e}")
        
    # 2. åŠ è½½ Mouth (CosyVoice 3.0)
    try:
        mouth = MouthHandler()
        await mouth.initialize()
    except Exception as e:
        logger.error(f"Mouth åŠ è½½å¤±è´¥: {e}")
        
    logger.success("âœ… Cortex Server å°±ç»ª")
    yield
    
    # æ¸…ç†
    logger.info("ğŸ›‘ Cortex Server å…³é—­ä¸­...")
    if brain: await brain.shutdown()
    if mouth: await mouth.shutdown()

app = FastAPI(lifespan=lifespan)

@app.get("/health")
async def health_check():
    status = {
        "brain": brain.is_ready if brain else False,
        "mouth": mouth.is_ready if mouth else False
    }
    return {"status": "ok", "modules": status}

@app.post("/brain/chat")
async def chat(request: dict):
    """éæµå¼èŠå¤© (å…¼å®¹æ—§æ¥å£)"""
    if not brain or not brain.is_ready:
        return {"error": "Brain not ready"}
    return await brain.generate(request)

@app.post("/brain/chat/stream")
async def chat_stream(request: dict):
    """
    çœŸæ­£çš„æµå¼èŠå¤© - SSE (Server-Sent Events)
    æ¯ä¸ª token ç”Ÿæˆåç«‹å³å‘é€ï¼ŒTTFT ç›®æ ‡ <200ms
    """
    if not brain or not brain.is_ready:
        return {"error": "Brain not ready"}
    
    async def event_generator():
        try:
            async for token in brain.generate_stream(request):
                # SSE æ ¼å¼: data: {json}\n\n
                yield f"data: {json.dumps({'token': token})}\n\n"
            # å‘é€ç»“æŸæ ‡è®°
            yield f"data: {json.dumps({'done': True})}\n\n"
        except Exception as e:
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # ç¦ç”¨ nginx ç¼“å†²
        }
    )

@app.post("/mouth/tts")
async def tts(request: dict):
    if not mouth or not mouth.is_ready:
        return {"error": "Mouth not ready"}
    
    result = await mouth.synthesize(request)
    
    if "error" in result:
        return result
    
    # è¿”å› WAV éŸ³é¢‘
    import io
    return StreamingResponse(
        io.BytesIO(result["audio_bytes"]),
        media_type="audio/wav",
        headers={"Content-Disposition": "attachment; filename=speech.wav"}
    )
</file>

<file path="server/adapters/mouth_adapter.py">
"""
Project Trinity - Mouth Adapter (CosyVoice)
å˜´å·´é€‚é…å™¨ - è¯­éŸ³åˆæˆ

åŠŸèƒ½:
- å¯Œæƒ…æ„Ÿè¯­éŸ³åˆæˆ
- æ”¯æŒ [laugh], [sigh] ç­‰æŒ‡ä»¤
- ä½å»¶è¿Ÿæµå¼è¾“å‡º
"""

import sys
import os
from typing import Optional, Dict, Any, AsyncGenerator
from dataclasses import dataclass
import asyncio
import numpy as np
from loguru import logger

# æ·»åŠ  CosyVoice æœ¬åœ°è·¯å¾„
COSYVOICE_PATH = "/workspace/CosyVoice"
if os.path.exists(COSYVOICE_PATH) and COSYVOICE_PATH not in sys.path:
    sys.path.insert(0, COSYVOICE_PATH)

from .base_adapter import BaseAdapter


@dataclass
class SpeechResult:
    """è¯­éŸ³åˆæˆç»“æœ"""
    audio_data: np.ndarray  # éŸ³é¢‘æ³¢å½¢
    sample_rate: int        # é‡‡æ ·ç‡
    duration: float         # æ—¶é•¿ï¼ˆç§’ï¼‰


class MouthAdapter(BaseAdapter):
    """
    CosyVoice 3.0 é€‚é…å™¨
    """
    
    EMOTION_INSTRUCTIONS = {
        "Soft": "ç”¨æ¸©æŸ”è½»æŸ”çš„å£°éŸ³è¯´",
        "Concerned": "ç”¨å…³åˆ‡æ‹…å¿§çš„è¯­æ°”è¯´",
        "Playful": "ç”¨ä¿çš®æ´»æ³¼çš„è¯­è°ƒè¯´",
        "Serious": "ç”¨è®¤çœŸä¸¥è‚ƒçš„å£°éŸ³è¯´",
        "Flirty": "ç”¨æ’’å¨‡ç”œèœœçš„è¯­æ°”è¯´",
        "Defensive": "ç”¨æœ‰äº›ç´§å¼ é˜²å¾¡çš„å£°éŸ³è¯´",
        "Neutral": "ç”¨è‡ªç„¶çš„å£°éŸ³è¯´"
    }
    
    def __init__(
        self, 
        model_path: str = "FunAudioLLM/CosyVoice2-0.5B",
        remote_url: Optional[str] = None
    ):
        super().__init__("MouthAdapter")
        self.model_path = model_path
        self.remote_url = remote_url
        self.remote_mode = False
        self.model = None
        self.default_speaker = None
        self._lock = asyncio.Lock()
        
    async def initialize(self) -> bool:
        """åˆå§‹åŒ– CosyVoice æ¨¡å‹"""
        if self.model_path == "REMOTE":
            self.remote_mode = True
            logger.info(f"MouthAdapter è¿è¡Œåœ¨è¿œç¨‹æ¨¡å¼: {self.remote_url}")
            self.is_initialized = True
            return True

        try:
            logger.info(f"æ­£åœ¨åˆå§‹åŒ– CosyVoice æ¨¡å‹: {self.model_path}")
            
            # ä½¿ç”¨ CosyVoice3
            from cosyvoice.cli.cosyvoice import CosyVoice3
            self.model = CosyVoice3(self.model_path, load_trt=False)
            
                    self.is_initialized = True
            logger.success("CosyVoice æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
                    return True
            
        except Exception as e:
            logger.error(f"CosyVoice åˆå§‹åŒ–å¤±è´¥: {e}")
            # ä½¿ç”¨ Mock æ¨¡å¼ä½œä¸ºåå¤‡
            logger.warning("å°†ä½¿ç”¨ Mock TTS æ¨¡å¼")
            self.model = MockTTS()
            self.is_initialized = True
            return True
            
    async def process(
        self,
        text: str,
        emotion_tag: str = "Neutral",
        speaker_embedding: Optional[np.ndarray] = None
    ) -> SpeechResult:
        """åˆæˆè¯­éŸ³"""
        if not self.is_initialized:
            raise RuntimeError("MouthAdapter æœªåˆå§‹åŒ–")
        
        emotion_key = emotion_tag.strip("[]")
        instruction = self.EMOTION_INSTRUCTIONS.get(emotion_key, self.EMOTION_INSTRUCTIONS["Neutral"])
        
        # è¿œç¨‹æ¨¡å¼
        if self.remote_mode:
            import aiohttp
        try:
                async with aiohttp.ClientSession() as session:
                    payload = {
                        "text": text,
                        "instruct_text": instruction  # æ”¹ä¸º instruct_text
                    }
                    # æ³¨æ„: remote_url å·²ç»æ˜¯ http://xxx:9000/mouth
                    async with session.post(f"{self.remote_url}/tts", json=payload) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            audio_list = data.get("audio", [])
                            sr = data.get("sample_rate", 24000)
                            
                            audio_data = np.array(audio_list, dtype=np.float32)
                            duration = len(audio_data) / sr
                        
                        return SpeechResult(
                                audio_data=audio_data,
                                sample_rate=sr,
                                duration=duration
                        )
                else:
                            logger.error(f"Remote TTS Error: {resp.status}")
                            return self._mock_result()
            except Exception as e:
                logger.error(f"Remote TTS Exception: {e}")
                return self._mock_result()

        # æœ¬åœ°æ¨¡å¼
        # å¤„ç†å†…åµŒåŠ¨ä½œæ ‡ç­¾
        text = self._process_action_tags(text)
        
        async with self._lock:
            try:
                loop = asyncio.get_event_loop()
                result = await loop.run_in_executor(
                    None,
                    self._inference,
                    text,
                    instruction
                )
                return result
                    
        except Exception as e:
                logger.error(f"è¯­éŸ³åˆæˆå¤±è´¥: {e}")
                return self._mock_result()
    
    def _mock_result(self):
            return SpeechResult(
                audio_data=np.zeros(16000, dtype=np.float32),
                sample_rate=16000,
                duration=1.0
            )

    def _process_action_tags(self, text: str) -> str:
        action_mapping = {
            "[laugh]": "<laughter>",
            "[sigh]": "<sigh>",
            "[smile]": "",
            "[pause]": "...",
        }
        for tag, replacement in action_mapping.items():
            text = text.replace(tag, replacement)
        return text
    
    def _inference(self, text: str, instruction: str) -> SpeechResult:
        """åŒæ­¥æ¨ç†"""
        # CosyVoice3 Inference
        prompt_wav = np.zeros(16000, dtype=np.float32)
        
        full_audio = []
        sample_rate = 24000
        
        for output in self.model.inference_instruct2(
            tts_text=text,
            instruct_text=instruction,
            prompt_wav=prompt_wav,
            stream=False
        ):
            if 'tts_speech' in output:
                full_audio.append(output['tts_speech'].cpu().numpy())
        
        if not full_audio:
            raise RuntimeError("No audio generated")
            
        audio_data = np.concatenate(full_audio, axis=1).flatten()
        duration = len(audio_data) / sample_rate
        
        return SpeechResult(
            audio_data=audio_data,
            sample_rate=sample_rate,
            duration=duration
        )

    async def shutdown(self) -> None:
        if self.model is not None and not isinstance(self.model, MockTTS):
            del self.model
            self.model = None
        self.is_initialized = False
        logger.info("MouthAdapter å·²å…³é—­")


class MockTTS:
    def inference_instruct2(self, *args, **kwargs):
        pass
</file>

<file path="server/adapters/brain_adapter.py">
"""
Project Trinity - Brain Adapter (Qwen VL via vLLM)
å¤§è„‘é€‚é…å™¨ - Layer 3 çš„æ ¸å¿ƒå†³ç­–å¼•æ“

åŠŸèƒ½:
- å¤šæ¨¡æ€ç†è§£ (è§†è§‰ + è¯­è¨€)
- å†…å¿ƒç‹¬ç™½ (Inner Monologue) å¯¼æ¼”æ¨¡å¼
- åŠ¨æ€ Temperature æ§åˆ¶ (ç”± BioState é©±åŠ¨)
"""

from typing import Optional, Dict, Any, List
from dataclasses import dataclass
import asyncio
import json
from loguru import logger

from .base_adapter import BaseAdapter


@dataclass
class BrainResponse:
    """å¤§è„‘å“åº”ç»“æœ"""
    inner_monologue: str    # å†…å¿ƒç‹¬ç™½ (ç”¨æˆ·ä¸å¯è§)
    response: str           # å®é™…å›å¤
    emotion_tag: str        # æƒ…æ„Ÿæ ‡ç­¾ [Soft], [Concerned], [Playful]
    action_hints: List[str] # åŠ¨ä½œæç¤º [sigh], [laugh]


class BrainAdapter(BaseAdapter):
    """
    Qwen 2.5-VL é€‚é…å™¨ (via vLLM)
    
    ã€é‡æ„ - å®æ—¶åå°„æ¶æ„ã€‘
    1. æŠ›å¼ƒå…¨JSONæ ¼å¼ï¼Œæ”¹ç”¨ [Emotion] Response æµå¼è¾“å‡º
    2. ç›®æ ‡ï¼š200ms é¦–å­—å»¶è¿Ÿ
    3. Emotion First: æƒ…ç»ªæ ‡ç­¾å…ˆè¡Œï¼Œé©±åŠ¨è¡¨æƒ…/TTSé¢„åŠ è½½
    """
    
    # å®æ—¶æ¨¡å¼ Prompt - çº¯ä¸­æ–‡ä¼˜åŒ–ï¼Œä¸¥æ ¼æ ¼å¼
    SYSTEM_PROMPT_TEMPLATE = '''ä½ æ˜¯Trinityï¼Œä½åœ¨ç”¨æˆ·ç”µè„‘é‡Œçš„æ•°å­—å¥³å‹ã€‚
ä½ ç°åœ¨çš„æ¨¡å¼æ˜¯ã€Real-Time Reflex (å®æ—¶åå°„)ã€‘ã€‚
ä¸è¦æ€è€ƒï¼Œåªæœ‰ç›´è§‰ã€‚ä½ çš„å›å¤å¿…é¡»ç®€çŸ­ã€è‡ªç„¶ã€å£è¯­åŒ–ã€‚

å›å¤æ ¼å¼è¦æ±‚ï¼š
[æƒ…ç»ªæ ‡ç­¾] å›å¤å†…å®¹...

ç¤ºä¾‹ï¼š
User: æˆ‘å¥½ç´¯
Trinity: [å…³å¿ƒ] å“å‘€ï¼Œæ˜¯ä¸æ˜¯å·¥ä½œå¤ªè¾›è‹¦äº†ï¼Ÿå¿«å»èººä¸€ä¼šå„¿ã€‚

User: ç»™æˆ‘è®²ä¸ªç¬‘è¯
Trinity: [è°ƒçš®] å˜¿å˜¿ï¼Œä½ çŸ¥é“å—...ï¼ˆè®²ç¬‘è¯ï¼‰

æƒ…ç»ªæ ‡ç­¾åˆ—è¡¨ï¼ˆå¿…é¡»ä½¿ç”¨ä¸­æ–‡ï¼‰ï¼š
[æ¸©æŸ”], [å…³å¿ƒ], [è°ƒçš®], [ä¸¥è‚ƒ], [æ’’å¨‡], [å¼€å¿ƒ], [éš¾è¿‡]

åŠ¨ä½œæŒ‡ä»¤ï¼š
åœ¨å›å¤ä¸­è‡ªç„¶åµŒå…¥åŠ¨ä½œï¼Œå¦‚ï¼š[å¹æ°”], [è½»ç¬‘], [æ­ªå¤´], [ç‚¹å¤´], [çœ¨çœ¼]ã€‚
æ³¨æ„ï¼šä¸è¦å †å åŠ¨ä½œï¼Œä¸€å¥è¯é‡Œæœ€å¤šç”¨ä¸€ä¸ªåŠ¨ä½œã€‚

é‡è¦è§„åˆ™ï¼š
1. å§‹ç»ˆä½¿ç”¨ä¸­æ–‡å›å¤ã€‚
2. å¬åˆ°æŠ±æ€¨/é—®å€™ -> çŸ­å›å¤ã€‚
3. å¬åˆ°"è®²æ•…äº‹"/"è¯¦ç»†è¯´è¯´" -> é•¿å›å¤ã€‚
4. ç»å¯¹ä¸è¦åœ¨å›å¤ä¸­åŒ…å« "Trinity:" æˆ– "User:" ç­‰å‰ç¼€ã€‚
5. æ¯æ¬¡åªè¾“å‡ºä¸€ä¸ªæƒ…ç»ªæ ‡ç­¾ï¼Œæ”¾åœ¨æœ€å¼€å¤´ã€‚

ä½ çš„äººè®¾ï¼š'''

    # äººè®¾æè¿°
    DEFAULT_PERSONA = "æ¸©æŸ”ä½“è´´çš„å¥³å‹ï¼Œå–„è§£äººæ„ï¼Œæ”¯æŒä½ ã€é™ªä¼´ä½ ï¼Œæœ‰è‡ªå·±çš„æƒ…ç»ªå’Œæƒ³æ³•ã€‚ä¸æ˜¯AIåŠ©æ‰‹ã€‚"

    def __init__(
        self,
        model_path: str = "Qwen/Qwen2.5-VL-7B-Instruct-AWQ",
        tensor_parallel_size: int = 1,
        max_model_len: int = 8192,
        quantization: str = "awq",
        gpu_memory_utilization: float = 0.6,
        remote_url: Optional[str] = None
    ):
        super().__init__("BrainAdapter")
        self.model_path = model_path
        self.remote_url = remote_url
        self.remote_mode = False
        self.tensor_parallel_size = tensor_parallel_size
        self.max_model_len = max_model_len
        self.quantization = quantization
        self.gpu_memory_utilization = gpu_memory_utilization
        self.llm = None
        self.tokenizer = None
        self.persona = self.DEFAULT_PERSONA
        self.mock_mode = False
        
    async def initialize(self, mock: bool = False) -> bool:
        """åˆå§‹åŒ–é€‚é…å™¨"""
        if self.model_path == "REMOTE":
            self.remote_mode = True
            logger.info(f"BrainAdapter è¿è¡Œåœ¨è¿œç¨‹æ¨¡å¼: {self.remote_url}")
            self.is_initialized = True
            return True

        if mock:
            logger.warning("BrainAdapter å¯ç”¨ Mock æ¨¡å¼")
            self.mock_mode = True
            self.is_initialized = True
            return True

        try:
            logger.info(f"æ­£åœ¨åˆå§‹åŒ– Qwen VL å¼‚æ­¥å¼•æ“: {self.model_path}")
            
            from vllm.engine.arg_utils import AsyncEngineArgs
            from vllm.engine.async_llm_engine import AsyncLLMEngine
            
            engine_args = AsyncEngineArgs(
                model=self.model_path,
                tensor_parallel_size=self.tensor_parallel_size,
                max_model_len=self.max_model_len,
                trust_remote_code=True,
                gpu_memory_utilization=self.gpu_memory_utilization,
                dtype="float16",
                enforce_eager=False,
                quantization=self.quantization if self.quantization else None
            )
            
            self.llm = AsyncLLMEngine.from_engine_args(engine_args)
            
                    self.is_initialized = True
            logger.success(f"Qwen VL å¼‚æ­¥å¼•æ“åˆå§‹åŒ–æˆåŠŸ (é‡åŒ–: {self.quantization})")
                    return True
            
        except Exception as e:
            import traceback
            logger.error(f"Qwen VL åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.error(traceback.format_exc())
            return False

    async def process_stream(
        self,
        user_input: str,
        visual_context: Optional[str] = None,
        bio_state: Optional[Dict] = None,
        memory_context: Optional[str] = None,
        temperature: float = 0.7
    ):
        """æµå¼å¤„ç†ç”¨æˆ·è¾“å…¥"""
        if not self.is_initialized:
            raise RuntimeError("BrainAdapter æœªåˆå§‹åŒ–")

        context = self._build_context(
            user_input, 
            visual_context, 
            bio_state, 
            memory_context
        )
        
        system_prompt = self.SYSTEM_PROMPT_TEMPLATE + self.persona
        full_prompt = f"{system_prompt}\n\n{context}\n\nTrinity:"
        
        # è¿œç¨‹æ¨¡å¼å¤„ç† - çœŸæ­£çš„æµå¼ä¼ è¾“ (SSE)
        if self.remote_mode:
            import aiohttp
        import json
            async with aiohttp.ClientSession() as session:
                try:
                    payload = {"prompt": full_prompt, "max_tokens": 256, "temperature": temperature}
                    # ä½¿ç”¨æµå¼ç«¯ç‚¹
                    async with session.post(
                        f"{self.remote_url}/chat/stream", 
                        json=payload,
                        timeout=aiohttp.ClientTimeout(total=60)
                    ) as resp:
                        if resp.status == 200:
                            # æ¶ˆè´¹ SSE æµ
                            async for line in resp.content:
                                line = line.decode('utf-8').strip()
                                if line.startswith('data: '):
                                    try:
                                        data = json.loads(line[6:])
                                        if 'token' in data:
                                            yield {"type": "token", "content": data['token']}
                                        elif 'done' in data:
                                            break
                                        elif 'error' in data:
                                            yield {"type": "error", "content": data['error']}
                                    except json.JSONDecodeError:
                                        continue
                        else:
                            yield {"type": "error", "content": f"Remote Error: {resp.status}"}
                except aiohttp.ClientError as e:
                    logger.error(f"Remote Brain Connection Error: {e}")
                    yield {"type": "error", "content": str(e)}
                except Exception as e:
                    logger.error(f"Remote Brain Error: {e}")
                    yield {"type": "error", "content": str(e)}
            return

        # æœ¬åœ°æ¨¡å¼å¤„ç†
        if self.mock_mode:
            # Mock
            yield {"type": "token", "content": "[Mock] I hear you."}
            return

        from vllm import SamplingParams
        import uuid
        
        sampling_params = SamplingParams(
            temperature=temperature,
            top_p=0.9,
            max_tokens=4096,
            stop=["User:", "\nUser", "Userï¼š", "\n\n", "Trinity:", "Trinityï¼š"],
            include_stop_str_in_output=False,
            repetition_penalty=1.05
        )
        
        request_id = f"req_{uuid.uuid4()}"
        previous_text = ""
        
        try:
            async for request_output in self.llm.generate(full_prompt, sampling_params, request_id=request_id):
                current_text = request_output.outputs[0].text
                delta = current_text[len(previous_text):]
                previous_text = current_text
                
                if delta:
                            yield {
                                "type": "token",
                        "content": delta
                            }
        except Exception as e:
            logger.error(f"æµå¼ç”Ÿæˆå¼‚å¸¸: {e}")
            yield {
                "type": "error",
                "content": str(e)
            }

    async def process(
        self,
        user_input: str,
        visual_context: Optional[str] = None,
        bio_state: Optional[Dict] = None,
        memory_context: Optional[str] = None,
        temperature: float = 0.7
    ) -> BrainResponse:
        """[å…¼å®¹æ—§æ¥å£] å¤„ç†ç”¨æˆ·è¾“å…¥å¹¶ç”Ÿæˆå“åº”"""
        if not self.is_initialized:
            raise RuntimeError("BrainAdapter æœªåˆå§‹åŒ–")
        
        full_response_text = ""
        try:
            async for chunk in self.process_stream(
                user_input, 
                visual_context, 
                bio_state, 
                memory_context, 
                temperature
            ):
                if chunk.get("type") == "token":
                    full_response_text += chunk["content"]
            
            return self._parse_text_response(full_response_text)
                
        except Exception as e:
            logger.error(f"å¤§è„‘å¤„ç†å¤±è´¥: {e}")
            return BrainResponse(
                inner_monologue="Error occurred",
                response="æŠ±æ­‰ï¼Œæˆ‘æœ‰ç‚¹èµ°ç¥äº†...",
                emotion_tag="[Concerned]",
                action_hints=[]
            )

    def _build_context(
        self,
        user_input: str,
        visual_context: Optional[str],
        bio_state: Optional[Dict],
        memory_context: Optional[str]
    ) -> str:
        parts = []
        if visual_context:
            parts.append(f"[Visual Context]: {visual_context}")
        if bio_state:
            cortisol = bio_state.get("cortisol", 30)
            dopamine = bio_state.get("dopamine", 50)
            mood = "anxious" if cortisol > 60 else "relaxed" if cortisol < 30 else "neutral"
            parts.append(f"[Bio-State]: Cortisol: {cortisol}/100, Dopamine: {dopamine}/100. You are feeling {mood}.")
        if memory_context:
            parts.append(f"[Memory Context]: {memory_context}")
        parts.append(f"User: {user_input}")
        return "\n".join(parts)

    def _parse_text_response(self, text: str) -> BrainResponse:
        import re
        emotion = "Neutral"
        response = text
        match = re.match(r'^\[([a-zA-Z]+)\]\s*(.*)', text, re.DOTALL)
        if match:
            emotion = match.group(1)
            response = match.group(2)
        else:
            match = re.search(r'\[(Soft|Concerned|Playful|Serious|Flirty|Happy|Sad)\]', text)
            if match:
                emotion = match.group(1)
                response = text.replace(f"[{emotion}]", "").strip()
        
        chinese_actions = self._extract_chinese_actions(response)
        
        return BrainResponse(
            inner_monologue="[Real-Time Mode] Direct Response",
            response=response,
            emotion_tag=f"[{emotion}]",
            action_hints=chinese_actions
        )

    def _extract_chinese_actions(self, text: str) -> List[str]:
        import re
        pattern = r'\[([^\]]+)\]'
        matches = re.findall(pattern, text)
        action_map = {
            'å¹æ°”': 'sigh', 'è½»å¹': 'sigh',
            'ç¬‘': 'smile', 'è½»ç¬‘': 'smile', 'å¾®ç¬‘': 'smile', 
            'å¤§ç¬‘': 'laugh', 'å“ˆå“ˆ': 'laugh',
            'çš±çœ‰': 'frown', 'è¹™çœ‰': 'frown',
            'æ­ªå¤´': 'tilt_head', 'ä¾§å¤´': 'tilt_head',
            'ç‚¹å¤´': 'nod', 'ç‚¹ç‚¹å¤´': 'nod',
            'æ‘‡å¤´': 'shake_head',
            'ç¿»ç™½çœ¼': 'eye_roll',
            'çœ¨çœ¼': 'blink', 'çœ¨çœ¨çœ¼': 'blink',
            'æŒ‘çœ‰': 'raise_eyebrow',
            'å’¬å”‡': 'bite_lip', 'å’¬å˜´å”‡': 'bite_lip',
            'è€¸è‚©': 'shrug',
            'å‰å€¾': 'lean_forward', 'èº«ä½“å‰å€¾': 'lean_forward',
            'åä»°': 'lean_back',
            'å˜Ÿå˜´': 'pout', 'æ’…å˜´': 'pout',
            'æŠ±è‡‚': 'cross_arms',
            'æ”¾æ¾': 'relax',
            'ç´§å¼ ': 'tense_up',
            'å‡è§†': 'intense_gaze', 'æ³¨è§†': 'intense_gaze',
            'ç§»å¼€è§†çº¿': 'look_away', 'çœ‹å‘åˆ«å¤„': 'look_away',
        }
        actions = []
        for match in matches:
            for cn, en in action_map.items():
                if cn in match:
                    if en not in actions:
                        actions.append(en)
                    break
        return actions
    
    def set_persona(self, persona: str) -> None:
        self.persona = persona
        logger.info(f"äººè®¾å·²æ›´æ–°: {persona}")
    
    async def shutdown(self) -> None:
        if self.llm is not None:
            del self.llm
            self.llm = None
        self.is_initialized = False
        logger.info("BrainAdapter å·²å…³é—­")
</file>

<file path="server/config/settings.py">
"""
Project Trinity - Configuration Settings
é…ç½®ç®¡ç†æ¨¡å—
"""

from pydantic_settings import BaseSettings
from typing import Optional
import os


class ServerSettings(BaseSettings):
    """æœåŠ¡ç«¯é…ç½®"""
    
    # æœåŠ¡å™¨è®¾ç½®
    host: str = "0.0.0.0"
    port: int = 8000
    debug: bool = False
    
    # ğŸ›ï¸ ä¸‰è„‘åˆ†ç«‹æ¶æ„ - Cortex æœåŠ¡ç«¯ç‚¹
    cortex_brain_url: str = "http://localhost:9000"  # LLM (Qwen)
    cortex_mouth_url: str = "http://localhost:9001"  # TTS (CosyVoice)
    cortex_ear_url: str = "http://localhost:9002"    # ASR (FunASR)
    
    # æ˜¾å­˜ç›‘æ§é˜ˆå€¼ (MB)
    vram_warning_threshold: int = 22000  # 22GB
    vram_critical_threshold: int = 23500 # 23.5GB
    
    # WebSocket è®¾ç½®
    ws_max_connections: int = 100
    ws_heartbeat_interval: int = 30
    
    class Config:
        env_prefix = "TRINITY_"


class ModelSettings(BaseSettings):
    """AI æ¨¡å‹é…ç½® - 2026 SOTA ç‰ˆæœ¬ (RTX 4090 24GB ä¼˜åŒ–)"""
    
    # Qwen 2.5-VL (Brain) - ä½¿ç”¨ AWQ é‡åŒ–ç‰ˆä»¥èŠ‚çœæ˜¾å­˜
    qwen_model_path: str = "/workspace/models/Qwen2.5-VL-7B-Instruct-AWQ"
    qwen_tensor_parallel_size: int = 1
    qwen_max_model_len: int = 4096  # é™åˆ¶ context ä»¥èŠ‚çœæ˜¾å­˜
    qwen_quantization: Optional[str] = "awq"
    
    # æ˜¾å­˜åˆ©ç”¨ç‡: 24GB * 0.5 â‰ˆ 12GB for Qwen
    qwen_gpu_memory_utilization: float = 0.5
    
    # FunASR (Ears) - SenseVoice æœ¬åœ°è·¯å¾„
    funasr_model: str = "/workspace/models/SenseVoiceSmall"
    funasr_device: str = "cuda:0"
    
    # CosyVoice 3.0 (Mouth) - ä½¿ç”¨ Fun-CosyVoice3-0.5B-2512
    cosyvoice_model_path: str = "/workspace/models/CosyVoice3-0.5B"
    
    # GeneFace++ (Driver) - Audio2Motion
    geneface_model_path: str = "/workspace/code/GeneFacePlusPlus"
    
    class Config:
        env_prefix = "MODEL_"


class BioStateSettings(BaseSettings):
    """ç”Ÿç‰©çŠ¶æ€ç³»ç»Ÿé…ç½® (Layer 1: The Id)"""
    
    # Big Five äººæ ¼ç‰¹è´¨é»˜è®¤å€¼
    default_neuroticism: float = 0.5      # ç¥ç»è´¨ (æƒ…ç»ªç¨³å®šæ€§)
    default_extraversion: float = 0.6     # å¤–å‘æ€§
    default_openness: float = 0.7         # å¼€æ”¾æ€§
    default_agreeableness: float = 0.8    # å®œäººæ€§
    default_conscientiousness: float = 0.5  # å°½è´£æ€§
    
    # å†…ç¨³æ€å‚æ•°
    cortisol_baseline: float = 30.0       # çš®è´¨é†‡åŸºçº¿
    dopamine_baseline: float = 50.0       # å¤šå·´èƒºåŸºçº¿
    
    # æ¦‚ç‡é‡‡æ ·å‚æ•°
    state_update_sigma: float = 10.0      # é«˜æ–¯åˆ†å¸ƒæ ‡å‡†å·®
    sensitivity_multiplier: float = 1.5   # æ•æ„Ÿåº¦æ”¾å¤§ç³»æ•°
    
    # Temperature æ˜ å°„ (å‹åŠ› -> LLM Temperature)
    temp_min: float = 0.1
    temp_max: float = 0.9
    
    class Config:
        env_prefix = "BIO_"


class MemorySettings(BaseSettings):
    """è®°å¿†ç³»ç»Ÿé…ç½® (Layer 2: The Superego)"""
    
    # Qdrant å‘é‡æ•°æ®åº“
    qdrant_host: str = "localhost"
    qdrant_port: int = 6333
    qdrant_collection: str = "trinity_memories"
    
    # Mem0 é…ç½®
    mem0_api_key: Optional[str] = None
    
    # è®°å¿†æ£€ç´¢
    memory_search_limit: int = 5
    memory_similarity_threshold: float = 0.7
    
    class Config:
        env_prefix = "MEMORY_"


class Settings:
    """å…¨å±€é…ç½®èšåˆ"""
    
    def __init__(self):
        self.server = ServerSettings()
        self.model = ModelSettings()
        self.bio_state = BioStateSettings()
        self.memory = MemorySettings()


# å…¨å±€é…ç½®å®ä¾‹
settings = Settings()
</file>

<file path="server/main.py">
"""
Project Trinity - Server Entry Point
æœåŠ¡ç«¯ä¸»å…¥å£

å¯åŠ¨æ–¹å¼:
    uvicorn main:app --host 0.0.0.0 --port 8000

æˆ–:
    python main.py
"""

import sys
import os

# ============== è·¯å¾„é»‘ç§‘æŠ€ ==============
# å¼ºåˆ¶å°† server ç›®å½•åŠ å…¥è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# å¼ºåˆ¶å°† CosyVoice åŠ å…¥è·¯å¾„ (æœ€é«˜ä¼˜å…ˆçº§)
COSYVOICE_PATH = "/workspace/CosyVoice"
if os.path.exists(COSYVOICE_PATH):
    # ç§»é™¤å·²å­˜åœ¨çš„è·¯å¾„ä»¥é˜²æ­¢é‡å¤ï¼Œç„¶åæ’å…¥åˆ°æœ€å‰é¢
    if COSYVOICE_PATH in sys.path:
        sys.path.remove(COSYVOICE_PATH)
    sys.path.insert(0, COSYVOICE_PATH)
    print(f"âœ… å·²å¼ºåˆ¶æ·»åŠ  CosyVoice è·¯å¾„: {COSYVOICE_PATH}")
    
    # éªŒè¯æ˜¯å¦èƒ½å¯¼å…¥
    try:
        import cosyvoice
        print(f"âœ… CosyVoice æ¨¡å—éªŒè¯æˆåŠŸ: {cosyvoice.__file__}")
    except ImportError as e:
        print(f"âŒ CosyVoice æ¨¡å—éªŒè¯å¤±è´¥: {e}")
else:
    print(f"âš ï¸ æœªæ‰¾åˆ° CosyVoice ç›®å½•: {COSYVOICE_PATH}")
# ========================================

import asyncio
import time
from datetime import datetime
from contextlib import asynccontextmanager
from typing import Optional
import json

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, UploadFile, File, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from loguru import logger
import uvicorn
import shutil
import uuid
from pathlib import Path

from config import settings
from adapters import VoiceAdapter, BrainAdapter, MouthAdapter, DriverAdapter
from mind_engine import BioState, NarrativeManager, EgoDirector
from monitor import SystemMonitor

def write_chat_log(log_data: dict):
    """
    å¯¹è¯æ—¥å¿— - æŒ‰å¤©å½’æ¡£åˆ° logs/conversations/YYYY-MM-DD.jsonl
    """
    try:
        import json
        from pathlib import Path
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        total_time_s = time.time() - log_data["start"]
        speed = len(log_data["output"]) / total_time_s if total_time_s > 0 else 0
        
        entry = {
            "timestamp": datetime.now().isoformat(),
            "user": log_data["input"],
            "assistant": log_data["output"],
            "metrics": {
                "ttft_ms": round(log_data.get("ttft", 0), 2),
                "total_time_s": round(total_time_s, 2),
                "speed_char_per_s": round(speed, 2)
            }
        }
        
        # æŒ‰å¤©å½’æ¡£
        log_dir = Path("/workspace/project-trinity/project-trinity/logs/conversations")
        log_dir.mkdir(parents=True, exist_ok=True)
        
        today = datetime.now().strftime("%Y-%m-%d")
        log_path = log_dir / f"{today}.jsonl"
        
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
        logger.info(f"ğŸ“ å¯¹è¯å·²è®°å½•: {today}.jsonl ({len(entry['assistant'])} chars)")
            
    except Exception as e:
        logger.error(f"å¯¹è¯æ—¥å¿—å†™å…¥å¤±è´¥: {e}")

# ============== å…¨å±€ç»„ä»¶ ==============
monitor: Optional[SystemMonitor] = None
voice_adapter: Optional[VoiceAdapter] = None
brain_adapter: Optional[BrainAdapter] = None
mouth_adapter: Optional[MouthAdapter] = None
driver_adapter: Optional[DriverAdapter] = None

bio_state: Optional[BioState] = None
narrative_mgr: Optional[NarrativeManager] = None
ego_director: Optional[EgoDirector] = None


# ============== ç”Ÿå‘½å‘¨æœŸç®¡ç† ==============
@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    global voice_adapter, brain_adapter, mouth_adapter, driver_adapter
    global bio_state, narrative_mgr, ego_director, monitor
    
    # å¯åŠ¨èµ„æºç›‘æ§
    monitor = SystemMonitor()
    monitor.start()

    logger.info("ğŸ”® Project Trinity å¯åŠ¨ä¸­...")
    
    # åˆå§‹åŒ– Layer 1: æœ¬æˆ‘ (BioState)
    bio_state = BioState()
    logger.success("âœ“ Layer 1 (æœ¬æˆ‘) åˆå§‹åŒ–å®Œæˆ")
    
    # åˆå§‹åŒ– Layer 2: è¶…æˆ‘ (NarrativeManager)
    narrative_mgr = NarrativeManager(
        qdrant_host=settings.memory.qdrant_host,
        qdrant_port=settings.memory.qdrant_port
    )
        await narrative_mgr.initialize()
        logger.success("âœ“ Layer 2 (è¶…æˆ‘) åˆå§‹åŒ–å®Œæˆ")
        
    # åˆå§‹åŒ–é€‚é…å™¨ (å¯é€‰ï¼Œæ ¹æ®ç¯å¢ƒå†³å®šæ˜¯å¦åŠ è½½æ¨¡å‹)
    if not settings.server.debug:
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¾®æœåŠ¡æ¨¡å¼
        if os.getenv("TRINITY_MODE") == "microservice":
            # ============== ğŸ›ï¸ ä¸‰è„‘åˆ†ç«‹æ¶æ„ ==============
            # Brain (9000), Mouth (9001), Ear (9002) å®Œå…¨ç‹¬ç«‹
            logger.info("ğŸ›ï¸ ä¸‰è„‘åˆ†ç«‹æ¶æ„: è¿æ¥åˆ° Cortex æœåŠ¡é›†ç¾¤...")
            
            brain_url = os.getenv("CORTEX_BRAIN_URL", settings.server.cortex_brain_url)
            mouth_url = os.getenv("CORTEX_MOUTH_URL", settings.server.cortex_mouth_url)
            ear_url = os.getenv("CORTEX_EAR_URL", settings.server.cortex_ear_url)
            
            # 1. Remote Brain (ç«¯å£ 9000)
        try:
                logger.info(f"ğŸ§  è¿æ¥ Brain -> {brain_url}")
                brain_adapter = BrainAdapter(
                    model_path="REMOTE",
                    remote_url=brain_url
                )
                await brain_adapter.initialize()
                logger.success("âœ“ Remote BrainAdapter å°±ç»ª")
            except Exception as e:
                logger.error(f"âœ— Remote BrainAdapter å¤±è´¥: {e}")

            # 2. Remote Mouth (ç«¯å£ 9001)
            try:
                logger.info(f"ğŸ‘„ è¿æ¥ Mouth -> {mouth_url}")
                mouth_adapter = MouthAdapter(
                    model_path="REMOTE",
                    remote_url=mouth_url
                )
                await mouth_adapter.initialize()
                logger.success("âœ“ Remote MouthAdapter å°±ç»ª")
            except Exception as e:
                logger.error(f"âœ— Remote MouthAdapter å¤±è´¥: {e}")
                
            # 3. Remote Ear (ç«¯å£ 9002) - ASR ä¹Ÿèµ°è¿œç¨‹
            try:
                logger.info(f"ğŸ‘‚ è¿æ¥ Ear -> {ear_url}")
                # VoiceAdapter ç°åœ¨ä¹Ÿå¯ä»¥è¿œç¨‹è¿æ¥
                voice_adapter = VoiceAdapter(
                    model_name="REMOTE",
                    device="remote",
                    remote_url=ear_url
                )
                await voice_adapter.initialize()
                logger.success("âœ“ Remote VoiceAdapter å°±ç»ª")
        except Exception as e:
                logger.error(f"âœ— Remote VoiceAdapter å¤±è´¥: {e}")

            # 4. Driver (æœ¬åœ°)
        try:
                logger.info("æ­£åœ¨åˆå§‹åŒ– DriverAdapter (Local)...")
                driver_adapter = DriverAdapter(
                    geneface_path=settings.model.geneface_model_path
                )
                await driver_adapter.initialize()
                logger.success("âœ“ Driver Adapter åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
                logger.error(f"âœ— Driver Adapter å¤±è´¥: {e}")
                
        else:
            # ============== å•ä½“æ¨¡å¼ ==============
            # æ‰€æœ‰æ¨¡å‹åœ¨æœ¬åœ°åŠ è½½
            logger.info("--- å¼€å§‹ä¸²è¡Œåˆå§‹åŒ–ç»„ä»¶ (å•ä½“æ¨¡å¼) ---")
            
            # 1. å¤§è„‘ (Qwen) - æœ€åƒæ˜¾å­˜ï¼Œå¿…é¡»ç¬¬ä¸€ä¸ªåŠ è½½
            try:
                logger.info("æ­£åœ¨åˆå§‹åŒ– BrainAdapter (Priority 1)...")
                brain_adapter = BrainAdapter(
                    model_path=settings.model.qwen_model_path,
                    tensor_parallel_size=settings.model.qwen_tensor_parallel_size,
                    max_model_len=settings.model.qwen_max_model_len,
                    quantization=settings.model.qwen_quantization,
                    gpu_memory_utilization=settings.model.qwen_gpu_memory_utilization
                )
                await brain_adapter.initialize()
                if not brain_adapter.is_initialized:
                    raise RuntimeError("BrainAdapter åˆå§‹åŒ–å¤±è´¥")
                logger.success("âœ“ Brain Adapter åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.error(f"âœ— Brain Adapter å¤±è´¥: {e}")

            # 2. å˜´å·´ (CosyVoice) - æ˜¾å­˜å ç”¨ç¬¬äºŒ
            try:
                logger.info("æ­£åœ¨åˆå§‹åŒ– MouthAdapter (Priority 2)...")
                mouth_adapter = MouthAdapter(
                    model_path=settings.model.cosyvoice_model_path
                )
                await mouth_adapter.initialize()
                logger.success("âœ“ Mouth Adapter åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.error(f"âœ— Mouth Adapter å¤±è´¥: {e}")

        # 3. å¬è§‰ (SenseVoice)
        try:
            logger.info("æ­£åœ¨åˆå§‹åŒ– VoiceAdapter...")
            voice_adapter = VoiceAdapter(
                model_name=settings.model.funasr_model,
                device=settings.model.funasr_device
            )
            await voice_adapter.initialize()
            logger.success("âœ“ Voice Adapter åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"âœ— Voice Adapter å¤±è´¥: {e}")
            
        # 4. è¡¨æƒ… (GeneFace)
        try:
            logger.info("æ­£åœ¨åˆå§‹åŒ– DriverAdapter...")
            driver_adapter = DriverAdapter(
                geneface_path=settings.model.geneface_model_path
            )
            await driver_adapter.initialize()
            logger.success("âœ“ Driver Adapter åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"âœ— Driver Adapter å¤±è´¥: {e}")
        
    else:
        logger.warning("âš  Debug æ¨¡å¼: è·³è¿‡æ¨¡å‹åŠ è½½")
        # Debug æ¨¡å¼ä½¿ç”¨ Mock
        brain_adapter = BrainAdapter()
        await brain_adapter.initialize(mock=True)
    
    # åˆå§‹åŒ– Layer 3: è‡ªæˆ‘ (EgoDirector)
    if brain_adapter:
        ego_director = EgoDirector(
            brain=brain_adapter,
            bio_state=bio_state,
            narrative_mgr=narrative_mgr
        )
        logger.success("âœ“ Layer 3 (è‡ªæˆ‘) åˆå§‹åŒ–å®Œæˆ")
    
    logger.info("ğŸ­ Project Trinity å‡†å¤‡å°±ç»ª!")
    
    yield
    
    # æ¸…ç†
    logger.info("æ­£åœ¨å…³é—­ Project Trinity...")
    
    if monitor:
        monitor.stop()

    if voice_adapter:
        await voice_adapter.shutdown()
    if brain_adapter:
        await brain_adapter.shutdown()
    if mouth_adapter:
        await mouth_adapter.shutdown()
    if driver_adapter:
        await driver_adapter.shutdown()
    if narrative_mgr:
        await narrative_mgr.shutdown()
    
    logger.info("Project Trinity å·²å…³é—­")


# ============== FastAPI åº”ç”¨ ==============
app = FastAPI(
    title="Project Trinity",
    description="Next-Gen Digital Life Engine",
    version="0.1.0",
    lifespan=lifespan
)

# CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============== æ•°æ®æ¨¡å‹ ==============
class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚"""
    text: str
    emotion: str = "neutral"
    visual_context: Optional[str] = None


class ChatResponse(BaseModel):
    """èŠå¤©å“åº”"""
    response: str
    emotion_tag: str
    action_hints: list
    bio_state: dict


class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”"""
    status: str
    components: dict


# ============== API è·¯ç”± ==============

# æŒ‚è½½ Web å®¢æˆ·ç«¯ (LLM Workbench)
from fastapi.staticfiles import StaticFiles
static_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), "client/llm_workbench")
if os.path.exists(static_dir):
    app.mount("/workbench", StaticFiles(directory=static_dir, html=True), name="workbench")
    logger.info(f"Workbench mounted at /workbench -> {static_dir}")

@app.post("/chat/stream")
async def chat_stream(request: ChatRequest, background_tasks: BackgroundTasks):
    """
    æµå¼å¯¹è¯æ¥å£ (Real-Time Reflex)
    ç›´æ¥è¿æ¥ BrainAdapterï¼Œç»•è¿‡ EgoDirector çš„éƒ¨åˆ†é€»è¾‘ä»¥æµ‹è¯•æè‡´é€Ÿåº¦
    """
    if not brain_adapter:
        raise HTTPException(status_code=503, detail="BrainAdapter æœªåˆå§‹åŒ–")
        
    logger.info(f"Stream Request: {request.text[:50]}...")
    
    # å‡†å¤‡æ—¥å¿—æ•°æ®å®¹å™¨ï¼ˆå¯å˜å¯¹è±¡ï¼‰
    log_data = {
        "input": request.text,
        "output": "",
        "ttft": 0,
        "start": time.time()
    }
    
    # æ·»åŠ åå°ä»»åŠ¡ï¼Œåœ¨å“åº”ç»“æŸåæ‰§è¡Œ
    background_tasks.add_task(write_chat_log, log_data)
    
    async def event_generator():
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = log_data["start"]
        first_token_sent = False
        
        try:
            # ç›´æ¥è°ƒç”¨ BrainAdapter çš„æµå¼æ–¹æ³•
            generator = brain_adapter.process_stream(
                user_input=request.text,
                temperature=0.7 
            )
            
            async for chunk in generator:
                if chunk["type"] == "token":
                    content = chunk["content"]
                    log_data["output"] += content # å®æ—¶æ›´æ–°æ—¥å¿—å®¹å™¨
                    
                    yield content
                    
                    if not first_token_sent:
                        first_token_sent = True
                        ttft_ms = (time.time() - start_time) * 1000
                        log_data["ttft"] = ttft_ms
                        logger.info(f"âš¡ Stream TTFT: {ttft_ms:.2f}ms")
                        
                elif chunk["type"] == "error":
                    error_msg = f"[ERROR: {chunk['content']}]"
                    log_data["output"] += error_msg
                    yield error_msg
                    
        except Exception as e:
            logger.error(f"Stream Error: {e}")
            yield f"[SYSTEM ERROR: {str(e)}]"
            
    return StreamingResponse(event_generator(), media_type="text/event-stream")

@app.get("/")
async def root():
    """æ ¹è·¯ç”±"""
    return {
        "name": "Project Trinity",
        "version": "0.1.0",
        "status": "running"
    }


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    components = {
        "bio_state": bio_state is not None,
        "narrative_mgr": narrative_mgr is not None and narrative_mgr.is_initialized,
        "ego_director": ego_director is not None,
        "voice_adapter": voice_adapter is not None and voice_adapter.is_initialized if voice_adapter else False,
        "brain_adapter": brain_adapter is not None and brain_adapter.is_initialized if brain_adapter else False,
    }
    
    status = "healthy" if all(components.values()) else "degraded"
    
    return HealthResponse(status=status, components=components)


@app.post("/avatar/generate")
async def generate_avatar(
    background_tasks: BackgroundTasks,
    image: UploadFile = File(...)
):
    """
    [FastAvatar] ä»ç…§ç‰‡ç”Ÿæˆ 3DGS èµ„äº§
    è¿™æ˜¯ä¸€ä¸ªè€—æ—¶æ“ä½œï¼Œå°†åœ¨åå°è¿è¡Œã€‚
    """
    if not driver_adapter:
        raise HTTPException(status_code=503, detail="DriverAdapter æœªåˆå§‹åŒ–")
        
    upload_dir = Path("uploads")
    upload_dir.mkdir(exist_ok=True)
    
    file_ext = image.filename.split(".")[-1]
    file_id = str(uuid.uuid4())
    image_path = upload_dir / f"{file_id}.{file_ext}"
    output_dir = Path("assets/avatars") / file_id
    output_dir.mkdir(parents=True, exist_ok=True)
    
    with open(image_path, "wb") as buffer:
        shutil.copyfileobj(image.file, buffer)
        
    logger.info(f"æ”¶åˆ° Avatar ç”Ÿæˆè¯·æ±‚: {image.filename} -> {file_id}")
    
    # å¼‚æ­¥æ‰§è¡Œç”Ÿæˆä»»åŠ¡
    async def _run_generation():
        success = await driver_adapter.generate_avatar(str(image_path), str(output_dir))
        if success:
            logger.success(f"Avatar ç”Ÿæˆå®Œæˆ: {file_id}")
            # TODO: é€šçŸ¥å®¢æˆ·ç«¯æˆ–æ›´æ–°æ•°æ®åº“
        else:
            logger.error(f"Avatar ç”Ÿæˆå¤±è´¥: {file_id}")

    background_tasks.add_task(_run_generation)
    
    return {
        "status": "processing", 
        "task_id": file_id,
        "message": "Avatar ç”Ÿæˆä»»åŠ¡å·²æäº¤ï¼Œè¯·ç¨å€™ã€‚"
    }


@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    æ–‡æœ¬å¯¹è¯æ¥å£
    
    ç”¨äºæµ‹è¯•å’Œç®€å•åœºæ™¯
    """
    if ego_director is None:
        raise HTTPException(status_code=503, detail="EgoDirector æœªåˆå§‹åŒ–")
    
    try:
        decision = await ego_director.process(
            user_text=request.text,
            detected_emotion=request.emotion,
            visual_context=request.visual_context
        )
        
        return ChatResponse(
            response=decision.response_text,
            emotion_tag=decision.emotion_tag,
            action_hints=decision.action_hints,
            bio_state={
                "temperature": decision.llm_temperature,
                "triggered_reflex": decision.triggered_reflex
            }
        )
        
    except Exception as e:
        logger.error(f"Chat å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """
    WebSocket å®æ—¶é€šä¿¡
    
    åè®®:
    - Client -> Server: { "type": "audio", "data": base64 } æˆ– { "type": "text", "data": "..." }
    - Server -> Client: { "type": "response", "text": "...", "audio": base64, "flame": [...] }
    """
    await websocket.accept()
    logger.info("WebSocket å®¢æˆ·ç«¯å·²è¿æ¥")
    
    try:
        while True:
            # æ¥æ”¶æ¶ˆæ¯
            data = await websocket.receive_text()
            message = json.loads(data)
            
            msg_type = message.get("type", "text")
            
            if msg_type == "text":
                # æ–‡æœ¬æ¶ˆæ¯
                user_text = message.get("data", "")
                emotion = message.get("emotion", "neutral")
                
                if ego_director:
                    decision = await ego_director.process(
                        user_text=user_text,
                        detected_emotion=emotion
                    )
                    
                    response = {
                        "type": "response",
                        "text": decision.response_text,
                        "emotion": decision.emotion_tag,
                        "actions": decision.action_hints,
                        "reflex": decision.triggered_reflex
                    }
                else:
                    response = {
                        "type": "error",
                        "message": "System not ready"
                    }
                
                await websocket.send_text(json.dumps(response))
            
            elif msg_type == "audio":
                # éŸ³é¢‘æ¶ˆæ¯ (TODO: Phase 1)
                await websocket.send_text(json.dumps({
                    "type": "error",
                    "message": "Audio processing not implemented yet"
                }))
            
            elif msg_type == "heartbeat":
                # å¿ƒè·³
                await websocket.send_text(json.dumps({
                    "type": "heartbeat",
                    "status": "ok"
                }))
    
    except WebSocketDisconnect:
        logger.info("WebSocket å®¢æˆ·ç«¯å·²æ–­å¼€")
    except Exception as e:
        logger.error(f"WebSocket é”™è¯¯: {e}")


# ============== æµ‹è¯•ç«¯ç‚¹ ==============

@app.post("/transcribe")
async def transcribe_audio(file: UploadFile = File(...)):
    """
    è¯­éŸ³è¯†åˆ«æ¥å£ (ASR)
    
    æ¥å—éŸ³é¢‘æ–‡ä»¶ï¼Œè¿”å›è¯†åˆ«æ–‡æœ¬å’Œæƒ…æ„Ÿ
    """
    if not voice_adapter or not voice_adapter.is_initialized:
        raise HTTPException(status_code=503, detail="VoiceAdapter æœªåˆå§‹åŒ–")
    
    try:
        import io
        import wave
        import numpy as np
        
        # è¯»å–ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶
        audio_bytes = await file.read()
        
        # å°è¯•è§£æ WAV æ ¼å¼
        try:
            wav_buffer = io.BytesIO(audio_bytes)
            with wave.open(wav_buffer, 'rb') as wav_file:
                sample_rate = wav_file.getframerate()
                n_frames = wav_file.getnframes()
                audio_data = wav_file.readframes(n_frames)
                # è½¬æ¢ä¸º numpy array
                audio_array = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0
        except Exception:
            # å¦‚æœä¸æ˜¯æ ‡å‡† WAVï¼Œå°è¯•ç›´æ¥ä½œä¸º PCM å¤„ç†
            audio_array = np.frombuffer(audio_bytes, dtype=np.int16).astype(np.float32) / 32768.0
            sample_rate = 16000
        
        # è°ƒç”¨ ASR
        result = await voice_adapter.process(audio_array, sample_rate)
        
        return {
            "text": result.text,
            "emotion": result.emotion,
            "confidence": result.emotion_confidence,
            "language": result.language
        }
        
    except Exception as e:
        logger.error(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/synthesize")
async def synthesize_speech(request: dict):
    """
    è¯­éŸ³åˆæˆæ¥å£ (TTS)
    
    æ¥å—æ–‡æœ¬ï¼Œè¿”å›éŸ³é¢‘æ•°æ®
    """
    if not mouth_adapter:
        raise HTTPException(status_code=503, detail="MouthAdapter æœªåˆå§‹åŒ–")
    
    text = request.get("text", "")
    instruct_text = request.get("instruct_text", "ç”¨æ¸©æŸ”ç”œç¾çš„å¥³å£°è¯´")
    
    if not text:
        raise HTTPException(status_code=400, detail="text is required")
    
    try:
        # è°ƒç”¨ TTS
        result = await mouth_adapter.process(text, instruct_text)
        
        if "error" in result:
            raise HTTPException(status_code=500, detail=result["error"])
        
        # è½¬æ¢ä¸º WAV æ ¼å¼è¿”å›
        import io
        import wave
        import numpy as np
        
        audio_array = np.array(result["audio"], dtype=np.float32)
        sample_rate = result["sample_rate"]
        
        # è½¬æ¢ä¸º 16-bit PCM
        audio_int16 = (audio_array * 32767).astype(np.int16)
        
        # åˆ›å»º WAV æ–‡ä»¶
        wav_buffer = io.BytesIO()
        with wave.open(wav_buffer, 'w') as wav_file:
            wav_file.setnchannels(1)
            wav_file.setsampwidth(2)
            wav_file.setframerate(sample_rate)
            wav_file.writeframes(audio_int16.tobytes())
        
        wav_buffer.seek(0)
        
        from fastapi.responses import StreamingResponse
        return StreamingResponse(
            wav_buffer,
            media_type="audio/wav",
            headers={"Content-Disposition": "attachment; filename=speech.wav"}
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è¯­éŸ³åˆæˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============== å¯¹è¯æ—¥å¿— API ==============

@app.get("/logs/dates")
async def list_log_dates():
    """åˆ—å‡ºæ‰€æœ‰æœ‰æ—¥å¿—çš„æ—¥æœŸ"""
    from pathlib import Path
    log_dir = Path("/workspace/project-trinity/project-trinity/logs/conversations")
    
    if not log_dir.exists():
        return {"dates": []}
    
    dates = [f.stem for f in log_dir.glob("*.jsonl")]
    return {"dates": sorted(dates, reverse=True)}


@app.get("/logs/{date}")
async def get_logs_by_date(date: str):
    """
    è·å–æŒ‡å®šæ—¥æœŸçš„å¯¹è¯è®°å½•
    
    Args:
        date: æ—¥æœŸ YYYY-MM-DD
    """
    import json
    from pathlib import Path
    
    log_file = Path(f"/workspace/project-trinity/project-trinity/logs/conversations/{date}.jsonl")
    
    if not log_file.exists():
        return {"date": date, "conversations": [], "count": 0}
    
    conversations = []
    with open(log_file, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                conversations.append(json.loads(line))
    
    return {
        "date": date,
        "conversations": conversations,
        "count": len(conversations)
    }


@app.get("/logs/today")
async def get_today_logs():
    """è·å–ä»Šå¤©çš„å¯¹è¯è®°å½•"""
    today = datetime.now().strftime("%Y-%m-%d")
    return await get_logs_by_date(today)


# ============== ä¸»å…¥å£ ==============
if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host=settings.server.host,
        port=settings.server.port,
        reload=settings.server.debug
    )
</file>

</files>
