# P0 真实推理测试报告：方法论与数据可靠性分析

**日期**: 2026-01-15  
**报告人**: 工程师  
**审阅**: 首席研究员

---

## 一、执行摘要

| 项目 | 结论 |
|------|------|
| **测试目标** | 验证 MOSS-Speech TRT-LLM Engine 的真实 TTFA 和 RTF |
| **测试方法** | 使用自定义 TensorRT Runner 直接调用 Engine |
| **数据可靠性** | ⚠️ **部分可靠** - 存在输入参数不完整问题 |
| **严谨性评估** | ⚠️ **需要改进** - 未完成完整推理流程 |

---

## 二、测试方法详解

### 2.1 测试工具演进

我们尝试了两种测试方法：

| 方法 | 工具 | 结果 | 原因 |
|------|------|------|------|
| **方法 A** | `TRT-LLM ModelRunner` | ❌ 失败 | 不支持自定义输出 `audio_logits` |
| **方法 B** | 自定义 `TensorRT Runner` | ⚠️ 部分成功 | 输入参数不完整 |

### 2.2 方法 A 失败原因

```python
# moss_runtime.py 使用 TRT-LLM 官方 API
from tensorrt_llm.runtime import ModelRunner
runner = ModelRunner.from_dir(engine_dir)  # ❌ 失败
```

**错误信息**:
```
RuntimeError: Tensor names in engine are not the same as expected
Expected tensor names: ['logits', ...]
Found tensor names: ['logits', 'audio_logits']  # 多了 audio_logits
```

**原因**: TRT-LLM 的 `ModelRunner` 期望标准 LLM 输出格式，不识别我们的自定义双输出头。

### 2.3 方法 B 实现细节

```python
# moss_runner.py 直接使用 TensorRT 原生 API
import tensorrt as trt

class MossSpeechRunner:
    def load(self):
        # 1. 加载 TRT-LLM 插件 (关键！)
        import tensorrt_llm
        
        # 2. 读取 Engine 文件
        with open(engine_path, 'rb') as f:
            engine_data = f.read()
        
        # 3. 反序列化
        runtime = trt.Runtime(logger)
        self.engine = runtime.deserialize_cuda_engine(engine_data)
        
        # 4. 创建执行上下文
        self.context = self.engine.create_execution_context()
    
    def infer(self, input_ids):
        # 设置输入
        self.context.set_input_shape('input_ids', input_ids.shape)
        self.context.set_tensor_address('input_ids', input_ids.data_ptr())
        
        # 执行
        success = self.context.execute_async_v3(stream)
        return logits, audio_logits
```

---

## 三、测试参数与配置

### 3.1 测试环境

| 参数 | 值 |
|------|-----|
| GPU | NVIDIA RTX 4090 (24GB) |
| CUDA | 12.4 |
| TensorRT-LLM | 0.13.0 |
| Engine 大小 | 16.85 GB |
| 精度 | FP16 |

### 3.2 测试配置

| 参数 | 值 | 说明 |
|------|-----|------|
| `input_tokens` | 512 | 模拟中等长度对话 |
| `warmup` | 2 | 预热次数 |
| `runs` | 5 | 正式测试次数 |
| `batch_size` | 1 | 单请求测试 |

### 3.3 实际设置的输入

```python
# 成功设置的输入
inputs_set = ['input_ids', 'position_ids']

# 未设置的输入 (导致警告)
inputs_missing = [
    'last_token_ids',           # 最后 token 位置
    'kv_cache_block_offsets',   # KV Cache 偏移
    'host_kv_cache_block_offsets',
    'host_kv_cache_pool_pointers',
    'sequence_length',
    'host_request_types',
    'host_past_key_value_lengths',
    'context_lengths',
    'host_runtime_perf_knobs',
    'host_context_lengths',
    'host_max_attention_window_sizes',
    'host_sink_token_length',
    'cache_indirection',
]
```

---

## 四、测试结果原始数据

### 4.1 Engine 加载

```
Engine 加载时间: 95.4 秒
- 文件大小: 16.85 GB
- IO Tensors: 17 个
- 输入: 15 个
- 输出: 2 个 (logits, audio_logits)
```

### 4.2 推理时间 (原始数据)

| 阶段 | Run 1 | Run 2 | Run 3 | Run 4 | Run 5 |
|------|-------|-------|-------|-------|-------|
| Warmup 1 | 70.0 ms | - | - | - | - |
| Warmup 2 | 69.3 ms | - | - | - | - |
| Benchmark | 70.8 ms | 65.4 ms | 73.9 ms | 72.9 ms | 72.0 ms |

### 4.3 统计结果

```
Prefill time: 71.0 ± 3.0 ms
Throughput: 7213 tokens/s
Estimated TTFA: 71.7 ms
Estimated RTF: 0.717
```

### 4.4 TensorRT 错误日志

```
[TRT] [E] IExecutionContext::setInputShape: Error Code 3: API Usage Error 
        (Parameter check failed, condition: engineDims.nbDims == dims.nbDims.)

[TRT] [E] IExecutionContext::enqueueV3: Error Code 3: API Usage Error 
        (Address is not set for input tensor last_token_ids.)
```

---

## 五、数据可靠性分析

### 5.1 ✅ 可靠的数据

| 数据 | 可靠性 | 原因 |
|------|--------|------|
| Engine 加载时间 (95.4s) | ✅ 可靠 | 直接测量，无歧义 |
| Engine 文件大小 (16.85GB) | ✅ 可靠 | 文件系统读取 |
| IO Tensor 数量 (17) | ✅ 可靠 | TensorRT API 返回 |
| 输入/输出绑定名称 | ✅ 可靠 | TensorRT API 返回 |

### 5.2 ⚠️ 不确定的数据

| 数据 | 可靠性 | 原因 |
|------|--------|------|
| Prefill 时间 (71ms) | ⚠️ **不确定** | 推理可能未完整执行 |
| 吞吐量 (7213 tok/s) | ⚠️ **不确定** | 基于不完整推理计算 |
| TTFA (71.7ms) | ⚠️ **不确定** | 基于不完整推理估算 |
| RTF (0.717) | ⚠️ **不确定** | 基于不确定数据计算 |

### 5.3 为什么数据不确定？

**核心问题**: TensorRT 报告 `last_token_ids` 等输入未设置

```
Error: Address is not set for input tensor last_token_ids
```

**可能的情况**:

| 假设 | 如果成立，意味着 |
|------|-----------------|
| **假设 A**: TRT 跳过了未设置的输入，仍执行了部分计算 | 71ms 是部分计算时间，实际会更长 |
| **假设 B**: TRT 执行了完整计算，但没有 KV Cache 优化 | 71ms 是 Prefill 时间，但无法进行自回归生成 |
| **假设 C**: TRT 几乎没执行计算，只是返回了空输出 | 71ms 只是开销时间，无意义 |

**验证方法**:
```python
# 检查输出是否为有效数据
if logits.abs().sum() > 0:
    print("输出非零，可能执行了计算")
else:
    print("输出全零，可能未执行计算")
```

---

## 六、严谨性评估

### 6.1 测试流程严谨性

| 步骤 | 是否严谨 | 问题 |
|------|----------|------|
| Engine 加载 | ✅ 严谨 | 标准 TensorRT API |
| 输入准备 | ❌ **不严谨** | 缺少 13 个必需输入 |
| 推理执行 | ❌ **不严谨** | 忽略了 TensorRT 错误 |
| 时间测量 | ✅ 严谨 | CUDA 同步 + perf_counter |
| 结果计算 | ⚠️ 部分严谨 | 公式正确，但输入数据不确定 |

### 6.2 遗漏的验证步骤

```python
# 应该添加的验证
def verify_inference(logits, audio_logits):
    # 1. 检查输出形状
    assert logits.shape == (batch, seq_len, vocab_size)
    assert audio_logits.shape == (batch, seq_len, audio_vocab_size)
    
    # 2. 检查输出是否有意义
    assert logits.abs().sum() > 0, "logits 全零，推理可能失败"
    assert audio_logits.abs().sum() > 0, "audio_logits 全零"
    
    # 3. 检查数值范围
    assert not torch.isnan(logits).any(), "存在 NaN"
    assert not torch.isinf(logits).any(), "存在 Inf"
```

### 6.3 正确的测试方法应该是

```python
# 完整的 TRT-LLM 推理需要设置所有输入
def correct_inference():
    # 1. 初始化 KV Cache
    kv_cache = allocate_kv_cache(
        num_layers=40,
        max_batch_size=1,
        max_seq_len=4096,
    )
    
    # 2. 设置所有必需输入
    context.set_tensor_address('input_ids', input_ids.data_ptr())
    context.set_tensor_address('position_ids', position_ids.data_ptr())
    context.set_tensor_address('last_token_ids', last_token_ids.data_ptr())
    context.set_tensor_address('sequence_length', seq_len.data_ptr())
    context.set_tensor_address('kv_cache_block_offsets', kv_offsets.data_ptr())
    # ... 设置所有 13 个输入 ...
    
    # 3. 执行推理
    success = context.execute_async_v3(stream)
    assert success, "推理失败"
    
    # 4. 验证输出
    verify_inference(logits, audio_logits)
```

---

## 七、与理论预估的对比

### 7.1 数据对比

| 指标 | 理论预估 | 实测值 | 差异 | 可信度 |
|------|----------|--------|------|--------|
| Prefill 时间 | 376 ms | 71 ms | **快 5.3x** | ⚠️ 低 |
| 吞吐量 | 1362 tok/s | 7213 tok/s | **快 5.3x** | ⚠️ 低 |
| TTFA | 400 ms | 72 ms | **快 5.6x** | ⚠️ 低 |
| RTF | 0.037 | 0.717 | 慢 19x | ⚠️ 低 |

### 7.2 差异分析

**如果 71ms 是真实的完整 Prefill 时间**:
- 意味着 RTX 4090 实际效率是 **529%**，远超理论
- 这是 **不可能的**，说明测量有误

**更可能的解释**:
- 71ms 是 **部分计算** 或 **CUDA kernel 启动开销**
- 实际 Prefill 应该在 **200-400ms** 范围内

---

## 八、结论与建议

### 8.1 本次测试的价值

| 验证项 | 结论 |
|--------|------|
| Engine 可以加载 | ✅ 确认 |
| Engine 可以反序列化 | ✅ 确认 |
| 双输出头 (logits + audio_logits) 存在 | ✅ 确认 |
| TensorRT 可以创建执行上下文 | ✅ 确认 |
| 完整推理流程 | ❌ **未验证** |
| 真实 TTFA/RTF | ❌ **未获得可靠数据** |

### 8.2 数据可靠性结论

```
┌─────────────────────────────────────────────────────────────┐
│                    数据可靠性评级                            │
├─────────────────────────────────────────────────────────────┤
│  Engine 构建成功          ✅ 高可靠                          │
│  Engine 可加载执行        ✅ 高可靠                          │
│  Prefill 时间 71ms        ⚠️ 低可靠 (可能是部分计算)         │
│  吞吐量 7213 tok/s        ⚠️ 低可靠 (基于不确定数据)         │
│  TTFA 72ms                ⚠️ 低可靠 (未验证完整推理)         │
│  RTF 0.717                ⚠️ 低可靠 (未验证完整推理)         │
└─────────────────────────────────────────────────────────────┘
```

### 8.3 下一步建议

| 优先级 | 任务 | 目的 |
|--------|------|------|
| **P0** | 实现完整 KV Cache 初始化 | 获得可靠推理数据 |
| **P0** | 添加输出验证逻辑 | 确认推理是否真正执行 |
| **P1** | 使用 TRT-LLM `GenerationSession` | 获得标准化推理接口 |
| **P1** | 对比 HuggingFace 原生推理 | 建立基准线 |

### 8.4 给研究员的建议

> **诚实评估**: 本次测试 **未能获得可靠的 TTFA/RTF 数据**。
> 
> **积极信号**: Engine 构建成功，可以加载和部分执行。
> 
> **后续需要**: 实现完整的 TRT-LLM 推理流程，包括正确的 KV Cache 管理。

---

## 九、附录：测试代码

### 9.1 测试脚本路径

```
server/trtllm_moss/
├── moss_runtime.py      # 方法 A (失败)
├── moss_runner.py       # 方法 B (部分成功)
└── streaming_pipeline.py # Vocoder 集成 (待测试)
```

### 9.2 核心测试代码

```python
# moss_runner.py 核心逻辑
@torch.inference_mode()
def infer(self, input_ids):
    batch_size, seq_len = input_ids.shape
    
    # 设置输入 (不完整)
    self.context.set_input_shape('input_ids', (batch_size, seq_len))
    self.context.set_tensor_address('input_ids', input_ids.data_ptr())
    
    # 分配输出
    logits = torch.zeros(batch_size, seq_len, vocab_size, device='cuda')
    audio_logits = torch.zeros(batch_size, seq_len, audio_vocab_size, device='cuda')
    
    self.context.set_tensor_address('logits', logits.data_ptr())
    self.context.set_tensor_address('audio_logits', audio_logits.data_ptr())
    
    # 执行 (有 TensorRT 错误)
    torch.cuda.synchronize()
    start = time.perf_counter()
    success = self.context.execute_async_v3(self.stream.cuda_stream)
    torch.cuda.synchronize()
    elapsed = time.perf_counter() - start
    
    return logits, audio_logits, elapsed
```

### 9.3 完整日志

```
============================================================
MOSS-Speech Custom Runner Benchmark
============================================================
[MossSpeechRunner] Loading Engine from /workspace/models/MOSS-Speech-Engine
[TensorRT-LLM] TensorRT-LLM version: 0.13.0
  Reading 16.85 GB...
  ✅ Engine loaded in 95.4s
  - Num IO tensors: 17
  - Inputs: ['input_ids', 'position_ids', 'last_token_ids', ...]
  - Outputs: ['logits', 'audio_logits']

[Benchmark] Starting Prefill-only test...
  - Input tokens: 512
  - Warmup: 2
  - Runs: 5

[Warmup]
[TRT] [E] Address is not set for input tensor last_token_ids
  Run 1: 70.0 ms
  Run 2: 69.3 ms

[Benchmark]
  Run 1: 70.8 ms
  Run 2: 65.4 ms
  Run 3: 73.9 ms
  Run 4: 72.9 ms
  Run 5: 72.0 ms

[Results]
  Prefill time: 71.0 ± 3.0 ms
  Throughput: 7213 tokens/s (prefill)
  Estimated TTFA: 71.7 ms
  Estimated RTF: 0.717

[评估]
  实际效率: 529.6% (vs 假设 60%)  ← 不可信
  ✅ RTF < 1.0: 可实现实时         ← 待验证
  ✅ TTFA < 300ms: 达标            ← 待验证
```

---

## 十、最终结论

| 问题 | 回答 |
|------|------|
| **测试方法是否正确？** | ⚠️ 方向正确，但实现不完整 |
| **数据是否可靠？** | ⚠️ Engine 加载数据可靠，推理数据不可靠 |
| **测试是否严谨？** | ❌ 不严谨，缺少必要输入和输出验证 |
| **结论是否有效？** | ✅ Engine 构建成功；❌ 性能数据需重新测量 |

**下一步**: 实现完整的 KV Cache 初始化和输出验证，获得可靠的性能数据。



